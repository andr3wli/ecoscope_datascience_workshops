[["index.html", "ECOSCOPE: Data Science Workshops Chapter 1 ECOSCOPE 1.1 Schedule 1.2 Workshops", " ECOSCOPE: Data Science Workshops Andrew Li Stephan Koenig 2021-08-23 Chapter 1 ECOSCOPE This is an open source textbook aimed at introducing data science and R programming to undergraduate and graduate students. It was originally written as a learning tool for the ECOSCOPE workshops hosted at the University of British Columbia. The book is structured so that each chapter is a different workshop. Note: This book is in its preliminary stages. Content will be continuously updated and added. 1.1 Schedule 2021 Term 1 (Sept - Dec) Date Time Workshop Register Sept 21 (Tues) 2-4 PM Intro to R Click here to register Sept 28 &amp; 30 (Tues &amp; Thurs) 1-4 PM Intro to the tidyverse Click here to register Oct 19 (Tues) 2-4 PM Intro to R Click here to register Oct 26 &amp; 28 (Tues &amp; Thurs) 1-4 PM Reproducible research Click here to register Nov 16 (Tues) 2-4 PM Intro to R Click here to register Nov 23 &amp; 25 (Tues &amp; Thurs) 1-4 PM Statistical models in R Click here to register 2021 Term 2 (Jan - Apr) Date Time Workshop Register Sept 21 (Tues) 2-4 PM Intro to R Click here to register Sept 28 &amp; 30 (Tues &amp; Thurs) 1-4 PM Intro to the tidyverse Click here to register Oct 19 (Tues) 2-4 PM Intro to R Click here to register Oct 26 &amp; 28 (Tues &amp; Thurs) 1-4 PM Beyond ggplot2 Click here to register Nov 16 (Tues) 2-4 PM Intro to R Click here to register Nov 23 &amp; 25 (Tues &amp; Thurs) 1-4 PM Intermediate R programming Click here to register 1.2 Workshops 1.2.1 Introduction to R and R Studio Author(s): Gil B. Henriques, Florent Mazel, Yue Liu &amp; Kim Dill-McFarland This is a truly introductory workshop for beginners with no experience in R. In this workshop, we introduce you to R and RStudio at the beginner level. This condensed 2-hour workshop is meant to get you started in R and acts as a pre-requisite for our more advanced workshops. In it, we cover: R and RStudio RStudio projects R scripts Installing packages Reading in data as a data frame Vectors, single values, and data types Basic data visualization The help function 1.2.2 Introduction to the tidyverse Author(s): Kim Dill-McFarland, Andrew Li &amp; Kris Hong In this workshop, we provide a brief introduction to RStudio, then delve into data manipulation and graphics in the tidyverse including the packages dplyr, tidyr, and ggplot2. We teach different ways to manipulate data in tabular and text forms as well as the critical concepts underlying the grammar of graphics and how they are implemented in ggplot. We will use RStudio, a powerful but user-friendly R environment, and teach you how to use it effectively. You will learn how to: create an R project and import data from a file into R, create subsets of rows or columns from data frames using dplyr, select pieces of an object by indexing using element names or position, change your data frames between wide and narrow formats, create various types of graphics, modify the various features of a graphic, and save your graphic in various formats 1.2.3 Reproducible research Author(s): Kim Dill-McFarland &amp; Kris Hong In this workshop, we introduce computational reproducibility and its importance to modern research. We will teach the general principles for reproducible computer-based analyses, along with specific methods and tools for reproducibility and version control with RStudio and GitHub. You will learn how to: Construct reproducible, automatable workflows in R with scripts and Make Create reproducible documents using Rmarkdown to include underlying code / computations with relevant graphical and statistical results in several different formats (reports, presentation slides, handouts, notes) Use Git version control Integrate version control with GitHub for both personal and group projects 1.2.4 Statistical models in R Author(s): Yue Liu, Kim Dill-McFarland &amp; Andrew Li In this workshop, we introduce various types of regression models and how they are implemented in R. We cover linear regression, ANOVA, ANCOVA and mixed effects models for continuous response data, logistic regression binary response data, and Poisson and Negative Binomial regression for count response data. You will learn: the assumptions behind the different models how to interpret the main effects and interaction terms in a model various experimental design concepts that help maximize the power In R, you will learn how to; build a statistical model define and manipulate model terms use the lsmeans package to answer specific research questions 1.2.5 Intermediate R programming Author(s): Kim Dill-McFarland &amp; Andrew Li In this workshop, we teach you to use R as a programming environment, allowing you to write more complex, yet clearer data analysis code. We will teach you three fundamental concepts of R programming: functions, classes, and packages. You will learn how to: define objects, classes, and attributes in data and built-in functions write functions for loops output large result tables to your hard drive write and publish an R package write formal automated tests (aka unit testing) 1.2.6 Beyond ggplot2 Author(s): Andrew Li Coming soon! "],["introduction-to-r-and-rstudio.html", "Chapter 2 Introduction to R and RStudio 2.1 Introduction 2.2 Using R in your own computer 2.3 R as a calculator 2.4 Assigning variables 2.5 Functions 2.6 Data types 2.7 Structures 2.8 Data frames 2.9 Quick plots 2.10 Accessing and subsetting data 2.11 Additional exercises 2.12 Survey", " Chapter 2 Introduction to R and RStudio 2.1 Introduction In this tutorial, we introduce you to R and RStudio at the beginner level. This self-guided tutorial is meant to get you started in R and acts as a prerequisite for our other tutorials. First, you will start by learning the fundamentals of the R language through interactive exercises. Then, at the end of the tutorial, you will learn more about working with R on your own computer, creating scripts and projects, and using RStudio, a friendly user interface between you and R. 2.1.1 What is R? Let’s start with the basics: what is R? It is a programming language used across many sciences and industries. Because it was originally designed by and for statisticians, it excels at tasks like data analysis and data visualization. 2.1.1.1 what is a programming language? It is a language that you use to give instructions to your computer. It interfaces between the user and the machine. This common language has particular rules (called syntax). 2.1.1.2 Why is this important? Why not use excel? Why use a language like R instead of point-and-click alternatives like Excel? For many reasons! Firstly, R is free and open-source. And because it is a programming language, it is vastly more flexible and reproducible (more on these later). 2.2 Using R in your own computer In this section, you will learn more about the RStudio environment, how to create files and scripts, load data files that are saved in your computer, and install packages. 2.2.1 What is RStudio? A friendly interface between you and the R console. RStudio is an integrated development and analysis environment (IDE) for R that brings a number of conveniences over using R in a terminal or other editing environments. You may be running this tutorial from within RStudio. If that is not the case, open RStudio in your computer. When you start RStudio, you will see something like the following window appear: Notice that it is divided into different “panes.” For now we will focus on the console (left side). 2.2.2 The R console The console is your view into the R engine. You can use it to give commands to R and immediately see the output. The &gt; symbol in the console is called the prompt. It is inviting you to type R commands, such as 2 * 2 or mean(x). When you press Return, you can see the output printed by R. Try this out in your own console. Sometimes, you may accidentally press Return before finishing a complete command. This often happens because you forgot to close parentheses. For example, you may have written: mean(c(1,2,3); this command is incomplete because it is missing one ) at the end. When you give the console an incomplete command, R expects you to complete it. Therefore, the prompt (&gt;) is replaced by a plus sign (+). This is R’s way of telling you to continue writting. When this happens, you have two alternatives. Either you complete the command (e.g., write the missing )) and then press Return or you can press Esc, to cancel the command and show the prompt. 2.2.3 The R Environment Using your own console, create a new variable, e.g., my_variable &lt;- 3. (Note that RStudio has a shortcut for the assignment operator: Alt+- in Windows or Option+- in a Mac.) This variable now “exists” for the remainder of the R session. Whenever you type my_variable, it will be like typing the value 3. The set of all variables that have been created in a particular R session is called the environment (also known as the workspace). In RStudio, you can see all currently-existing variables in the Environment pane (tabbed in the upper right). If you open this pane, you will find your newly-created variable. This is helpful to keep track of the currently existing variables and their current values. A word of caution: auto-saving of the R environment By default, when you save your work, RStudio also auto-saves your environment (in an .RData file). This means that when you close RStudio and then re-open it, all of the variables will be created for you immediately. This may seem practical, because it means that you can start right where you left off without re-importing any data or re-calculating any intermediate steps. In practice, however, this is not a good idea. For example, when you reload a saved environment, you may inadvertently use variables from an older version of your R code. It will save you many headaches if you learn to avoid thinking of the variables in your environment as “real” files that are saved somewhere in your computer (the way your R scripts and data files are). Instead, if you want to ensure that you can work across multiple sessions, and that your work remains reproducible when you revisit it a long time in the future, you should focus on keeping a good script. You should be able to re-create your environment from your script every session. It is a best practice to disable RStudio’s environment auto-saving option. Do the following: Click Tools &gt; Global Options. Where it says “Save workspace to .RData on exit,” select the option “Never.” Click “OK” and close the dialog. Now R will never save or prompt you to save your environment from RStudio. 2.2.4 Scripts So far, we have been typing commands into the console directly. This simple process lets you “talk” with R one command at a time, and get immediate output. However, this approach is not very practical. Each time you want to execute a set of commands, you have to re-enter them manually, one by one. That is very different from the code boxes we used throughout this tutorial. In those interactive boxes, you typed multiple lines of code, and then pressed Run, which sent the entire set of instructions to the console, all at once. Each of those interactive boxes is a small script. R scripts are simply text files that contain all the code that loads your raw data, cleans it, performs the analyses, and creates and saves visualizations. You can save a script in your computer as a file, so that you can run it again in the future, if you need to, and you can edit the script at any point. 2.2.4.1 Advantages of working with scripts R scripts maintain a record of everything that is done to the raw data to reach the final result: It is very easy to write up and communicate your methods because you have a document listing the precise steps you used to conduct your analyses. This is one of R’s primary advantages compared to traditional tools like Excel, where it may be unclear how to reproduce the results. If you get new or different data, you can re-run your analysis in a single click by simply running the script again! (Compare to Excel where you would need to re-do all of the analysis again.) 2.2.4.2 When to use a script? When to use the console? Generally, if you are testing an operation (e.g., what would my data look like if I applied a log-transformation to it?), you should do it in the console. But if you are committing a step to your analysis (e.g., I want to apply a log-transformation to my data and then conduct the rest of my analyses on the log-transformed data), you should add it to your R script so that it is saved for future use. 2.2.4.3 How to create a script? To create a script, go to File &gt; New File &gt; R Script and save it in a convenient directory. If you open the directory on your computer, you will see the script is now saved there. Open the example script in RStudio and examine it. Don’t worry if you don’t understand every single line of code: you should still be able to understand the gist of what the script does (use the help() function if necessary). You can run the commands from the script into the console line by line. To do this, click on the first line and press Command+Return or the “Run” button in RStudio. Alternatively, you can run all of the script at once (click the “Source” button in RStudio). If you source the script, you won’t see any output in the console, but all the commands will be executed and you should see all the variables that were created in the Environment pane. You can also modify this script. For example, you can add other words to the names vector. Once you have done this, you can repeat all the calculations with a single click by sourcing the script: no need to redo your analysis. 2.2.5 Installing and loading packages In the Quick plots section, we discussed packages. They are sets of functions that extend R’s capabilities. Packages facilitate and enhance analyses. Anyone can make a package and share it with others. However, because those functions are not part of base R, they do not come pre-installed. You need to install any package you use in your computer. In the Quick plots section, we used the quickplot() function, from the popular data visualization package ggplot2. Let’s install ggplot2 in your computer. Packages are typically installed from CRAN (The Comprehensive R Archive Network), which is a database containing R itself as well as many R packages. Any package can be installed from CRAN using the install.packages() function. When you install a package, it is saved into your computer, and once a package is installed, you won’t need to reinstall it again. Copy the following code into your own RStudio console and run it to install ggplot2. install.packages(&quot;ggplot2&quot;) After installing a package, and every time you open a new RStudio session, the packages you want to use need to be loaded into the R workspace with the library() function (see the code box below). This tells R to access the package’s functions and prevents RStudio from lags that would occur if it automatically loaded every downloaded package every time you opened it. It is a good idea to load all of the packages you need for a particular analysis at the top of that analysis’ script. library(&quot;ggplot2&quot;) # Load ggplot2 package. 2.2.6 Paths and working directory In all operating systems, files “live” inside a directory (or “folder”). This directory, in turn, is nested within its own directory, which itself is nested within a larger directory, and so on, following a hierarchical structure. The top-most directory in such a filesystem is called the root directory. In Windows, the root directory is usually called C:\\Windows; in Mac OS and Linux it is simply called /. To tell your computer about a given file, it is often not enough to provide just the filename. Many different files with the same filename could exist in a given computer. Instead, you often have to provide the file’s absolute path, which describes the file’s unique location in the directory tree hierarchy. For example, in Mac OS or Linux a file’s absolute path could look like this: /home/user/docs/Letter.txt and in Windows, it could look like this: C:\\user\\docs\\Letter.txt Whenever you open R, there is some directory in your environment associated with your current R session. This is called the working directory. It is not always obvious what the current working directory is. You can learn what the current working directory is by typing the following in your console: getwd() # Which stands for &quot;get working directory&quot; This prints out the path of your current working directory. In RStudio, you can also open the Files pane (on the bottom-right) which shows all the files in the current working directory. Knowing the current working directory is important because whenever you want to load data from your computer (or save data or figures into your computer) R will look for it (or save it) in your current working directory. We will learn more about loading data later on. But first, we will learn about projects, a nice tool that simplifies our interactions with the working directory. 2.2.7 RStudio projects Working with absolute paths is very complicated, and can cause a lot of problems. For example, if you move the directory that contains your project, all of the absolute paths will change, and any script that relies on them will not work any more. Similarly, if you send your script to a colleague, it won’t work because the absolute paths will be different in their computer. To solve this problem, you should always work with RStudio projects. A project is essentially a directory (folder) in your computer that contains all of the files and outputs that you are working on. When you create a project, RStudio creates a directory containing an .Rproj file. This file ensures that, when you are working on the project, the R working directory is set to its project directory, no matter where in your computer that directory is located. Advantages: When you import data, R automatically looks for the file in the working directory (i.e., the project directory) instead of you having to specify an absolute file path on your computer like /Users/username/Desktop/. We will learn more about this soon. If you decide to save any plots or data files, R will also automatically save any of them in the project directory. 2.2.7.1 Creating an RStudio project RStudio has a simple interface to create and switch between projects, accessed from the button in the top-right corner of the RStudio window (labeled “Project: (None),” initially). Start by clicking the Project button in the upper right or going to the File menu. Select New Project and the following will appear. You can either create a project in an existing directory or make a new directory on your computer. Create a new project on your desktop. After your project is created, navigate to its directory using your Finder/File explorer. You will see the .RProj file has been created. To access this project in the future, simply double-click the .RProj file and RStudio will open the project. Alternatively, open RStudio and choose File &gt; Open Project. Whenever you open a project, the working directory is immediately set to the project’s directory. Make sure to save all scripts and data files associated with your project in that project’s directory. 2.3 R as a calculator The window below is a small interactive environment. You can write R code in it, and then, when you click the Run button, the code is executed by the computer and you receive an output. We can use it to perform simple calculator-style operations, such as addition (+), subtraction (-), multiplication (*), division (/), or power (^). The following code computes the answer to \\(1 + 1\\). How can we have R compute the answer to \\(3^2\\)?: 1 + 1 ## [1] 2 Just like when you use a calculator, you can use parentheses ( ) to define the order of operations. The following code calculates the answer to \\(\\frac{1+1}{4}\\). Change it so that it computes the answer to \\(\\frac{(1+1)}{4}\\): 1 + 1 / 4 ## [1] 1.25 For the most part, in R, spaces between symbols are optional. 1+1 or 1 + 1 mean the same thing, as does (3*2) or (3 * 2). R also knows common constants, for example \\(\\pi\\) can be accessed with pi. Write pi in the box below and run the code. Examine the output. Then, write code to divide \\(\\pi\\) by zero. What happens? pi ## [1] 3.141593 2.4 Assigning variables A variable is like a box that can contain any value inside of it. You can name your variables anything you want, provided that it doesn’t start with a number and it does not include any spaces or special characters other than . and _. For example, a variable could be called x, my_variable, or height. To create a variable, we need to assign a value to it, using the assignment operator &lt;-. For example, the code below creates a variable called x and assigns the value 5 to it. Think of the assignment operator as an arrow, “putting” the value 5 into a “box” labeled x. x &lt;- 5 As you can see, assigning variables does not return any output. Once a variable is created, it becomes part of your working environment, which means that you can use it to perform calculations. Go back to the box above and, in a new line, calculate x + 3, then run the code. When you perform an operation with a variable (like x + 3 above), R will merely print the result, but it won’t change the variable. To see this, return to the box above, and, in a third line, calculate x^2. When you run the code, the output calculates the square of the current value of x. Because no new value has been assigned to x, its current value is still 5. If we write down a variable that does not yet exist, R doesn’t know what it means. To demonstrate this, write the name of any variable that does not exist (e.g. y or does_not_exist) and run it: does_not_exist Error: object &#39;does_not_exist&#39; not found If you want to perform a sequence of calculations, you will need to assign intermediate results to variables. For example, say you set x to 2, which you want to square and then add 1 to the result. To do this, create a new line of code in which you assign to the variable x the value x^2. On the third line, calculate x + 1. Before you press Run, try to guess what the output will be. x &lt;- 2 x ## [1] 2 In the example above, you start with some initial data (the value of x) and then perform a series of operations with those data (square it, then add one). If you ever get different data, you can easily re-do the entire calculation: replace the initial value of x (in line 1) by some other number and press Run. If you had a long list of operations, it would be tedious to re-do the whole analysis from scratch, but because you are using R, all it takes is one click. 2.4.1 Comments It is best practice to annotate your R instructions with comments. In each line of code, any text preceded by the # symbol will not execute. Comments can be useful to remind yourself and to tell other readers what a specific chunk of code does. In the box below, the first line has a comment. Add similar comments explaining the second and third lines, then press Run. x &lt;- 2 # create a variable called x, assign the value 2 x &lt;- x + 5 x + 1 ## [1] 8 Again, try to predict the out of x before you run it! 2.5 Functions Functions are one of the basic units in programming. Generally speaking, a function takes some input (one or more “arguments”) and generates some output. In R, function argument(s) always go inside parentheses (). Some built-in functions include the natural logarithm (log()), the base-10 logarithm (log10()), the exponential function (exp()), and the square root (sqrt()). For example, the code below creates a variable called x and assigns it the value of the natural logarithm of 5. In a new line of code, calculate exp(x). Before you press run, think about what you think the outcome will be. x &lt;- log(5) 2.5.1 The most helpful function of all: the ‘help’ function You can get information about a specific function by running the command ?&lt;function&gt; or help(&lt;function&gt;) (replace &lt;function&gt; by the name of the function you are interested in). This command opens the help page, where you can find all information about a function’s purpose and its arguments. For beginners, it is useful to concentrate on the “Examples” and “Arguments” section to understand the typical usage of the function better. Run the code below to read the documentation for the logarithm function. Don’t worry if you don’t understand everything. When you are done, write code to return the base-3 logarithm of 8. (The answer should be about 1.89.) help(log) As you can see from the example above, some functions take multiple arguments. When that is the case, some or all of the arguments may be named. For example, the ‘log’ function has an argument named base. Writing the argument names is optional as long as you provide the arguments in the same sequence as given in the “Usage” section of the help page. For example, log(3, 8) is the same as log(3, base = 8). Furthermore, as you can see with the logarithm example, it is often the case that some of the arguments are optional, i.e. the argument has a default value: argument = default_value. You already learned that you could just call log(3), without specifying the base argument. Whenever this argument is not specified, it is assumed to be base \\(e\\) by default. When reading the help page of a function, the sheer number of arguments can often be overwhelming. It is often helpful to first concentrate on the mandatory arguments (i.e. arguments without a default value) to get a grasp on the usage of a function. 2.6 Data types So far, we’ve been manipulating numbers. Any variable that holds a number is of type numeric. But R can also handle other types of data. We can find a variable’s data type with the function class(). The code below confirms that the variable x is numeric: x &lt;- 3 class(3) ## [1] &quot;numeric&quot; A completely different data type consists of alphanumeric strings, like words. Note how we surrounded the character string by quotation marks. If we don’t do this, R will think you are referring to a variable name. x &lt;- &quot;Hello world!&quot; y &lt;- &quot;123&quot; class(x) ## [1] &quot;character&quot; class(y) ## [1] &quot;character&quot; A third data type is used for logical variables. Logical variables can only have two values: TRUE or FALSE. You can create a logical variable by assigning one of these values directly. Here we assign the value TRUE to the variable x and then confirm its data type: x &lt;- TRUE class(x) ## [1] &quot;logical&quot; But logical variables are most useful when they describe logical statements such as the ones in the box below. Each of these statements has a logical value: they are either TRUE or FALSE. Run the code below to find out the logical value of each statement. 0 &lt; 1 # smaller than ## [1] TRUE 0 &gt;= 0 # larger-or-equal to ## [1] TRUE 5 == 7.1 # equal to. Note TWO equal symbols. ## [1] FALSE 5 != pi # not equal to ## [1] TRUE We can assign the statements above to a variable. Then, use the function class() to check the variable’s data type. x &lt;- 0 &lt; 1 class(x) ## [1] &quot;logical&quot; 2.6.1 Exercises Given x &lt;- pi, what is the data type of x? Given x &lt;- \"2\", what is the data type of x? Given x &lt;- 1 + 2, what is the data type of x? Given x &lt;- 1 == 2, what is the data type of x? Given x &lt;- \"FALSE\", what is the data type of x? 2.7 Structures So far, all of the objects we encountered consist of a single element: a single number, character string, or logical value. But R objects can also contain several elements. Examples of R object that contain multiple elements are vectors (one-dimensional) and data frames (two-dimensional). 2.7.1 Vectors This is a set of elements of the same type. Vectors are constructed with the function c(), which stands for combine. # Vector of numbers x &lt;- c(4, 5, 6) x ## [1] 4 5 6 # Vector of characters y &lt;- c(&quot;Hello&quot;, &quot;Goodbye&quot;) y ## [1] &quot;Hello&quot; &quot;Goodbye&quot; # Vector of logical z &lt;- c(TRUE, FALSE, TRUE) z ## [1] TRUE FALSE TRUE 2.7.2 Apply functions over vectors Many common descriptive statistics functions, like the mean (mean()), variance (var()), median (median()), standard deviation (sd()), maximum (max()), and minimum (min()), take numerical vectors as arguments. In the code below, a vector containing the numbers 3, 5.1, 7, and 1.23 was assigned to the variable vector. Then, the mean of the vector was computed via the mean() function. vector &lt;- c(3, 5.1, 7, 1.23) mean(vector) ## [1] 4.0825 2.7.3 Arithmetic operations on vectors Common mathematical operators, such as +, -, *, /, and ^ can also be used with vectors. The code in the box below performs an operation (in this case, a product) between two vectors of the same length. Run it and examine the result, to understand what this operation means to R. Then, re-assign the value of x to a scalar (a single number). Run the code again and, again, examine the result to understand what this operation means to R. x &lt;- c(2, 3, 4) c(1, 2, 3) * x ## [1] 2 6 12 As you can see, when you multiply two vectors with the same length, R multiplies all of the corresponding elements. The same holds for other operations, such as addition or division. This is called an element-wise operation. When you multiply a vector by a scalar, R multiplies every element of the vector by the scalar. 2.8 Data frames Whereas a vector is a one-dimensional structure, data frames are two-dimensional. You can think of a data frame as a collection of vectors. Data frames are one of R’s most essential data structures. They organise data into a tabular format with rows and columns. If you have some data from an experiment or study, you will typically save it as a table in a format such as .csv, .txt, or .xlsx. When you load those files into R, you will create a data frame object. At the end of this tutorial (section Using R in your own computer) you will learn how to load a data file stored in your computer into R. 2.8.1 Example data frame Instead of loading data from your computer, you can also use an online data set: we will use this one, which is a .csv file hosted on our GitHub. Click on the link and examine the format of the data. Notice that the first row consists of column names, that each column corresponds to a data variable, and that each row corresponds to one observation. These data contain information on oxygen concentrations sampled at Saanich Inlet, British Columbia at several depths and during various seasons. It contains: Season - season in which measurement was obtained; Fall or Summer Depth_m - depth in meters (m) at which measurement as obtained O2_uM - oxygen (O2) concentration in micromolar (µM) Add_data - whether additional microbiological data was collected; TRUE or FALSE For a brief introduction to the data used in our workshops, see Hallam SJ et al. 2017. Sci Data 4: 170158 “Monitoring microbial responses to ocean deoxygenation in a model oxygen minimum zone.” More detailed information on the environmental context and time series data can be found in Torres-Beltrán M et al. 2017. Sci Data 4: 170159. “A compendium of geochemical information from the Saanich Inlet water column.” 2.8.2 Importing the data frame You can import .csv data files into R using read.csv(). Since our file is hosted online, the function’s argument should be a character string containing the link to the data set. In the code box below, we will import the data file using read.csv() and assign it to the variable dat. link &lt;- &quot;https://raw.githubusercontent.com/EDUCE-UBC/educer/main/data-raw/data_intro_ws.csv&quot; dat &lt;- read.csv(link) ou can examine the first few rows of dat using the head() function. Do this in the code box above. The first row, containing the column names, is called header. Each column can be thought of as a vector corresponding to a single data variable. For example, the second row is like a numeric vector, describing the data variable “Depth.” All elements of a column must be of the same data type (just like a vector) Each row corresponds to a single data observation: in this case, each row is a single study site. Data frames are integral to data analysis in R. We will learn how to manipulate data frames later in this tutorial. 2.9 Quick plots There are many data visualization options in R. Some data visualization functions are part of R by default, much like other functions you already encountered, such as log() or mean(). Functions that are part of R by default are called base R. However, the most popular way to create publication-quality figures is to use a package called ggplot2. A package is a collection of functions that enhance R’s capabilities beyond base R. Anyone can create their own functions that solve additional problems, and share them with other users in the form of packages. People can then download those packages and use the functions therein. In this tutorial, we will only scratch the surface of data visualization in R. We will use the function quickplot(), which is part of ggplot2. When you work in R in your own computer, outside of this tutorial, you will need to install and load ggplot2 in order to use the quickplot() function. In the box below, we start by loading our data (like we did in the Structures section), and then we create a simple depth profile (scatterplot) of the Oxygen concentrations with colors indicating the Season that the sample was collected. # load the ggplot2 library library(ggplot2) # Link to data in csv format link &lt;- &quot;https://raw.githubusercontent.com/EDUCE-UBC/educer/main/data-raw/data_intro_ws.csv&quot; # Read csv file dat &lt;- read.csv(link) # Plot data -- needs ggplot2 quickplot(data = dat, x = O2_uM, y = Depth_m, colour = Season, main = &quot;Saanich Inlet: Seasonal oxygen depth profile&quot;) 2.10 Accessing and subsetting data 2.10.1 Acessing vector elements Every vector has a length, which is the number of elements in the vector. In the code box below, we created a vector called x. Using the function length() we can determine the number of elements in the vector: x &lt;- c(4, 5, 6, 7) length(x) ## [1] 4 Since vectors are one-dimensional and have a defined length, you can retrieve their individual values using vector indices. R uses 1-based indexing, meaning the first value in an R vector corresponds to index 1, the second value to index 2, and so on. The index of the final element of the vector is equal to the vector’s length. We can extract the value of the 2nd element of a vector using the square bracket operator [], like in the code box below. Examine the output. It is a single number (or, identically, a vector of length 1). x &lt;- c(4, 5, 6, 7) x[2] ## [1] 5 You can also subset multiple elements of a vector, thus outputting a smaller vector. When you want to subset a single element of the vector, you write the index of that element between square brackets (like in the example above). Similarly, if you want to subset multiple elements, you must provide a vector of indices inside the square brackets. x &lt;- c(4, 5, 6, 7) x[c(2, 4)] # You can use a vector of indices to subset vector x ## [1] 5 7 There is a convenient shortcut to create a vector containing all integers between \\(a\\) and \\(b\\): the : operator. In the code box below, we created two vectors: x and y. The logical operator == confirms that the two vectors are the same. x &lt;- c(4, 5, 6, 7) y &lt;- 4:7 x == y ## [1] TRUE TRUE TRUE TRUE Here we subset the 2nd, 3rd, and 4th elements of x using square brackets and the : operator. Note how the variable x itself did not change: it is still a vector of length 4. x &lt;- c(4, 5, 6, 7) x[2:4] ## [1] 5 6 7 length(x) ## [1] 4 If you want to use a subsetted vector later in your code, you need to save it into a variable using the assignment operator &lt;-, like this: word_vector &lt;- c(&quot;Hello&quot;, &quot;World&quot;) word_vector &lt;- word_vector[1] length(word_vector) ## [1] 1 Note that we redefined the length-2 variable word_vector into a length-1 vector. Had we wanted to keep the length-2 vector available for future use, we could also have assigned the subsetted vector to a new variable name, such as hello_vector &lt;- word_vector[1]. 2.10.2 Accessing data frame elements Since vectors are 1D objects, their elements can be accessed using a single index. To access data frames, which are 2D objects, you need to specify two indices: the row number and the column number. The code below subsets the element located on the 4th row, 3th column. Then, using what you learned in the previous section, modify the code so that it outputs the 5th, 6th, and 7th elements of the third column. # Link to data in csv format link &lt;- &quot;https://raw.githubusercontent.com/EDUCE-UBC/educer/main/data-raw/data_intro_ws.csv&quot; dat &lt;- read.csv(link) # Read csv file # 4th row, 3rd column dat[4, 3] ## [1] 91.115 # How will we get the 5th, 6th, and 7th elements of the 3rd column? It is often more useful to refer to data frame columns by name rather than number. The easiest way to do this is using the $ symbol, which subsets an entire column from the data frame. # get the entire column dat$O2_uM ## [1] 203.533 183.787 130.579 91.115 69.828 26.972 11.066 8.997 6.605 ## [10] 5.933 2.891 2.766 14.465 24.239 28.885 26.766 216.667 159.672 ## [19] 141.778 97.894 44.978 25.807 27.011 34.436 38.012 27.557 32.354 ## [28] 20.446 0.000 0.000 0.000 0.000 Note that the output (which is the entire O2_uM column) is simply an R vector. We can manipulate the dat$O2_uM object just like we would manipulate any other vector. Modify the code so that it outputs only the 5th, 6th, and 7th elements of the O2_uM column. Use the vector subsetting notation [] that you learned in the previous section. If you need help, use the Hint button. # get the 5th, 6th, and 7th elements of the `O2_uM` column dat$O2_uM[5:7] ## [1] 69.828 26.972 11.066 2.10.3 Subsetting rows according to condition Sometimes, we want to access only those observations (rows) of a data frame that obey certain conditions. We can do this with the subset() function, which takes two arguments: (1) the full data frame, and (2) the condition. For example, imagine that, from our data frame, we were only interested in those observations that were measured in the Fall. Then, we could subset the data like so: subset(dat, Season == \"Fall\"). In the Quick plots section, we plotted our data’s oxygen concentration as a function of depth. In that plot, four points had an oxygen concentration equal to zero. Complete the code below (replace the words surrounded by angular brackets &lt; &gt; by the appropriate code) to make a plot that includes only those observations where oxygen concentration is larger than zero: # Link to data in csv format link &lt;- &quot;https://raw.githubusercontent.com/EDUCE-UBC/educer/main/data-raw/data_intro_ws.csv&quot; dat &lt;- read.csv(link) # Read csv file reduced_data &lt;- subset(dat, O2_uM &gt; 0) # Needs ggplot2 package quickplot(data = reduced_data, x = O2_uM, y = Depth_m, colour = Season, main = &quot;Saanich Inlet: Seasonal oxygen depth profile&quot;) Subsetting is extremely useful when working with large data. You can learn more complex subsets in our Tidyverse workshop using the tidyverse packages, which are a popular and widely used suite of R package for working with and visualizing data. 2.11 Additional exercises To practice skills covered in this workshop and prepare for your future R endeavors, please complete the following exercises. When you are done, you can confirm your answers by downloading this script with solutions to exercises 2 through 4 and running it in your computer. Happy coding! 2.11.1 Exercise 1: Install packages In your computer, install the packages used in the next workshop you plan to attend. The R tidyverse: tidyverse, lubridate, cowplot Intermediate R programming: tidyverse, lmerTest, devtools, roxygen2 Statistical models: tidyverse, broom, plyr, lme4, car, lsmeans, MASS, faraway, gapminder, HSAUR3 Reproducible Research: tidyverse, packrat None (but to practice package install anyway): tidyverse Please note that if you have R v3.3 or older, you may not be able to install tidyverse. In this case, you need to separately install each package within the tidyverse. This includes: readr, tibble, dplyr, tidyr, stringr, ggplot2, purr, forcats 2.11.2 Exercise 2: Calculating a probability density Using R’s built-in functions you can perform some complex calculations. For example, the formula for the Probability Density Function (PDF) of the Normal distribution is: \\[y(x) = \\frac{1}{\\sigma \\sqrt{2 \\pi}} \\exp\\bigg(- \\frac{1}{2}\\left( \\frac{x - \\mu}{\\sigma}\\right)^2\\bigg),\\] where \\(\\mu\\) is the mean and \\(\\sigma\\) is the standard deviation. Let’s write a script that calculates the probability density of \\(x\\) given some mean and standard deviation. In your own script, complete the missing code (replacing the code surrounded by angular brackets &lt; &gt;) to finish the calculation. x &lt;- 0.8 mu &lt;- 0 # mean sigma &lt;- 1 # standard deviation 1/(sigma*sqrt(2*pi)) * exp(&lt;enter_code_here&gt;) 2.11.3 Exercise 3: Use the help function to learn how to flip coins R can be used to generate random numbers from different distributions. For example, when you flip some coins you will get a random number of tails (“successes”), drawn from a binomial distribution. In R, you can use the function rbinom() to generate random binomially-distributed numbers. In you own computer or in the box below, use the help function to learn about the rbinom() function. Then, use this function to simulate three observations, where each observation is the number of tails obtained by flipping ten fair coins. Run this command multiple times. Notice how you get a different output each time: this is because the output is a vector random numbers. Assign the output of the function to a variable and then, using one of the functions you learned in this tutorial, calculate the mean number of tails. Because each observation you flip ten fair coins, the expected number of tails is five. But you only perform three observations, so the mean number of tails will probably not be exactly five. Try increasing the number of observations and running the script again. What happens to the mean number of tails as you increase the number of observations? 2.11.4 Exercise 4: Working with data Let’s return to our data set on oxygen concentrations in Saanich Inlet. If you haven’t done so yet, download the data file into your project directory. Then, load this file as a data frame and: Using an R function, determine what data type the Depth_m variable is. Using indexing and the square bracket operator []: determine what depth value occurs in the 20th row return the observation (row) where oxygen equals 91.115. Hint: Use the subset() function. Subset the data to observations where depth is deeper than 100 m. Hint: Use the subset() function. Create a stacked scatterplot of oxygen concentrations within the two different seasons, colored by whether or not microbial data are available. 2.11.4.1 Solution to exercise 2 x &lt;- 0.8 mu &lt;- 0 # mean sigma &lt;- 1 # standard deviation # Normal distribution PDF: 1/(sigma*sqrt(2*pi)) * exp(-1/2*((x - mu)/sigma)^2) ## [1] 0.2896916 dnorm(x, mu, sigma) # Confirm using R&#39;s built-in Normal PDF function ## [1] 0.2896916 2.11.4.2 Solution to exercise 3 # Count nr of tails out of ten coins, three times: nr_tails &lt;- rbinom(n = 3, size = 10, prob = 0.5) mean(nr_tails) ## [1] 3.666667 nr_tails &lt;- rbinom(n = 3000, size = 10, prob = 0.5) mean(nr_tails) ## [1] 4.947667 nr_tails &lt;- rbinom(n = 300000, size = 10, prob = 0.5) mean(nr_tails) ## [1] 4.998407 2.11.4.3 Solution to exercise 4 path &lt;- &quot;https://raw.githubusercontent.com/EDUCE-UBC/educer/main/data-raw/data_intro_ws.csv&quot; dat &lt;- read.csv(path) # Read csv file class(dat$Depth_m) # What type is the Depth_m variable? ## [1] &quot;integer&quot; dat$Depth_m[20] # Depth of the 20th observation ## [1] 60 subset(dat, O2_uM == 91.115) # Observation for which equals 91.115 ## Season Depth_m O2_uM Add_data ## 4 Fall 60 91.115 TRUE subset(dat, Depth_m &gt; 100) # Subset such that depth is deeper than 100 m ## Season Depth_m O2_uM Add_data ## 10 Fall 110 5.933 FALSE ## 11 Fall 120 2.891 TRUE ## 12 Fall 135 2.766 FALSE ## 13 Fall 150 14.465 FALSE ## 14 Fall 165 24.239 FALSE ## 15 Fall 185 28.885 FALSE ## 16 Fall 200 26.766 TRUE ## 26 Summer 110 27.557 FALSE ## 27 Summer 120 32.354 TRUE ## 28 Summer 135 20.446 FALSE ## 29 Summer 150 0.000 FALSE ## 30 Summer 165 0.000 FALSE ## 31 Summer 185 0.000 FALSE ## 32 Summer 200 0.000 TRUE # Stacked scatterplot: quickplot(data = dat, x = Season, y = O2_uM, colour = Add_data, main = &quot;Saanich Inlet: Oxygen in Fall vs. Summer&quot;) 2.12 Survey Please provide us with feedback through this short survey. "],["introduction-to-the-tidyverse-1.html", "Chapter 3 Introduction to the tidyverse 3.1 Introduction 3.2 Data description 3.3 Getting Started 3.4 Data in R 3.5 Data wrangling with dplyr 3.6 Graphics with ggplot2 3.7 Fine-tuning your ggplots 3.8 Saving ggplots 3.9 Additional resources 3.10 Survey", " Chapter 3 Introduction to the tidyverse 3.1 Introduction In this workshop series, we will learn to manipulate and visualize real world data using packages in the tidyverse. You will learn how to: create an R project and import data from a file into R, create subsets of rows or columns from data frames using dplyr, select pieces of an object by indexing using element names or position, change your data frames between wide and narrow formats, create various types of graphics, modify the various features of a graphic, and save your graphic in various formats This comprehensive introduction to the tidyverse workshop series is ideal for beginners but assumes some prior experience with R (such as that in our Introduction to R workshop). 3.1.1 Setup intructions Please come to the workshop with your laptop setup with the required software and data files as described in our setup instructions. 3.1.2 Background Please read Hallam SJ et al. 2017. Sci Data 4: 170158 “Monitoring microbial responses to ocean deoxygenation in a model oxygen minimum zone” to learn more about the data used in this workshop. You can also check out this short video showing how the sampling was done! 3.2 Data description The data in this workshop were collected as part of an on-going oceanographic time series program in Saanich Inlet, a seasonally anoxic fjord on the East coast of Vancouver Island, British Columbia (Figure 1). / Figure 1. Map of Saanich Inlet indicating conventional sample collection stations (S1-S9). Data used in this workshop is sourced from S3. Saanich Inlet is a steep sided fjord characterized by a shallow glacial sill located at the mouth of the inlet that restricts circulation in basin waters below 100 m (Figure 2). / Figure 2. Structure of Saanich Inlet. The glacial sill restricts water circulation into and out of the lower depth of the inlet basin. During spring and summer months, elevated primary production (like photosynthesis) in surface waters combined with restricted circulation results in progressive water column stratification and complete oxygen starvation (anoxia) in deep basin waters. In late summer, pulses of oxygenated nutrient-rich ocean waters upwelling from the Haro Straight cascade over the sill, displacing oxygen starved bottom waters upward. The intensity of these renewal events varies from year to year with implications for microbial ecology and biogeochemical cycles (Figure 3). / Figure 3. Contour plot of water column oxygen concentrations over multiple years in the time series. Warmer colors indicate high oxygen concentrations while cooler colors are low. Note the recurring pattern of oxygen decline below 100 m depth intervals followed by seasonal renewal events in late Summer into early Fall carrying more oxygenated waters into the Inlet. The seasonal cycle of stratification and deep water renewal enables spatial and temporal profiling across a wide range of water column energy states and nutrients, thus making Saanich Inlet a model ecosystem for studying microbial community responses to ocean deoxygenation. Ocean deoxygenation is a widespread phenomenon currently increasing due to climate change. The data we will use in this workshop include various geochemical measurements at many depths in Saanich Inlet. Samples were taken approximately monthly from 2006 to 2014, though there is much missing data to contend with. For a brief introduction to the data used in this workshop series, see Hallam SJ et al. 2017. Sci Data 4: 170158 “Monitoring microbial responses to ocean deoxygenation in a model oxygen minimum zone.” More detailed information on the environmental context and time series data can be found in Torres-Beltrán M et al. 2017. Sci Data 4: 170159. “A compendium of geochemical information from the Saanich Inlet water column.” 3.3 Getting Started 3.3.1 Making an RStudio project Projects allow you to divide your work into self-contained contexts. Let’s create a project to work in. In the top-right corner of your RStudio window, click the “Project: (None)” button to show the projects dropdown menu. Select “New Project…” &gt; “New Directory” &gt; “New Project.” Under directory name, input “intro_tidyverse” and choose a parent directory to contain this project on your computer. 3.3.2 Installing and loading packages At the beginning of every R script, you should have a dedicated space for loading R packages. R packages allow any R user to code reproducible functions and share them with the R community. Packages exist for anything ranging from microbial ecology to complex graphics to multivariate modeling and beyond. In this workshop, we will use several packages within the tidyverse, in particular dplyr, tidyr, and ggplot2. Since these packages are used together so frequently, they can be downloaded and loaded into R all together as the “tidyverse.” We will also use the packages lubridate and cowplot. Here, we load the necessary packages which must already be installed (see setup instructions for details). library(tidyverse) # Easily Install and Load the &#39;Tidyverse&#39; library(lubridate) # Make Dealing with Dates a Little Easier library(cowplot) # Streamlined Plot Theme and Plot Annotations for &#39;ggplot2&#39; library(patchwork) # The Composer of Plots 3.3.3 Downloading the data The following command downloads the data from our GitHub and since you’re working in a Project, saves it in the Project directory under the data directory on your computer. write.csv( read.csv(&quot;https://raw.githubusercontent.com/EDUCE-UBC/educer/main/data-raw/data_tidyverse_ws.csv&quot;), &quot;data/Saanich_Data.csv&quot;, row.names=FALSE) 3.3.4 Loading data with readr The most common formats for medium to medium-large data sets are comma- or tab-separated values (.csv and .tsv/.txt, respectively). In this format, the data is stored in plain text, with one observation per line, and variables within each observation separated by a comma or tab. The readr functions read_csv and read_tsv help read in data at quick speeds compared to base R’s read.csv and read.tsv functions. Furthermore, readr’s data loading functions automatically parse your data into the correct data type (numeric, character, etc). Let’s start by reading in the data we will use for this workshop. Here, we specify that the data file is named “Saanich_Data.csv.” Furthermore, we specify that the first row of the data contain variable names, and that empty, NA, NAN, or ND values should be treated by R as missing data (coded in R as NA). raw_data &lt;- read_csv(file=&quot;data/Saanich_Data.csv&quot;, col_names=TRUE, na=c(&quot;&quot;, &quot;NA&quot;, &quot;NAN&quot;, &quot;ND&quot;)) This is a relatively large data set with 1605 observations and 29 variables. dim(raw_data) ## [1] 1605 29 Let’s get an understanding of the data by looking at the first three rows. head(raw_data, 3) ## # A tibble: 3 x 29 ## Longitude Latitude Cruise Date Depth WS_O2 PO4 SI WS_NO3 Mean_NH4 ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 -124. 48.6 1 2006-02-18 0.01 NA 2.42 NA 26.7 NA ## 2 -124. 48.6 1 2006-02-18 0.025 NA 2.11 NA 23.2 NA ## 3 -124. 48.6 1 2006-02-18 0.04 NA 2.14 NA 19.5 NA ## # … with 19 more variables: Std_NH4 &lt;dbl&gt;, Mean_NO2 &lt;dbl&gt;, Std_NO2 &lt;dbl&gt;, ## # WS_H2S &lt;dbl&gt;, Std_H2S &lt;dbl&gt;, Cells.ml &lt;dbl&gt;, Mean_N2 &lt;dbl&gt;, Std_n2 &lt;dbl&gt;, ## # Mean_O2 &lt;dbl&gt;, Std_o2 &lt;dbl&gt;, Mean_co2 &lt;dbl&gt;, Std_co2 &lt;dbl&gt;, Mean_N2O &lt;dbl&gt;, ## # Std_N2O &lt;dbl&gt;, Mean_CH4 &lt;dbl&gt;, Std_CH4 &lt;dbl&gt;, Temperature &lt;dbl&gt;, ## # Salinity &lt;dbl&gt;, Density &lt;dbl&gt; We see that we have each observation as a row and each measurement (variable) as a column. This data is in what is called wide (or fat) format. 3.4 Data in R 3.4.1 Basic data types in R Here are some of the most common data types in R. We will use and learn more about these data as we work with them throughout this workshop. numeric (1, 1.41, 3.142) character (“dplyr,” “1.41,” “TRUE”) logical (TRUE, FALSE) 3.4.2 Basic data structures in R The above data types are packaged together in data structures. You can see what structures you have loaded into your current RStudio session in the “Environment” panel in the upper right quadrant of RStudio. Generally, you will be working with data frames (i.e. tables). atomic vector zero to many elements of the same data type (and may include NA) factor vectors with a limited number of unique values for example, a vector (“A,” “B,” “B,” “A”) with levels: “A” and “B” used to code categorical variables data frame the most important data structure in R: represents an m by n table of data columns are variables, rows are observations Identifying what type and structure of data you are working with is important because certain functions can only be applied to certain types/structures. 3.4.3 Conditional statements and logical operators Conditional statements and logical operators are important when working with data. We will practice using different conditional statements and logical operators on the oxygen data. In base R, a column vector can be extracted from a data frame using the $ operator. oxygen &lt;- raw_data$WS_O2 We can further subset these data by pulling out specific measurements. In this case, we are using the 710th, 713th, 715th, etc. values from the vector of all oxygen values (1605 in total). oxygen &lt;- oxygen[c(710, 713, 715, 716, 709, 717, 718, 719)] oxygen ## [1] 295.488 204.259 144.362 96.356 NA 40.745 23.731 8.426 Conditional statements: a == b returns true when a matches b oxygen == 204.259 ## [1] FALSE TRUE FALSE FALSE NA FALSE FALSE FALSE a != b returns true if a does not match b oxygen != 204.259 ## [1] TRUE FALSE TRUE TRUE NA TRUE TRUE TRUE a &gt; b returns true if a is greater than b oxygen &gt; 204.259 ## [1] TRUE FALSE FALSE FALSE NA FALSE FALSE FALSE a &lt;= b returns true if a is less than or equal to b oxygen &lt;= 204.259 ## [1] FALSE TRUE TRUE TRUE NA TRUE TRUE TRUE a %in% b returns true if the value of a is in b 204.259 %in% oxygen ## [1] TRUE is.na() returns true in indices where the values of the input are NA (Not Available) is.na(oxygen) ## [1] FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE It is important to remember that these statements give answer of TRUE/FALSE. They do not subset the data to all the TRUE observations. Logical operators: logical NOT !. Find the indices where the data is NOT NA. oxygen ## [1] 295.488 204.259 144.362 96.356 NA 40.745 23.731 8.426 !is.na(oxygen) ## [1] TRUE TRUE TRUE TRUE FALSE TRUE TRUE TRUE logical AND &amp;. Find the indices where the value is &lt;= 120 AND &gt;= 20 oxygen &lt;= 120 &amp; oxygen &gt;= 20 ## [1] FALSE FALSE FALSE TRUE NA TRUE TRUE FALSE logical OR |. Find the indices where the value is &lt;= 50 OR &gt;= 150 oxygen &lt;= 50 | oxygen &gt;= 150 ## [1] TRUE TRUE FALSE FALSE NA TRUE TRUE TRUE 3.4.4 Exercises Using the first 16 depth observations in the depth data in the R Script: Find the indices where depth is greater or equal to 0.055 Check if the value 0.111 is in the depth data Find where depth is less than 0.060 OR depth is greater than 0.140 Check that your answers to these questions are: 3 FALSE, followed by 13 TRUE FALSE 3.4 TRUE, followed by 7 FALSE, followedby 5 TRUE 3.5 Data wrangling with dplyr Typical data wrangling tasks: select a subset of variables (columns) filter out a subset of observations (rows) rename variables arrange the observations by sorting a variable in ascending or descending order mutate all values of a variable (apply a transformation) group_by a variable and summarise data by the grouped variable *_join two data frames into a single data frame While base R can accomplish all of these tasks, base R code is rather slow and can quickly become extremely convoluted. Currently, the most popular alternative for data wrangling is the package dplyr. It is so good at what it does, and integrates so well with other popular tools like ggplot2, that it has rapidly become the de-facto standard and it is what we will focus on today. Compared to base R, dplyr code runs much faster. It is also much more readable because all operations are based on using dplyr functions or verbs (select, filter, mutate…) rather than base R’s more difficult to read indexing system (brackets, parentheses…). Each verb works similarly: input data frame in the first argument other arguments can refer to variables as if they were local objects output is another data frame Before working with our data, we first want to make a copy of the raw data so that we may revert to it quickly if we make any mistakes. This is best practices for data science in general. dat &lt;- raw_data 3.5.1 Select You can use the select function to focus on a subset of variables (columns). Let’s select the variables that we will need for this workshop. Here, we will use our copied raw data (dat) and select the variables: cruise # date depth in kilometers temperature in Celsius salinity in Practical Salinity Units (PSU) density in Sigma-Theta oxygen (O2) in uM nitrate (NO3) in uM hydrogen sulfide (H2S) in uM dat &lt;- select(dat, Cruise, Date, Depth, Temperature, Salinity, Density, WS_O2, WS_NO3, WS_H2S) There are several helper functions that can be used with select to describe which variables to keep: starts_with(x): variable names that start with x ends_with(x): variable names that end with x contains(x): variable names containing x We can use the starts_with helper function to quickly select all of the chemicals that we will be using in this workshop. dat &lt;- select(raw_data, Cruise, Date, Depth, Temperature, Salinity, Density, starts_with(&quot;WS_&quot;)) The full list of helper functions can be found by running ?select_helpers in the console. 3.5.2 Filter You can use filter to select specific rows based on a logical condition of a variable. This is where our practice with R logic comes in handy. Below we filter the data such that we only retain data collected after February 2008 because a different instrument was used prior to this date. Notice that we save this filtered data as “dat” because we want to save it in our R environment and use it throughout this workshop. dat &lt;- filter(dat, Date &gt;= &quot;2008-02-01&quot;) As we saw earlier, conditional statements and logical expressions in R are extremely powerful and allow us to filter the data in almost any way imaginable. For example, we can filter the data to consist of observations taken in June (using months in the lubridate package), at depths of either 0.1 or 0.2, and where nitrate data is not missing. Importantly, lubridate operates under the assumption that you are using English. To check what language R is using, run Sys.getlocale(\"LC_TIME\"). If your computer is running in another language, replace “June” with the equivalent in your language or set R to English with Sys.setlocale(\"LC_TIME\", \"en_CA.UTF-8\"). filter(dat, months(Date) == &quot;June&quot; &amp; Depth %in% c(0.1, 0.2) &amp; !is.na(WS_NO3)) ## # A tibble: 12 x 9 ## Cruise Date Depth Temperature Salinity Density WS_O2 WS_NO3 WS_H2S ## &lt;dbl&gt; &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 22 2008-06-11 0.1 8.43 30.9 24.0 98.1 26.6 0 ## 2 22 2008-06-11 0.2 9.43 31.3 24.2 0 0 5.50 ## 3 34 2009-06-16 0.1 8.20 30.9 24.0 53.0 25.2 0 ## 4 34 2009-06-16 0.2 9.24 31.3 24.2 0 0 3.00 ## 5 46 2010-06-16 0.1 9.03 30.6 23.7 67.0 26.0 0 ## 6 46 2010-06-16 0.2 9.26 31.3 24.2 0 0 20.5 ## 7 58 2011-06-15 0.1 8.57 30.7 23.8 78.0 25.8 0 ## 8 58 2011-06-15 0.2 9.36 31.3 24.2 0 0 14.7 ## 9 70 2012-06-14 0.1 8.40 30.7 23.9 43.1 26.2 0 ## 10 70 2012-06-14 0.2 9.14 31.3 24.2 0 0 13.3 ## 11 84 2013-06-13 0.1 8.59 30.9 23.9 37.8 23.9 0 ## 12 84 2013-06-13 0.2 9.01 31.3 24.2 0 0 1.96 Notice that we are not saving this filtered data as “dat” because we do not want to over-write the data in our environment. We are just using filter to look at the results. This also brings up another best practice in data science. Dates are usually recorded as 1 string like year-month-day. This means we must use an additional package and function to subset by any of these values. In actuality, the best practice is to give year, month, and day in separate columns so that you could filter as filter(dat, month==6). But this is not the most common format so we are showing the lubridate workaround here. 3.5.2.1 Exercise: select and filter Because we will be manipulating the data further, first copy the data “dat” to practice data “pdat” so that what you do in the exercises does not impact the rest of the workshop. Using your pdat data: select the Cruise, Date, Depth, and NO3 variables filter the data to retain data on Cruise 72 where Depth is &gt;= to 0.1 Your resulting pdat object should be a 8x4 data frame. 3.5.3 Rename You can use the rename function to assign new names to your variables. This can be very useful when your variable names are very long and tedious to type. Here, we remove the “CTD_” and “Mean_” prefixes from our geochemical variables. dat &lt;- rename(dat, O2=WS_O2, NO3=WS_NO3, H2S=WS_H2S) This is a good point to touch on best practices for units in data. It is best to either names the units in the variable name, such as O2_uM for oxygen in micromolar, or to create a separate column for units, such as O2 = 3.4 and O2_units = uM. You should never place the units in the same variable as the measurement because then you cannot plot the variable as a number! Here, we have no designations though we’ve noted units in the list in the section on the Select verb. For simplicity’s sake in this workshop, we will simply use the variable names. However, in a full data analysis, you should use variable names such as “O2_uM” and “temperature_C” for clarity. You could rename all these variables accordingly using rename as shown above. 3.5.4 Arrange Use arrange(x) to sort all the rows by the value of the variable x in ascending order. arrange(desc(x)) can be used to sort in descending order. For example, we can arrange the data in the order of ascending NO3 values. dat[1:10, ] ## # A tibble: 10 x 9 ## Cruise Date Depth Temperature Salinity Density O2 NO3 H2S ## &lt;dbl&gt; &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 18 2008-02-13 0.01 7.49 29.6 23.1 225. 18.9 0 ## 2 18 2008-02-13 0.02 7.34 29.7 23.2 221. 24.0 0 ## 3 18 2008-02-13 0.04 7.74 29.9 23.3 202. 29.2 0 ## 4 18 2008-02-13 0.06 7.74 30.1 23.5 198. 24.0 0 ## 5 18 2008-02-13 0.075 7.56 30.2 23.6 194. 23.8 0 ## 6 18 2008-02-13 0.085 8.04 30.4 23.7 150. 23.0 0 ## 7 18 2008-02-13 0.09 8.43 30.6 23.7 122. 21.9 0 ## 8 18 2008-02-13 0.097 8.47 30.6 23.8 108. 20.3 0 ## 9 18 2008-02-13 0.1 8.54 30.7 23.8 99.8 19.4 0 ## 10 18 2008-02-13 0.11 9.12 31.0 24.0 39.6 19.4 0 arrange(dat[1:10, ], NO3) ## # A tibble: 10 x 9 ## Cruise Date Depth Temperature Salinity Density O2 NO3 H2S ## &lt;dbl&gt; &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 18 2008-02-13 0.01 7.49 29.6 23.1 225. 18.9 0 ## 2 18 2008-02-13 0.11 9.12 31.0 24.0 39.6 19.4 0 ## 3 18 2008-02-13 0.1 8.54 30.7 23.8 99.8 19.4 0 ## 4 18 2008-02-13 0.097 8.47 30.6 23.8 108. 20.3 0 ## 5 18 2008-02-13 0.09 8.43 30.6 23.7 122. 21.9 0 ## 6 18 2008-02-13 0.085 8.04 30.4 23.7 150. 23.0 0 ## 7 18 2008-02-13 0.075 7.56 30.2 23.6 194. 23.8 0 ## 8 18 2008-02-13 0.02 7.34 29.7 23.2 221. 24.0 0 ## 9 18 2008-02-13 0.06 7.74 30.1 23.5 198. 24.0 0 ## 10 18 2008-02-13 0.04 7.74 29.9 23.3 202. 29.2 0 3.5.5 Mutate Use mutate(y=x) to apply a transformation to some variable x and assign it to the variable name y. Here we multiply Depth by 1000 to convert its units from kilometers to meters. dat &lt;- mutate(dat, Depth=Depth*1000) 3.5.5.1 Exercise: rename and mutate Using the practice data (pdat): Select the Date, Depth and O2 variables from pdat using select Rename the O2 variable to Oxygen using rename Keep August observations where Oxygen is non-missing using filter, months, and !is.na Transform Oxygen from micromoles/L to micrograms/L using mutate (multiply Oxygen by 32) Run the provided ggplot() code to create a scatterplot of Oxygen vs Depth You should obtain the following plot: ## `geom_smooth()` using formula &#39;y ~ x&#39; If you are attending a 3 x 2-hour workshop, this is the end of day 1 3.5.6 Piping with %&gt;% Recall the basic dplyr verb syntax: input data frame in the first argument other arguments can refer to variables as if they were local objects output is another data frame Our geochemical data cleaning code continuously overwrites the “dat” object every time we call a dplyr verb. Instead, we can chain commands together using the %&gt;% operator. f(x) %&gt;% g(y) is the same as g(f(x),y) select(dat, Cruise) is the same as dat %&gt;% select(Cruise) Piping works nicely to condense code and to improve readability. Dplyr syntax is very easy to read. Starting with the raw_data, we select the Cruise, Date, …, WS_H2S variables, filter out the data prior to February 2008, rename chemical variables, and then multiply Depth by 1000. dat &lt;- raw_data %&gt;% select(Cruise, Date, Depth, Temperature, Salinity, Density, WS_O2, WS_NO3, WS_H2S) %&gt;% filter(Date &gt;= &quot;2008-02-01&quot;) %&gt;% rename(O2=WS_O2, NO3=WS_NO3, H2S=WS_H2S) %&gt;% mutate(Depth=Depth*1000) 3.5.6.1 Exercise: pipes Rewrite your ocde from the previous exercise using pipes Pipe your data into the ggplot function 3.5.7 Group by and Summarise summarise (or summarize) is handy when we want to calculate summaries for groups of observations. This is done by first applying the group_by verb and then feeding it into summarise. For example, we can calculate the mean, standard deviation, and sample size of oxygen concentrations by depth as follows. dat %&gt;% group_by(Depth) %&gt;% summarise(Mean_O2=mean(O2, na.rm=TRUE), SD_O2=sd(O2, na.rm=TRUE), n=n()) ## # A tibble: 24 x 4 ## Depth Mean_O2 SD_O2 n ## * &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 10 211. 55.8 83 ## 2 20 187. 50.3 83 ## 3 40 159. 38.3 83 ## 4 60 138. 45.2 83 ## 5 75 105. 54.8 82 ## 6 80 32.3 NA 1 ## 7 85 81.9 48.8 82 ## 8 90 68.2 42.1 82 ## 9 97 49.1 34.7 82 ## 10 100 43.6 33.1 83 ## # … with 14 more rows 3.5.7.1 Exercise: group by and summarise Using the dat data set, calculate the median, interquartuile range and the sample size of Temperature by depth. If you are attending a 2 x 3-hour workshop, this is the end of day 1 3.5.8 Wide and long data - gather and spread Wide and long describe two different formats for a data frame. Wide data is where each variable is given its own column. Narrow data is where one column contains all of the variable names, and another column contains all of the values of these variables. For example, this wide data ## sample_ID year_2015 year_2016 year_2017 ## 1 1 0.288 1.140 1.051 ## 2 2 0.788 0.246 0.957 ## 3 3 0.409 0.728 1.457 ## 4 4 0.883 1.092 0.953 contains the same information as this long data. ## sample_ID Year Value ## 1 1 year_2015 0.288 ## 2 2 year_2015 0.788 ## 3 3 year_2015 0.409 ## 4 4 year_2015 0.883 ## 5 1 year_2016 1.140 ## 6 2 year_2016 0.246 ## 7 3 year_2016 0.728 ## 8 4 year_2016 1.092 ## 9 1 year_2017 1.051 ## 10 2 year_2017 0.957 ## 11 3 year_2017 1.457 ## 12 4 year_2017 0.953 Our data is in the wide format. Observe that Temperature, Salinity, Density, O2, NO3, and H2S are given their own columns. head(dat) ## # A tibble: 6 x 9 ## Cruise Date Depth Temperature Salinity Density O2 NO3 H2S ## &lt;dbl&gt; &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 18 2008-02-13 10 7.49 29.6 23.1 225. 18.9 0 ## 2 18 2008-02-13 20 7.34 29.7 23.2 221. 24.0 0 ## 3 18 2008-02-13 40 7.74 29.9 23.3 202. 29.2 0 ## 4 18 2008-02-13 60 7.74 30.1 23.5 198. 24.0 0 ## 5 18 2008-02-13 75 7.56 30.2 23.6 194. 23.8 0 ## 6 18 2008-02-13 85 8.04 30.4 23.7 150. 23.0 0 Some R functions work better with wide data and others work better with long data. gather and spread are two functions that enable easy conversion between the formats. The first will gather your wide data into one column containing all of the variable names (the “Key” column) and another column containing all of the values (the “Value” column). The second function will spread your long data into the wide data format. Here we gather our data from wide to long format, specifying that we do not want to gather Cruise, Date, and Depth. The factor_key argument specifies that we want to maintain the ordering of the columns, so gather will automatically mutate the Key column into a factor with levels in our desired ordering. dat &lt;- gather(dat, key=&quot;Key&quot;, value=&quot;Value&quot;, -Cruise, -Date, -Depth, factor_key=TRUE) # store the variable names as an ordered factor head(dat) ## # A tibble: 6 x 5 ## Cruise Date Depth Key Value ## &lt;dbl&gt; &lt;date&gt; &lt;dbl&gt; &lt;fct&gt; &lt;dbl&gt; ## 1 18 2008-02-13 10 Temperature 7.49 ## 2 18 2008-02-13 20 Temperature 7.34 ## 3 18 2008-02-13 40 Temperature 7.74 ## 4 18 2008-02-13 60 Temperature 7.74 ## 5 18 2008-02-13 75 Temperature 7.56 ## 6 18 2008-02-13 85 Temperature 8.04 head(dat$Key) ## [1] Temperature Temperature Temperature Temperature Temperature Temperature ## Levels: Temperature Salinity Density O2 NO3 H2S We can undo this by spreading our data from long to wide format. dat &lt;- spread(dat, key=&quot;Key&quot;, value=&quot;Value&quot;) head(dat) ## # A tibble: 6 x 9 ## Cruise Date Depth Temperature Salinity Density O2 NO3 H2S ## &lt;dbl&gt; &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 18 2008-02-13 10 7.49 29.6 23.1 225. 18.9 0 ## 2 18 2008-02-13 20 7.34 29.7 23.2 221. 24.0 0 ## 3 18 2008-02-13 40 7.74 29.9 23.3 202. 29.2 0 ## 4 18 2008-02-13 60 7.74 30.1 23.5 198. 24.0 0 ## 5 18 2008-02-13 75 7.56 30.2 23.6 194. 23.8 0 ## 6 18 2008-02-13 85 8.04 30.4 23.7 150. 23.0 0 3.6 Graphics with ggplot2 ggplot2 is an add-on package to R. It is an alternative to base graphics that has become very popular, to the point where it is recommended/preferred unless you have old code that already uses a different graphing package. ggplot2 documentation is available at docs.ggplot2.org 3.6.1 Why ggolot? Wilkinson, Grammar of Graphics (1999) Ggplot2 is an implementation of GoG for R Benefits: handsome default settings snap-together building block approach automatic legends, colours, facets statistical overlays: regressions lines and smoothers (with confidence intervals) Drawbacks: it can be hard to get it to look exactly the way you want requires having the input data in a certain format 3.6.2 ggplot building blocks data: 2D table (data.frame) of variables aesthetics: map variables to visual attributes (e.g., position) geoms: graphical representation of data (points, lines, etc.) stats: statistical transformations to get from data to points in the plot(binning, summarizing, smoothing) scales: control how to map a variable to an aesthetic facets: juxtapose mini-plots of data subsets, split by variable(s) guides: axes, legend, etc. reflect the variables and their values Idea: independently specify and combine the blocks to create the plot you want. There are at least three things we have to specify to create a plot: data aesthetic mappings from data variables to visual properties a layer describing how to draw those properties 3.6.3 geom_point The point geom is used to create scatterplots. Let’s examine the relationship between Oxygen (O2) and Nitrate (NO3). The first argument of ggplot is the data, hence we pipe our data into the ggplot function. The second argument is the aesthetics, which is used by all subsequent geoms. We then add a geom_point to the ggplot object, specifying that we want size 1 points. Here, geom_point takes the aesthetics of the ggplot object and understands that we want to plot oxygen on the x-axis and nitrate on the y-axis. dat %&gt;% ggplot(aes(x=O2, y=NO3)) + geom_point(size=1) ## Warning: Removed 212 rows containing missing values (geom_point). Note that above R code has a warning. Warnings do not always indicate that there is an error in your code. For example, this warning tells us that there is missing data in our data frame, which we already know. We are specifically interested in this relationship because oxygen is the most energetically favourable terminal electron acceptor for microbial metabolism in this system. However, oxygen is depleted in deeper waters as a result of the seasonal stratification. So what are microbes using to survive? As oxygen is depleted, microbes must use alternative terminal electron acceptors. In Saanich Inlet, nitrate is the next best option so we want to see if nitrate is available to microbes when oxygen is depleted. Do you see any patterns in the above plot? Is nitrate available when oxygen is at low concentrations? We are also interested in hydrogen sulfide (H2S) because sulfate (SO4) is the third best terminal electron acceptor in this system. If sulfate is being used, it will be reduced to hydrogen sulfide and we will see a build up of H2S when both oxygen and nitrate are depleted. Is this the case? Complete the next exercise to find out! 3.6.3.1 Exercise: geom_point Using the dat data set: Investigate the relationship between O2 and H2S Investigate the relationship between NO3 and H2S If you are attending a 3 x 2-hour workshop, this is the wnd of day 2 Perhaps we can get a better understanding of what is happening at Saanich Inlet if we colour the points by the corresponding Hydrogen Sulfide concentration. Ggplot plots the observations in the order that they appear in the data frame. Since most of the H2S values are zero, we want to plot the non-zero data last. To do this, we use arrange to sort the rows by ascending H2S values. To the ggplot, we add an additional colour aesthetic. This indicates to colour our points by the corresponding value of Hydrogen Sulfide. dat %&gt;% filter(!is.na(H2S)) %&gt;% arrange(H2S) %&gt;% ggplot(aes(x=O2, y=NO3, colour=H2S)) + geom_point(size=2.5) What do you notice about the relationship between oxygen, nitrate, and hydrogen sulfide? What might this tell you about what the microbes are doing in this system? We can also group points by a categorical variable. Let’s select the Cruise, Depth, O2, and H2S variables. Then, let’s filter to retain information on Cruise 72, and gather the data into the long format. We chose this cruise because it has accompanying microbial data that is used in ECOSCOPE’s upcoming summer workshops on amplicon sequencing and metagenomic analysis! Next, we create a depth-profile scatterplot comparing concentrations (x-axis) versus depth (y-axis) by chemical (shape). We also specify to reverse the y-axis with scale_y_reverse in order to obtain a depth profile where the top of the water column is at the top of the plot. dat %&gt;% select(Cruise, Depth, O2, H2S) %&gt;% filter(Cruise==72) %&gt;% gather(key=&quot;Chemical&quot;, value=&quot;Concentration&quot;, -Cruise, -Depth) %&gt;% ggplot(aes(x=Concentration, y=Depth, shape=Chemical)) + geom_point() + scale_y_reverse(limits=c(200, 0)) This plot shows the same trend as we saw in our previous oxygen vs. hydrogen sulfide plot. However, now we can see that not only does H2S only accumulate when O2 is depleted, but this only occurs at depths greater the 150 meters. 3.6.3.2 Exercise: color and aesthetics It may be difficult to differentiate between the different shapes in the previous plot so modify the previous code to add colors to the shapes as well. 3.6.4 geom_line The line geom connects observations in the order that they appear in the data. It is especially useful when plotting time series. Here we combine the point and line geoms to plot a time series of Oxygen at a depth of 200 meters. dat %&gt;% select(Date, Depth, O2) %&gt;% filter(Depth == 200 &amp; !is.na(O2)) %&gt;% gather(key=&quot;Chemical&quot;, value=&quot;O2 Concentration&quot;, -Date, -Depth) %&gt;% ggplot(aes(x=Date, y=`O2 Concentration`)) + geom_point() + geom_line() We can further layer geoms with to compare oxygen and hydrogen sulfide at 200 m over time. Here, we also keep the H2S data and mutate it to be negative such that is plots below the oxygen data. dat %&gt;% select(Date, Depth, O2, H2S) %&gt;% filter(Depth == 200 &amp; !is.na(O2) &amp; !is.na(H2S)) %&gt;% mutate(H2S=-H2S) %&gt;% gather(key=&quot;Chemical&quot;, value=&quot;Concentration&quot;, -Date, -Depth) %&gt;% ggplot(aes(x=Date, y=Concentration, colour=Chemical)) + geom_point() + geom_line() As in our depth profiles, we also see a relationship between oxygen and hydrogen sulfide at a single depth over time. Again, hydrogen sulfide only accumulated when oxygen is absent. In this case, this is because hydrogen sulfide (H2S) reacts with oxygen (O2) to form water (H2O) and sulfur dioxide (SO2). So both compounds cannot co-exist at one depth as they will quickly react and deplete each other until only 1 remains (the one with the highest original concentration). 3.6.5 geom_histogram The histogram geom is useful for visualizing the distribution of a continuous variable. It accomplishes this by dividing the x-axis into bins and counting the number of observations in each bin. By default, geom_histogram divides the x-axis evenly into 30 bins. Let’s examine the distribution of Oxygen at depths less than 100. dat %&gt;% filter(Depth &lt; 100) %&gt;% ggplot(aes(x=O2)) + geom_histogram() ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. We see that in general, oxygen is below ~250 uM in the first 100 m of the water column. And the distribution of oxygen at depths greater than or equal to 100. dat %&gt;% filter(Depth &gt;= 100) %&gt;% ggplot(aes(x=O2)) + geom_histogram() ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. Now we is that oxygen is much lower below 100 m, with most values being &lt; 25 uM. This makes sense as we know that oxygen is depleted as we go deeper in Saanich Inlet. We expect a different pattern for nitrate since it is less favourable in microbial metabolism. Complete the following exercise to find out how nitrate is distributed across the entire depth profile. What patterns do you see? 3.6.5.1 Exercise: geom_histogram Investigate the distribution of nitrate across all depths Test out different values for the bins argument (“bins=”) 3.6.6 geom_boxplot Boxplots let us visualize how a continuous variable varies with the levels of a categorical variable. Let’s examine the distribution of concentrations of different chemicals in July at depths greater than or equal to 150 . dat %&gt;% select(-Temperature, -Salinity, -Density) %&gt;% filter(months(Date) == &quot;July&quot; &amp; Depth &gt;= 150) %&gt;% gather(key=&quot;Chemical&quot;, value=&quot;Concentration&quot;, -Cruise, -Date, -Depth, factor_key=TRUE) %&gt;% ggplot(aes(x=Chemical, y=Concentration)) + geom_boxplot() In Saanich Inlet, July is right before the renewal season so, we expect the water to be the most stratified at this time of the year, thus resulting in low oxygen, low nitrate, and high hydrogen sulfide at lower depths. Remember, this occurs because the water hasn’t mixed in many months and microbes have used up the more favourable oxygen and nitrate in the water. We see that, indeed, this is the case in the above plot. What might you expect in the middle of the mixing period, like November? 3.6.7 facet_wrap Rather than putting a lot of information in a single graphic, we can split the graphic into panels by certain features. This is called faceting. We can make depth profiles for all six variables Temperature, Salinity, Density, Oxygen, Nitrate, and Hydrogen Sulfide. Here we gather the data into the long format and pipe it into the ggplot function. As with the previous scatterplots, we set the aesthetics in the ggplot function, specifying that we want the variable values on the x-axis, and the depth on the y-axis. We plot these data with geom_point, and create a depth-profile by reversing the y-axis. Lastly we call facet_wrap to separate the single depth-profile into six depth profiles – one for each variable. dat %&gt;% gather(key=&quot;Key&quot;, value=&quot;Value&quot;, -Cruise, -Date, -Depth, factor_key=TRUE) %&gt;% ggplot(aes(x=Value, y=Depth)) + geom_point(size=1) + scale_y_reverse(limits=c(200, 0)) + facet_wrap(~Key, ncol=2, dir=&quot;v&quot;, scales=&quot;free_x&quot;) Now we can really get a picture of what is happening in Saanich Inlet! Do the patterns match what we’ve seen in previous plots? Are there any patterns that surprised you? 3.6.8 labels We can change the labels of the previous plot with the labs function, specifying exactly what we want as the title, x-, and y-axis labels. You will have seen the title option in a couple of plots prior to this in the workshop. dat %&gt;% gather(key=&quot;Key&quot;, value=&quot;Value&quot;, -Cruise, -Date, -Depth, factor_key=TRUE) %&gt;% ggplot(aes(x=Value, y=Depth)) + geom_point(size=1) + scale_y_reverse(limits=c(200, 0)) + facet_wrap(~Key, ncol=2, dir=&quot;v&quot;, scales=&quot;free_x&quot;) + labs(title=&quot;Saanich Inlet Depth Profiles (2008-2014)&quot;, x=&quot;&quot;, y=&quot;Depth (m)&quot;) We saw in previous analyses that oxygen and nitrate might have a relationship. But we’ve also seen that depth and many compounds are related. So how might we tease apart this plot from earlier? ## Warning: Removed 212 rows containing missing values (geom_point). In the following exercise, investigate if depth contributes to the relationship between oxygen and nitrate. We will look at only a couple of depths to simplify the plot. Do oxygen and nitrate correlate? Is this correlation depth dependent? 3.6.8.1 Exercise: facets filter to data at depths of 10, 60, 100, or 200 Plot oxygen vs nitrate faceted by depth withouot providing arguments for dir or scales 3.7 Fine-tuning your ggplots ggplot is quite flexible. However, fine-tuning plots usually requires much more code. Let’s work to create a beautiful multi-panel figure showing the key compounds across depth as well as the overall relationship between oxygen, nitrate, and hydrogen sulfide. We will create two plots (p1 and p2) and then incorporate these into 1 figure. 3.7.1 Themes By default, ggplot uses a gray colour scheme. We can create a clean black and white theme and save it to a ggplot object. There are also many pre-made themes. my_theme &lt;- theme_bw() + theme(panel.grid.major=element_blank(), panel.grid.minor=element_blank()) 3.7.2 Panel 1 Let’s add the theme to the faceted plot and remove the x-axis label with labs and save the ggplot as an object “p1.” p1 &lt;- dat %&gt;% gather(key=&quot;Key&quot;, value=&quot;Value&quot;, -Cruise, -Date, -Depth, factor_key=TRUE) %&gt;% ggplot(aes(x=Value, y=Depth)) + geom_point(size=1) + scale_y_reverse(limits=c(200, 0)) + facet_wrap(~Key, ncol=2, dir=&quot;v&quot;, scales=&quot;free_x&quot;) + my_theme + labs(x=&quot;&quot;, y=&quot;Depth (m)&quot;) p1 3.7.3 Panel 2 Similarly, let’s add our theme to the scatterplot of Oxygen versus Nitrate versus Hydrogen Sulfide and save the ggplot as an object “p2.” p2 &lt;- dat %&gt;% filter(!is.na(H2S)) %&gt;% arrange(H2S) %&gt;% ggplot(aes(x=O2, y=NO3, colour=H2S)) + geom_point(size=2) + my_theme + labs(x=&quot;O2 in uM&quot;, y=&quot;NO3 in uM&quot;) + scale_colour_continuous(name=&quot;H2S in uM&quot;) p2 3.7.4 Multi-panel figues The gridExtra package is used for creating multi-panel figures. Let’s use the package’s primary function grid.arrange to arrange our plots p1 and p2 into a multi-panel figure. We can use the layout_matrix argument to specify exactly how to arrange our plots. p &lt;- cowplot::plot_grid(p1, p2, labels=c(&quot;A&quot;, &quot;B&quot;), rel_widths=c(2/5, 3/5)) ## Warning: Removed 826 rows containing missing values (geom_point). ## Warning: Removed 203 rows containing missing values (geom_point). p And sometimes after all this, it’s still not perfect. So, don’t feel bad if you need to finalize plots for publication in another editor like Illustrator. Sometimes it’s a necessary evil. 3.7.4.1 Patchwork Alternatively, we can use the patchwork package (developed in 2019). Patchwork makes it ridiculously simple to combine separate ggplots into the same graphic. It solves the same problem that gridExtra::grid.arrange() and cowplot::plot_grid but much simpler and intuitive. patchwork_plot &lt;- p1 + p2 patchwork_plot + plot_annotation(tag_levels = &#39;A&#39;) ## Warning: Removed 826 rows containing missing values (geom_point). ## Warning: Removed 203 rows containing missing values (geom_point). 3.8 Saving ggplots Using ggsave you can save your ggplots to many different file types just by changing the extension in the file name. Let’s save our ggplot to the projects folder as a pdf. ggsave(&quot;saanich.pdf&quot;, p, width=10, height=6) 3.9 Additional resources In this workshop, we have gone over just some of the functions and graphics within the tidyverse. Below are some resources for further learning and practice! R cheatsheets also available in RStudio under Help &gt; Cheatsheets Introduction to dplyr dplyr tutorial dplyr video tutorial More functions in dplyr and tidyr ggplot tutorial 1 ggplot tutorial 2 ggplot tutorial 3 ggplot maker (shinyRGT) 3.10 Survey Please provide us with feedback through this short survey. "],["reproducible-research-1.html", "Chapter 4 Reproducible research 4.1 Introduction 4.2 Prior to the workshop 4.3 Why reproducibility? 4.4 R projects 4.5 R scripts 4.6 Modularize by task 4.7 Use Make 4.8 Reproducible R environment with packrat 4.9 Documents made reproducible 4.10 Git 4.11 Git on your computer 4.12 Collaborate with GitHub 4.13 Additional resources 4.14 Survey", " Chapter 4 Reproducible research 4.1 Introduction In this series, we introduce computational reproducibility and its importance to modern research. We will teach the general principles for reproducible computer-based analyses, along with specific methods and tools for reproducibility with RStudio. We will also introduce version control with GitHub, which is an essential tool in a team environment. You will learn how to: Construct reproducible, automatable workflows in R Create reproducible documents using Rmarkdown to include underlying code / computations with relevant graphical and statistical results in several different formats (reports, presentation slides, handouts, notes) Use Git version control Integrate version control with GitHub for both personal and group projects This is an intermediate workshop series that assumes prior experience with R (such as that in our Introduction to R workshop). 4.2 Prior to the workshop 4.2.1 Setup instructions Please come to the workshop with your laptop setup with the required software and data files as described in our setup instructions. 4.2.2 Background Please read Hallam SJ et al. 2017. Sci Data 4: 170158 “Monitoring microbial responses to ocean deoxygenation in a model oxygen minimum zone” to learn more about the data used in this workshop. You can also check out this short video showing how the sampling was done! 4.2.3 Terminology Reproducibility is purported to be at the heart of science. But what do we mean when we say “reproducible research?” In general, researchers think of replicating an experiment and obtaining the same result (direct replicability). However, what we should actually strive for is true reproduciblity, reaching the same conclusions through different methods. Moreover, we usually think of the reproducibility of wet lab experiments. It is a somewhat newer movement for researchers to try to make analysis and computational methods reproducible. In this workshop, what we are really working toward is repeatability, the ability to exactly repeat a computational analysis. These methods can then be build upon by other researchers or yourself to find true reproducibility. 4.3 Why reproducibility? It helps you remember what you did. Your closest collaborator is you 6 months ago. And past you doesn’t answer email. ~ Karl Broman, UW-Madison Even if no other human ever reads your code, it is worth making it reproducible for your future self. It is like a classical lab notebook. You wouldn’t expect yourself to remember how many ng of DNA you used in a PCR reaction 2 weeks ago, right? So, you shouldn’t expect yourself to remember exact computational workflows. It prevents human error. If you do anything by hand once, you’ll likely do it a hundred times… and five of those times, you’ll make a mistake. The easiest way around this is to let a computer complete the repetitive tasks. So, this means you should NOT do anything analysis-related with a mouse (cough Excel cough). It helps you communicate with others. Past you may be your closest collaborator, but you likely have other people you work with and you probably want to communicate your science to the larger scientific community at some point (#PublishOrPerish). Making your code reproducible makes it accessible to others for their understanding of your research as well as potentially for their use in their own work. It is faster. This almost goes without saying because a computer can do pretty much anything computational faster than a human. But also, speed comes with repetition. If your code, report, slides, etc. are reproducible, then when you need to make a small change or produce a similar document in the future, that can be done quickly and with minimal effort on your part. 4.4 R projects One of the most common hurdles to code reproducibility is file paths. If you have all your code and data in ~/Documents and someone copies it all to their ~/Desktop, then any links to files in your code will be broken. For example, read.table(file=\"~/Documents/mydata.csv\") will not work for the the other person because they do not have that file in that place. The level 1 solution to this is to start your code by setting the working directory like setwd(&quot;~/Documents&quot;) Thus, others need only change this one line of code to be able to use your script. But we can do one better in R by eliminating the need for any file paths at all with R projects. When you create a project, RStudio creates an .Rproj file that links all of your files and outputs to the project directory. When you import data, R automatically looks for the file in the project directory instead of you having to specify a full file path on your computer like ~/Documents. R also automatically saves any output to the project directory. Finally, projects allow you to save your R environment in .RData so that when you close RStudio and then re-open it, you can start right where you left off without re-importing any data or re-calculating any intermediate steps. RStudio has a simple interface to create and switch between projects, accessed from the button in the top-right corner of the RStudio window. (Labelled “Project: (None),” initially.) Let’s create a project to work in for this workshop. Start by click the “Project” button in the upper right or going to the “File” menu. Select “New Project” and the following will appear. You can either create a project in an existing directory or make a new directory on your computer - just be sure you know where it is. After your project is created, navigate to its directory using your Finder/File explorer. You will see the .RProj file has been created. To access this project in the future, simply double-click the .RProj file and RStudio will open the project or choose File &gt; Open Project from within an already open RStudio window. 4.5 R scripts The next level of reproducibility in R is to use R scripts. You should never* input commands into the R console because they are not saved there. Instead, input them into an R script and run them from the script. Then, you have a record of all the steps you performed in an analysis. *Well… not actually never. If you’re using R as a fancy calculator, you’re probably fine to perform some math in the console. And you likely don’t want to save the time you use the help function ?. But pretty much everything else should go in a script. Let’s practice this. Create a new R script under File &gt; New File &gt; R script and input the following code # load packages ## Suite of packages for data manipulation and visualization library(tidyverse) # load data from our GitHub dat &lt;- read.csv(&quot;https://raw.githubusercontent.com/EDUCE-UBC/educer/main/data-raw/data_intro_ws.csv&quot;) # create plot of oxygen by depth O2_plot &lt;- quickplot(data=dat, x=O2_uM, y=Depth_m, colour=Season, main=&quot;Saanich Inlet: Seasonal oxygen depth profile&quot;) # save the plot ggsave(&quot;O2_plot.png&quot;) Save the file under the name “O2_plot.R.” Now we can execute our workflow a couple of different ways. We can: open it in RStudio and run it source it from the R console with source(\"O2_plot.R\") run it from the terminal open the terminal (GitBash for Windows users) navigate to your project directory type Rscript O2_plot.R and press Enter If we scripted our workflow right, running the script will produce the O2_plot.png file in our project directory. We can re-run it multiple times, (try it after deleting the plot), and it will always recreate the output. Congratulations, you’ve automated your workflow! 4.6 Modularize by task Keeping all of our analysis in one file is okay for a small example like this one. However, large analyses should be broken up into multiple scripts, each completing one step (like data cleaning, statistical analysis, plotting, etc.) and saving the output to a file on your computer. Then, you can create a driver script that is just a list of source commands calling all your individual scripts. Why go through this extra effort? Modular scripts allow you to: run individual steps manually if you need to get intermediate results in files that you can look at, save, and share execute another tool outside of R for a particular step in your workflow (e.g. python, Git, etc.) easily replace the driver with a full-blown workflow engine like Make (more on this later). What would this look like for our example? The overall driver script would be RootScript.R # load in data and packages source(&quot;load.R&quot;) # Create plot source(&quot;plot.R&quot;) And the individual step scripts could be load.R # load data dat &lt;- read.csv(&quot;https://raw.githubusercontent.com/EDUCE-UBC/educer/main/data-raw/data_intro_ws.csv&quot;) write.csv(dat, &quot;data.csv&quot;) Note that we now need to save the data object to our computer with write.csv since it needs to be saved somewhere for use in the next script in the workflow. plot.R # load packages # Suite of packages for data manipulation and visualization library(tidyverse) # read in data dat &lt;- read.csv(&quot;data.csv&quot;) # create plot of oxygen by depth O2_plot &lt;- quickplot(data=dat, x=O2_uM, y=Depth_m, colour=Season, main=&quot;Saanich Inlet: Seasonal oxygen depth profile&quot;) # save the plot ggsave(&quot;O2_plot.png&quot;) Note that loading of the tidyverse package has been moved from the start of our single script to the start of the plot.R script. Packages need to be loaded in the script(s) that use them, not all at once in the first script in your workflow. Now we can run our driver script in the terminal with Rscript RootScript.R. This will run first load.R then plot.R, thus outputting the desired plot. You can easily see how this might scale up when you have dozens of scripts for a project! 4.7 Use Make Make is an automation tool used to build executable programs and workflows. While it was originally designed to build software (link Linux), it now is widely used to manage projects where you have a number of input files and scripts that create a number of output files. Make is language independent; so, it can work with scripts for any program. Make is basically just one level more advanced than driver scripts in that is not only runs a list of scripts, it also manages versions of all the associated files and keeps track of dependencies. Make builds dependencies efficiently. If an input file hasn’t changed since its dependents were built, steps needed to build them don’t need to be executed. In contrast, a driver script will run all steps no matter what. Make’s syntax provides a documentation of your project’s workflow. Because you must list dependencies up-front in a Makefile, it’s easy to see what depends on what. This may not seem like a big deal in a straight pipeline as the one in our example, but once you have a web or network of dependencies, it’s a lot easier to visualize what’s going on by representing it this way. Make is succinct. Make allows expressing dependency patterns (e.g. each .PNG file is produced from the data in file X.csv by running Rscript on the .R file of the same name), so that you don’t have to write out the recipe for building every single plot. Make easily scales up. If down the line, you add another plot to the outputs produced by your workflow, and use the same naming scheme, it will get picked up and automatically built the exact same way. So how do we use Make? We specify our workflow in a Makefile (note the capitalization). Please note that Make syntax is not the same as R. Tabs and spaces have very different and particular uses as you will see below. The general Makefile formula is below where the horizontal space before “command” must be a tab. target1: dependency1 dependency2 dependencyN command --option argument(s) target2: dependency1 dependency2 dependencyN command --option argument(s) For our example workflow, the Makefile will look like this. You can make this file in any text editor, though working in the terminal with something like nano, Vmacs, etc. is best to avoid accidental formatting put in place by programs like Word. all: O2_plot.png data.csv: load.R Rscript load.R O2_plot.png: data.csv plot.R Rscript plot.R Note that if you copy-paste the above, you will need to manually edit the Makefile such that the horizontal spaces before “Rscript” are produced by a tab rather than spaces. As with scripts, there are several way to execute your Makefile. You can: run it directly in the terminal open the terminal (GitBash for Windows users) navigate to your project directory type make -f Makefile and press Enter This runs the make program on your Makefile use a driver script containing the command system(\"make -f Makefile\") In this case, you need to use the system() function to tell R to run the function in the terminal instead of in R For the sake of best practices, the later is better since you then have a record of how you executed the Makefile. So update RootScript.R with system(&quot;make -f Makefile&quot;) And then execute your updated driver script in the terminal with Rscript RootScript.R. This should output the following as it runs through each R script in your Makefile. Voila! The plot appears in your directory. Rscript load.R Rscript plot.R ── Attaching packages ─────────────────────────────────────── tidyverse 1.2.1 ── ✔ ggplot2 2.2.1 ✔ purrr 0.2.5 ✔ tibble 1.4.2 ✔ dplyr 0.7.5 ✔ tidyr 0.8.1 ✔ stringr 1.3.1 ✔ readr 1.1.1 ✔ forcats 0.3.0 ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ── ✖ dplyr::filter() masks stats::filter() ✖ dplyr::lag() masks stats::lag() Saving 7 x 7 in image 4.7.0.1 Exercise Alter a script and rerun make. Change the plotting script slightly by adding xlab and ylab to rename the axes. Then, rerun make from the terminal. What step(s) get executed? Add to the current workflow with make. Create a new script with the same plot now colored by whether or not microbial data exists for each sample (Add_data). Add this script to your Makefile. Rerun make to produce both plots at once. To make sure your code is executing successfully each time, it is best to delete the .png files each time so you can easily see the new output. 4.8 Reproducible R environment with packrat You could be the most detailed and reproducible coder out there but unfortunately, there are often aspects of your workflow that are out of your hands. The most common of these in R is packages. New versions can change functions or even remove them. Parameters change and are added. As a result, your beautiful, reproducible code is bust if someone (including you) tries to use it with the the wrong version of a package. In theory, version numbers should help you keep an eye on this with the general setup of MAJOR.MINOR.PATCH: MAJOR version when you make incompatible API changes MINOR version when you add functionality in a backwards-compatible manner PATCH version when you make backwards-compatible bug fixes However, not all authors follow this setup religiously and while old R package versions are available on CRAN in the archive area, it is tedious to get them. Fear not! R users realized this issue and developed the packrat package to deal with it. In conjunction with an R project, packrat records all the packages and versions used in that project. It also creates a private package library for the project, in which it will install a copy of all the required packages so that your projects are completely isolated from each other, even if they use mutually incompatible versions of other packages. As you continue to work on a project and perhaps use additional packages, update packages you are already using, or remove the need for a given package, packrat keeps track of all this to keep the project’s private library up-to-date and as small as possible. In general, it is not necessary to initiate packrat while you are actively working on a project on your own (since you know whether or not your code works on your machine with your versions of everything). However, it can be useful to use it right from the start if you are actively collaborating with others. Once you’ve installed the packrat package, you have the option to initiate it when you make a New Project. Otherwise, you an initiate packrat in an existing project like so. # Package version control library(packrat) packrat::init() For those who are unfamiliar with R’s :: syntax, this allows you to call functions from a specific package. It is required when more than one package uses the exact same name for a function. In this case, we use it to specify that we want R to use the init function from packrat as opposed to init from Git. Your console may print out a bunch of errors about fontspec and typesetting engine if you initiate packrat from a project with Rmarkdown. This is a current issue with latex in R and can be ignored at this time. As you work on your project, you can check whether your current project matches all the versions in your saved packrat library, packrat::status() Update the library with new packages or versions as needed, packrat::snapshot() Or remove unneeded packages from the library. packrat::clean() packrat also provides functions for “bundling” up your project and all its dependencies into a single file, which can be copied to another computer and “unbundled,” instantly reproducing your environment on a new machine. packrat::bundle() packrat::unbundle() Of note, most of these functions can also be done from the packrat menu that appears in the Packages tab in the lower right quadrant of RStudio. While we’re on the topic of versions, you also need to record the version of R as well as your current computing platform (operating system and hardware architecture) to ensure reproducibility. You can easily print all this information in R with sessionInfo(). 4.9 Documents made reproducible 4.9.1 What is a document really? There are two main aspects of creating a document: content and appearance. These two features are bundled together in a markup language, so-called because authors used to “mark up” physical documents with instructions on how to display the various parts e.g. title size, indentations, justification, etc. We don’t often think about these as two separate things as it is customary to simultaneously create both with programs like MS Word and PowerPoint where the markup language is hidden behind GUI features and buttons. However, in the background, what you always have is a source file with content and instructions for appearance that can be rendered into a viewable document. Think of the source file as a pie recipe with ingredients list while the viewable document is the pie that results from following that recipe. To be reproducible, you should setup your files such that you only need to modify the source file and re-render to change the viewable document. This is not possible with programs like Word because you never truly see the source file. Moreover, if you have inserted static content like figures, you’d need to go into another program to edit those and re-paste them into the source file. Not very reproducible at all! So, we will explore tools for creating this source file! Some powerful markup languages for developing source files are: LaTeX (through beamer) for PDF or slides Markdown (not up) for PDF, HTML, or slides We will focus on markdown (.md) as it pertains to reproducible workflows in R. 4.9.2 Rmarkdown Markdown itself (outside of R) is a flexible language for creating source documents. It is simple to learn, easy to read, and very widely used. Rmarkdown (.Rmd) is merely markdown with a few added features used in RStudio. Rmarkdown can be rendered to the following viewable document types. This is done using the knitr package with the Knit button in the upper left quadrant menu bar. html_document word_document pdf_document (requires LaTeX) ioslides_presentation: slides using HTML beamer_presentation: PDF slides (requires LaTeX) In Rmd, you have the header and the body. header: overall document settings in a language called YAML (one of the added features of .Rmd as compared to .md) body: content and appearance instructions in markdown If you are attending a 3 x 2-hour workshop, this is the end of day 1 4.9.2.1 YAML header Let’s create a new Rmarkdown document (File &gt; New File &gt; R Markdown…). Leaving the defaults (document, html output), our .Rmd file is already populated with a minimal YAML header like below. --- title: &quot;Untitled&quot; author: &quot;Your Name&quot; date: &#39;2018-06-22&#39; output: html_document --- The header must be at the start of the document and be encased by ---. As you can see, the basic YAML syntax is appearanceInfo: content. You can expand the header to customize your rendered document’s appearance. For example, here is the header for this notes document. --- title: &quot;Reproducible research&quot; author: &quot;Kim Dill-McFarland&quot; date: &quot;version August 23, 2021&quot; output: html_document: toc: yes toc_float: collapsed: no pdf_document: toc: yes latex_engine: xelatex --- There are a few things here that you may find useful in your future reports. The date section is filed in with dynamic code that will automatically access your computer and input the current date in the format “version month day, year.” There is information for rendering in either html or pdf formats, though only the one on top will be rendered each time. Under the html format, we have specified that we want a table of contents (toc) that floats as you scroll through the rendered html document and does not collapse into only high level headings. Under the pdf format, we have also specified to use a table of contents, though floating and collapsing are not options in this format. In addition, we’ve changed the rendering language to be xelatex as opposed to the default pdflatex. In this case, we did this to make packrat work with Rmd. 4.9.2.2 Exercise: YAML In your new .Rmd, replace the automatically populated header and contents with the following. Don’t worry about the contents of the body yet; we’ll go over that next. --- title: &quot;Learning Rmarkdown&quot; author: &quot;Your Name&quot; date: &quot;version August 23, 2021&quot; output: html_document: toc: yes toc_float: collapsed: no pdf_document: toc: yes latex_engine: xelatex --- # Header 1 ## Subheading 1A # Header 2 ## Subheading 2A ## Subheading 2B In the date section, play around with capital versus lowercase letters in the '%B %d, %Y' portion. Try to get the date in the format MO/DY/YR when you render (knit) the document. Change toc_float: collapsed: no to toc_float: yes. What happens to the table of contents? Remove toc_float: collapsed: no. What happens to the table of contents? Change toc: yes to toc: no. What happens to the table of contents? 4.9.3 Knitting To render your .Rmd, that is to take all the appearance and content information in the source file (.Rmd) and make the viewable document (html, PDF, etc.), you use knitr in R. This is most commonly done by clicking the “Knit” button with the blue ball of yarn in the menu above your file window (upper right quadrant). Note: The first time you knit a document, RStudio will prompt you to install several packages. 4.9.4 Markdown body 4.9.4.1 Headings and Subheadings The body of Rmd is organized by headings with #. The more # in a row there are, the farther in and smaller a heading will be. By default, only down to ### are shown in the table of contents. 4.9.4.2 Vertical spacing Headings automatically get vertical spaces around them. In your other content, you can separate 2 paragraphs with a half pt space with double spaces at the end of the first paragraph. Or you can separate with with a full pt space with 2 hard returns at the end of the first paragraph. This is separated by double spaces while this is separated by 2 hard returns. Note that a single hard return is simply treated as a space and the two paragraphs will run together. 4.9.4.3 Text/font *italics* or _italics_ **bold** or __bold__ superscript^2^ subscript~2~ ~~strikethrough~~ looks like code 4.9.4.4 Links To websites: [link](www.rstudio.com) To local images: ![caption](path/to/image.png) To web images: ![caption](http://example.com/image.png) 4.9.4.5 Lists * unordered list * item 2 + sub-item 1 + sub-item 2 1. ordered list 2. item 2 + sub-item 1 + sub-item 2 4.9.4.6 Formulae Use $ to denote formulas. Inline formulas are surrounded by single $ while new line formulas use $$. $a_1^2 + a_2^2 = a_3^2$ gets typeset to \\(a_1^2 + a_2^2 = a_3^2\\). $$ \\bar{x} = \\frac{1}{n} \\sum_{i=1}^n x_i $$ gets typeset as \\[ \\bar{x} = \\frac{1}{n} \\sum_{i=1}^n x_i \\] For more math symbols, see here. 4.9.5 Static documents All that we’ve gone over thus far contributes to Rmd static documents, or those that don’t change every time you render them. For example, our workshops README is in md and is simply a collection of text and .png images. There is no real “code” so-to-speak. It may seem silly to write static documents in markdown if you’re already familiar with programs like MS Word. However, as you will see when we get to Git, there are some distinct advantages to markdown when it comes to version control. More on this later! 4.9.5.1 At home exercise: Rmarkdown Convert your notes from this workshop into an .Rmd. Try use use headings, bold/italic, links, etc. 4.9.6 Dynamic documents The true power of Rmd for reproducible research comes with dynamic documents. These not only contain static content like text and images but also code chunks that run R code (and other languages) and display the outputs, be they descriptive, statistical, graphical, etc. This means you don’t need to copy-paste your analysis results into a document for reporting. No more fiddling with Excel graphs pasted into Word! No more completely remaking reports from scratch! No more collaborators asking how exactly you made that figure! Some people even write their entire thesis in markdown, so anything is possible! 4.9.6.1 Code chunks An R code chunk is formatted as ```{r} Your code here ``` For example, we can read in the data and packages for this workshop in our Rmarkdown file with ```{r} # load packages ## Suite of packages for data manipulation and visualization library(tidyverse) # load data dat &lt;- read.csv(&quot;https://raw.githubusercontent.com/EDUCE-UBC/educer/main/data-raw/data_intro_ws.csv&quot;) ``` Chunks have lots of customizable options that impact how their output looks in the rendered document like whether or not to show the underlying R code or warnings or messages, figure size, etc. You can also use other languages in code chunks. For example, python scripts can be inserted and run directly in an Rmarkdown by creating the corresponding chunk. ```{python} Your code here ``` As of 2018, R supports chunks in Bash, python, Rcpp, SQL, and Stan. 4.9.6.2 Inline code Small bits of code can also be placed inline with `r your code here` For example, you many be writing an abstract and want to state “We obtained geochemical data for X samples, Y of which had corresponding microbial data.” You could fill in the numbers yourself but what if you get more data later on? What if you remove samples that upon preliminary analysis, are not high enough quality? Well, then you’d have to go back and edit your abstract by-hand. Or markdown could do it for you! With the data we used previously in this workshop, you could instead write “We obtained geochemical data for nrow(dat) samples, nrow(filter(dat, Add_data==TRUE)) of which had corresponding microbial data.” which renders to “We obtained geochemical data for 32 samples, 10 of which had corresponding microbial data.” Now if you change the number of samples in the data frame with all your samples (dat), your abstract will fix itself with a simple re-render. 4.9.6.3 Exercise: dynamic Rmd Create a new .Rmd and replicate part of our Introduction to R workshop found in the dynamic_R_challenge.html document provided. If you are attending a 2 x 3-hour workshop, this is the end of day 1 4.9.7 Slides made reproducible Slides can also be created in .md/.Rmd either as static documents or with dynamic code. The formatting in the body is the same except each header ## indicates a new slide lists can be revealed incrementally using &gt; - for bullets There are several rendering options for Rmd slides with examples of the default appearances. It is quite difficult to change the overall look of markdown slides so it is best to pick a render package that is near the look you want. You can specify your render package of choice as the output in the YAML header of the .Rmd. --- title: author: date: output: ioslides_presentation --- As the syntax of slides is the same as documents, we will not go into detail during this workshop but checkout the slides .Rmd in the workshop material to see how slides were made for this workshop! 4.9.7.1 At home exercise: Slides The next time you have to give lab meeting, present in a class, lead a discussion, etc., try making your slides in Rmd. Once you get some practice, it take no more time than PowerPoint! 4.9.8 Rmd in your workflow To incorporate Rmd into your reproducible workflow, you can render an .Rmd document in your Make file with rmarkdown::render(\"input_file.Rmd\"). 4.9.9 Publish with Rpubs.com Rpubs.com is a free online service from RStudio that let’s you put your Rmarkdown documents online with a push of the button. After you register online, you can simply click the “Publish” button (blue dot with 2 blue curves around it) in the upper right corner of the toolbar for your markdown document in RStudio. This sends the rendered html to the site. For some examples, see here. 4.10 Git 4.10.1 What is version control? Version control is any means of recording changes to file(s) over time so that you can recall or revert back to specific versions later. One method of version control you may have employed in the past is dated or versioned file names like myFile_2018.06.25.txt or myFile_v1.2.txt. While these systems can get the job done, you end up will a long list of files that take up storage space and have little information on how they differ. It remains up to you to remember or tell someone what date or version to go back to for whatever purpose is needed. And heaven forbid you stamp a file with final! http://www.phdcomics.com/comics/archive.php?comicid=1531 4.10.2 Why Git? There is a better way to version control with distributed systems, and Git is one such language/program. While Git can be used to track changes for any file type, it is most useful with text-based files like .txt, .Rmd, .R, etc. because these are source files with the appearance directions clearly noted and not hidden behind a GUI. Think of Git like the “track changes” feature of MS Word, except there is a record of the changes after you “accept” them. The advantages of distributed version control over naming methods are that: it is easy to see what changed when (time-stamped), why (commented), and by whom (author identified) you can jump back to any point in time since the file’s creation, not just versions you deemed important enough to save at the time you have the freedom to experiment, try something crazy even, because you can always go back to the last known good version you can work on 2+ different versions in parallel you are able manage contributions from multiple people (see later section on GitHub) Specifically, we are using Git, as opposed to another program, because it: is very fast, even for projects with hundreds of files and years of history doesn’t require access to a server is already extremely popular 4.11 Git on your computer All Git functions are accessed from the command line with git followed by the function name like git init. You do not need to open a specific program other than your terminal. The commands used in this section are summarized in the following infographic. Please refer to it as we progress through the examples. 4.11.1 Initialize a repo (git init) Git will not implement version control on your files until you tell it to do so. This is accomplished by initializing a repository with init. First, create a directory on your desktop named TestRepo. Then, open your terminal/GitBash, navigate to the TestRepo directory, and initialize it as a repo with git init You should see an output similar to the following showing that Git knows this directory is a repo. Initialized empty Git repository in /Users/kim/Desktop/TestRepo/.git/ If you now list all the files in the TestRepo directory with ls -a, you will see a .git file that designates that this directory is a Git repo. Nothing about the directory itself has changed; Git just now knows to track it for version control. 4.11.2 Add files (git add) Create a README.md using a text editor of your choice and include the following as the contents of this file. This is a README for my first Git repo. Now that we’ve added a file, we can query if there are any differences between our local TestRepo directory and the version controlled repo with git status On branch main No commits yet Untracked files: (use &quot;git add &lt;file&gt;...&quot; to include in what will be committed) README.md nothing added to commit but untracked files present (use &quot;git add&quot; to track) We see that README.md simply being in a repo does not automatically tell Git to track this file; it remains “untracked.” So, we must add the file to Git tracking. We can either add just this file specifically with git add README.md Or if we had more than 1 file to add at once, we could add all with git add . Now we see with git status that our README is tracked but not committed. On branch main No commits yet Changes to be committed: (use &quot;git rm --cached &lt;file&gt;...&quot; to unstage) new file: README.md 4.11.3 Git staging area (index) If the file is tracked but not committed (i.e. version saved), where is it? Well, Git has a staging area (also called the index) where a version of a file is stored prior to it being more permanently saved as a committed version. This is useful because sometimes you may want to: save a version as a back-up in the short-term but do not want to permanently save that version in the long-term complete many small changes separately but save them all together as 1 new version compare your current version with the last committed version without causing any conflicts in your repo 4.11.4 Commit changes (git commit) Once you have a version you want to save for all time, you need to commit it from the index to the repo. It is best practices to include a message with each commit using git commit -m \"Your message here\". You wouldn’t believe the number of commits with a message simply of “typo!” We will use the following to commit our first version of TestRepo with the README. git commit -m &quot;Initialize README&quot; [main (root-commit) 151a8dc] Initilaize README 1 file changed, 1 insertion(+) create mode 100644 README.md You receive a summary of how many files were changed and what those changes where (though exact ID numbers will be specific to your repo). Now with git status, we see that our local directory, index, and repo are all identical. On branch main nothing to commit, working tree clean 4.11.4.1 Exercise: Git add and commit Change a file and commit the new version. Add the following to your README.md file, where you fill in your name. My name is Kim Dill-McFarland. Using a combination of git status, add, and commit, determine if your version matches your repo and commit the new version with a comment as needed. Note the messages Git outputs in the terminal as you complete each step. They are very informative and helpful! 4.11.5 Comapre versions (git status and diff) Since we are working with a single, small file in a small repo, it is easy to remember what versions are where. However, as you make larger files and repos, sometimes you will not know this. So, Git has functions for comparing versions. Your git status should be up-to-date after the previous exercise. Now let’s create some differing versions in our local, index, and repo. Add your birthday to README.md and git add . this change but do not commit. Then add your favorite color and do not add or commit this change. So, now we have versions the following versions. local: name, birthday, color index: name, birthday repo: name We can determine what version is where with various parameters in git diff. Added lines are indicated by +, deleted lines by -, and changed lines by showing both versions inline. git diff: local vs. index This is a README for my first Git repo. My name is Kim Dill-McFarland. My birthday is Nov 29. +My favorite color is purple. git diff --cached HEAD: index vs. repo This is a README for my first Git repo. My name is Kim Dill-McFarland. +My birthday is Nov 29. git diff HEAD: local vs. repo This is a README for my first Git repo. My name is Kim Dill-McFarland. +My birthday is Nov 29. +My favorite color is purple. And so if you are unsure which version you are using (local) or which one you want to revert to, diff can show you. 4.11.6 Past commits (git log and show) You’ll notice in the diff commands referencing the repo, it is referred to as HEAD. This is the most recent commit version in the repo and you can go back farther with HEAD~##. Let’s add and commit our current changes so that we have multiple commits to look at. Since we never committed the version with only the birthday added, this version is lost with a new git add. We can see all past commits in this repo with git log commit 0eafc7f9ac7b1f9adb239e730aead874a7888854 (HEAD -&gt; main) Author: Kim Dill-McFarland &lt;kdillmcfarland@gmail.com&gt; Date: Tue Jun 26 14:46:42 2018 -0700 Add birthday and fav color commit d4f6f1869c9846ba1809e58e3c5608af79383549 Author: Kim Dill-McFarland &lt;kdillmcfarland@gmail.com&gt; Date: Tue Jun 26 14:28:23 2018 -0700 Add name commit a2f0948cd60b3c1e17aaaf45fad966cf924e085d Author: Kim Dill-McFarland &lt;kdillmcfarland@gmail.com&gt; Date: Tue Jun 26 14:27:02 2018 -0700 Initialize README Or view a specific past version using HEAD~##. For example, we can view our first commit which is HEAD~2 git show HEAD~2 Author: Kim Dill-McFarland &lt;kdillmcfarland@gmail.com&gt; Date: Tue Jun 26 14:27:02 2018 -0700 Initialize README diff --git a/README.md b/README.md new file mode 100644 index 0000000..862c717 --- /dev/null +++ b/README.md @@ -0,0 +1 @@ +This is a README for my first Git repo. 4.11.7 Branches (git checkout -b) One of the best ways to avoid the need to undo anything in Git is to use branches. These are copies of your repo that you can work on independently of the “main” without impacting the main but still version control changes as you go. You can then choose to merge the branch with the main if you want to keep those changes or abandon the branch and go back the to the main if you do not want to keep those changes. You may have noticed that many of our previous Git outputs start with On branch main. This is because by default, we were working on the main branch of the project (the only branch up to this point). Branches are particularly useful to: test new features without breaking the current working code collaborate in parallel follow a risky idea that has the potential to break your working code develop the same base code into 2+ other applications To start a new branch, first make sure that everything is up-to-date with git status. Then, checkout or create and move into a new branch using the branch -b parameter. git checkout -b new_branch main With git status, we see that we are in the new branch. On branch new_branch nothing to commit, working tree clean Now we can make changes in this branch without impacting the main branch. Go ahead and edit README.md and pick a new favorite color. Then add and commit this change in your new_branch. The two branches no longer match so we must merge them and decide which version(s) to keep. To do so, we must Move back into the main branch git checkout main Merge the branches git merge new_branch which overwrites the older main branch with our new changes in the new_branch 4.11.8 Merge conflicts (git merge) Merging works simply if only 1 of the branches being merged has be altered since it branched off. If both branches contain non-synonymous changes, there will be a conflict. You resolve a conflicts by merging, opening the file(s) with conflict(s), editing until you have the version you want, and saving the new, correct merged version with add and commit. 4.11.8.1 Exercise: branches and conflicts Create a new_branch2 (since new_branch was already merged into main). Edit README.md in new_branch2 by adding your middle name. Add and commit to this branch. Edit README.md in main by removing your last name. Add and commit to this branch. Merge main and new_branch2. Open README.md and look for markers &lt;&lt;&lt;&lt;&lt;&lt;, =======, and &gt;&gt;&gt;&gt;&gt;&gt; to denote conflicts. Edit to your name of choice, making sure to remove the markers. Add and commit to main. Conflict solved! 4.11.9 Summary of Git Thus far, we have covered the main Git terms and functions that you will need to enact version control on your own computer (summarized below). 4.11.9.1 Terms Repository: the contents (directory/folder) of a project including all history and versions for all files in that project Commit: a snapshot of the state of the repository at a given point in time including the author, time, and a comment Staging/Index: files marked for inclusion in the next commit as whatever version that they were at the time they were staged, not necessarily the most up-to-date version on your computer Branch: a named sequence of commits kept separate from the original “main” sequence Merge: including changes from one branch into another Conflict: parallel changes to the same section(s) of a file that can’t be automatically merged 4.11.9.2 Commands status: query for differences in versions in local, index, or repo init: initialize a repo add: add files to the index (staging) prior to committing commit -m: save changes in the index as a time-stamped version with comment (message) diff (HEAD) (--cached HEAD): compare file versions in the local vs. index (local vs. repo) (index vs. repo) log: show list of previous commits in the current repo show: show a detail account of a previous commit checkout: create and move between branches or the local, index, and repo Not shown here but checkout HEAD can be used to move to a past commit and re-instate file versions from that commit merge: combine changes from a branch into the main repo (may result in conflicts) To checkout more functionalities in Git, see a list of all Git commands. 4.12 Collaborate with GitHub GitHub is a web service for hosting Git repositories - content and version history. It has web and GUI interfaces as well as many tools for facilitating collaboration within repos. Similar to our decision to use Git, GitHub finds much of its strength in that is is widely popular and easy to use. If you make the choice to use Git, then GitHub is the natural choice for hosting and collaboration. 4.12.1 Configure your computer (git config) Prior to this workshop, you should have signed-up for a free GitHub account. You now need to link your computer to your GitHub account using the email from this account and whatever name you want stamped on your commits. This need only be done once on your computer. git config --global user.name &quot;Your Name&quot; git config --global user.email &quot;youremail@email.com&quot; 4.12.2 Make an online repo Next we will make an online GitHub repo to link our local repo to. In a browser, navigate to your GitHub account homepage and click “New repository.” Name the repo TestRepo and create it. GitHub then gives you the URL for this repo. 4.12.3 Link local and online repo (git remote) Finally, we will link our TestRepo to the online GitHub repo using the URL provided. You link it to your local directory and repo by using your specific repo URL in the following command in your terminal. Make sure your terminal is still pointing at the correct directory! git remote add origin https://remote_repository_URL 4.12.4 Update an online repo (git push) If no errors occur, you can now upload your local repo to GitHub with push. The first time you push, you need to specify to GitHub that you are pushing from the main repo. git push -u origin main All subsequent push commands for this repo can simply be git push If you go back to your browser and refresh your page, you should now see README.md in your online repo. Once your local repo is fully linked with its online GitHub repo, you can go about version control as you normally would (change files &gt; add &gt; commit). When you have a committed version of the repo that you want to backup or share to GitHub, you git push. 4.12.5 Copy an online repo (git clone) If you are the only one working on your repo and only work on it from 1 computer, then GitHub acts as a one-way street. You push things there for storage, sharing, etc. and should only need to pull them back down if something terrible happens to your computer. However, GitHub also allows work on a repo from multiple computers and by multiple different users. You or others can copy an online GitHub repo to your computer with git clone URL.of.the.repo 4.12.6 Update from online repo (git fetch and pull) Now when you have multiple copies of a repo potentially being worked on and pushed to GitHub from multiple computers or people, the version on GitHub may be more up-to-date than the one you have on the computer you happen to currently be working on. To update you local repo from GitHub, you must go through a staging area (index) similar to the one in-between your local directory and repo. You can place GitHub content in this index with git fetch And use git status to compare your local and GitHub versions. Then, if you decide you want to update your local version, you reverse push with git pull 4.12.6.1 exercise: multiple repo users Now we will similate what is would be like to collaborate on the same repo with another person. In pairs: UserA: Use git clone to copy the TestRepo from UserB. UserB: Make a change to README.md in your repo on your own computer and push that change to your GitHub. UserA: Update your copy of UserB’s repo. Switch roles! 4.12.7 GitHub GUI After all this command line work, we now reveal to you that you can use Git in GUI form. While the GUI is useful for simple commit, push, pull activities, some functions (like hard resets, undos, etc.) are still only available via command line. We encourage you to explore both the command line and GUI forms of Git as you practice. You can explore all the repos on your computer, see specific changes to text based files, and add, commit, push from the GUI. 4.12.8 Git in RStudio Working in an R project in a directory that is also a Git repo will reveal a Git tab in the upper right quadrant of RStudio. To see this, close and then move your workshop project directory into your TestRepo. Re-open the project (from .Rproj) and navigate to this Git tab. You will see that similar to the GitHub GUI you can see diff and commit changes directly from RStudio as well as see a summary (M: modify, D: delete, ?: add) of changes in the project since your last commit. 4.13 Additional resources 4.13.1 Reproducible research Software Carpentry including lessons on Git, Rmd, and reproducible research Riffomonas including reproducible research methods in R 4.13.2 R/RStudio packrat tutorial Rmarkdown cheatsheet Rmarkdown tutorials Git in RStudio 4.13.3 Git/GitHub Git cheatsheet List of all Git commands Git tutorial Git Pro EBook Git help Build a web site from a repo! Explore private repositories and organizations 4.14 Survey Please provide us with feedback through this short survey. "],["statistical-models-in-r-1.html", "Chapter 5 Statistical models in R 5.1 Introduction 5.2 Experimental design 5.3 Analysis of Variance (ANOVA) 5.4 Linear regression 5.5 Cautions when using linear models 5.6 Multiple linear regression 5.7 Linear Mixed Effects models 5.8 Generalized Linear models 5.9 Survey", " Chapter 5 Statistical models in R 5.1 Introduction In this workshop, we introduce various types of regression models and how they are implemented in R. We cover linear regression, ANOVA, ANCOVA, and mixed effects models for continuous response data, logistic regression binary response data, as well as Poisson and Negative Binomial regression for count response data. You will learn: the different functions used to build a statistical model in R, the assumptions behind the different models, how the formula object in R is used to specify all the model terms, how to interpret the main effects and interaction terms in a model, various experimental design concepts that help maximize power. This is an intermediate workshop series that assumes prior R experience including RStudio projects and the R tidyverse. 5.1.1 Setup instructions Please come to the workshop with your laptop setup with the required software and data files as described in our setup instructions. 5.1.2 Data description Unlike our other workshops, ‘Statistical Models’ utilizes several data sets in order to accurately demonstrate the use of statistical tests. You will find more information on each of these data sets in its relevant section(s) within the notes below. 5.1.3 Making an RStudio project Projects allow you to divide your work into self-contained contexts. Let’s create a project to work in. In the top-right corner of your RStudio window, click the “Project: (None)” button to show the projects drop-down menu. Select “New Project…” &gt; “New Directory” &gt; “New Project.” Under directory name, input “statistical_models” and choose a parent directory to contain this project on your computer. 5.1.4 Installing and loading packages At the beginning of every R script, you should have a dedicated space for loading R packages. R packages allow any R user to code reproducible functions and share them with the R community. Packages exist for anything ranging from microbial ecology to complex graphics to multivariate modeling and beyond. In this workshop, we will use many different packages. Here, we load the necessary packages which must already be installed (see setup instructions for details). library(tidyverse) # Easily Install and Load the &#39;Tidyverse&#39; library(broom) # Convert Statistical Objects into Tidy Tibbles library(broomExtra) # Enhancements for &#39;broom&#39; and &#39;easystats&#39; Package Families library(plyr) # Tools for Splitting, Applying and Combining Data library(lme4) # Linear Mixed-Effects Models using &#39;Eigen&#39; and S4 library(car) # Companion to Applied Regression library(lsmeans) # Least-Squares Means library(MASS) # Support Functions and Datasets for Venables and Ripley&#39;s MASS library(faraway) # Functions and Datasets for Books by Julian Faraway library(gapminder) # Data from Gapminder library(HSAUR3) # A Handbook of Statistical Analyses Using R (3rd Edition) # Set global plot theme theme_set(theme_classic()) 5.2 Experimental design 5.2.1 Balanced designs A balanced design has equal (or roughly equal) number of observations for each group Balance eliminates confounding factors and increases power 5.2.2 Blocking factors and random effects Blocking factors and random effects should be used/recorded to control sources of variation that exist but are otherwise not of interest 5.2.3 Randomization Subjects should be randomized to groups to help balance the unobserved factors in the experiment. Randomization should be done in a way to keep the controlled and observational factors (blocking factors and random effects) balanced. 5.3 Analysis of Variance (ANOVA) ANOVA is used when you have data with: a quantitative response/dependent variable (\\(Y\\)) such as: height salary number of offspring one or more categorical explanatory/independent variable(s) (\\(X\\)’s) such as: eye color sex genotype at a given locus For example, you would use ANOVA to address questions like: Does diet has an effect on weight gain? response variable = weight gain (e.g. kg) explanatory variable = type of diet (e.g. low vs. medium vs. high sugar) Does the type sexual relationship practiced influence the fitness of male Red-winged Blackbirds? response variable = fitness of male bird (e.g. # eggs laid) explanatory variable = sexual relationship (e.g. monagamy vs. polygamy) 5.3.1 Key assumptions ANOVA is robust to the non-normality of sample data Balanced ANOVA (equal sample size between groups) is robust to unequal variance ANOVA is sensitive to sample independence 5.3.2 The gist of the math When we run an ANOVA on these data, we calculate a test statistic (F-statistic) that compares the between group variation with the within group variation. \\[F = MSB/MSW\\] where \\(MSB\\) = Mean Square Between and \\(MSW\\) = Mean Square Within Essentially, if there is greater variation between the groups than within the groups we will get a large test statistic value (and correspondingly, a small p-value) and reject that null hypothesis (\\(H_0\\): population means of all groups are equal). If you want to delve into ANOVA in more depth, checkout this video tutorial. 5.3.3 1-way ANOVA with 2 groups Now let’s run some ANOVAs on real data! There are two perfectly acceptable statistical tests in R that we could apply to compare data in 2 groups. The first, which you may be very familiar with, is the t-test. The second is the topic of our lesson today, Analysis of Variance (or ANOVA). Interestingly, the t-test is really a special case of ANOVA that can be used when only comparing 2 groups. ANOVA is a more generalizable test that we will later see can be used with &gt; 2 groups as well as more than one factor/category column. 5.3.3.1 Load and explore the data The first experiment we are going to analyze was done to address the question of whether sexual activity effects the longevity of male fruit flies. To assess this, we are going to use a modified version of the fruitfly data from the faraway R package. Our hypothesis for this experiment are as follows: Null Hypothesis, \\(H_{0}\\): Sexual activity has no effect on the population mean longevity of male fruit flies. Alternative Hypothesis, \\(H_{A}\\): Sexual activity has an effect on population mean longevity of male fruit flies. Let’s now load the fruit fly data from the faraway package, and create our categorical groups from the numerical variables. If you are unfamiliar with the functions below, checkout our R tidyverse workshop. # Read in the data from faraway package data(&quot;fruitfly&quot;) # Create categorical groups and subset data fruitfly_2groups &lt;- fruitfly %&gt;% # Convert factors to character variables mutate_if(is.factor, as.character) %&gt;% # Create 2-level activity variable mutate(activity = ifelse(activity %in% c(&quot;isolated&quot;, &quot;one&quot;, &quot;many&quot;), &quot;no&quot;, &quot;yes&quot;)) %&gt;% # Create 2-level size variable (for later ANOVA) mutate(thorax = ifelse(thorax &lt;= 0.8, &quot;short&quot;, &quot;long&quot;)) %&gt;% # Subset to equal group sizes for activity and size group_by(activity, thorax) %&gt;% sample_n(20) When we explore these data (ignoring thorax data for now)… # get number of rows and columns dim(fruitfly_2groups) ## [1] 80 3 # view first 6 records of data head(fruitfly_2groups) ## # A tibble: 6 x 3 ## # Groups: activity, thorax [1] ## thorax longevity activity ## &lt;chr&gt; &lt;int&gt; &lt;chr&gt; ## 1 long 70 no ## 2 long 72 no ## 3 long 75 no ## 4 long 81 no ## 5 long 90 no ## 6 long 96 no We see that there are 2 columns of interest, longevity and activity. longevity is a quantitative variable, and is our response/dependent variable for this experiment. activity, on the other hand, is a categorical variable, and is our single explanatory/independent variable for this experiment. There are 2 levels to the activity variable, yes and no. Let’s visualize these data to get some intuition as to whether or not there is a difference in longevity between the male flies that are sexually active and those that are not. Given that our sample sizes are not too large, the most informative plot we can make are strip plots. We will also add the mean to these: # plot raw data points for each group as a transparent grey/black point # overlay mean as a red diamond ggplot(fruitfly_2groups, aes(x = activity, y = longevity)) + geom_jitter(position = position_jitter(0.15), alpha = 0.7) + stat_summary(fun = mean, geom = &quot;point&quot;, shape = 18, size = 4, color=&quot;red&quot;) + xlab(&quot;Sexually Active&quot;) + ylab(&quot;Longevity (days)&quot;) We can see that there is a difference between the mean of these two samples, but what can we say/infer about the population means of these two groups? To say anything meaningful from a statistical standpoint, we need to perform a statistical test that will guide us in rejecting, or failing to reject, our null hypothesis (i.e Sexual activity has no effect on the population mean longevity of male fruit flies). 5.3.4 Formula notation in R To perform an ANOVA in R, we need to understand R’s formula notation, as this is the first argument we give to the ANOVA function (aov). The formula notation starts with providing the response variable, then a special character, ~, which can be read as “modeled as,” and then the explanatory/independent variable(s). Thus, the formula notation for this experiment is: longevity ~ activity The formula notation can get more complex such as including additional explanatory/independent variables or interaction terms. We will introduce these are we attempt more complex analyses later on. 5.3.5 ANOVA in R To do an ANOVA in R, we will use the aov function. As stated above, the first argument to this is the formula for the experiment/model, and the second argument we must provide is the name of the variable holding our data, here fruitfly_2groups. The aov function returns us a “model” object, and we need to use another function to see the results of the analysis. I suggest using the tidy function from the broom R package as it returns the results in a nice data frame, that is easy to do further work with. Another, more traditional function to access these data is the summary function, but again, I don’t recommend this as accessing the individual numbers from the output of aov from this model is a bit trickier. # create an ANOVA &quot;model&quot; object fruitfly_2groups_model &lt;- aov(longevity ~ activity, data = fruitfly_2groups) # view output of aov() as a nice dataframe using tidy() from the broom package tidy(fruitfly_2groups_model) ## # A tibble: 2 x 6 ## term df sumsq meansq statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 activity 1 5729. 5729. 21.2 0.0000155 ## 2 Residuals 78 21032. 270. NA NA So, what does this output mean? The most important result in regards to rejecting (or failing to reject) our null hypothesis is the p-value. In this simple one-way ANOVA, we have a single p-value which has a very small value of \\(1.5515618\\times 10^{-5}\\). Given that this is much much smaller than the commonly used threshold for rejecting the null hypothesis, p &lt; 0.05, we can reject our null hypothesis that sexual activity has no effect on the population mean longevity of male fruit flies, and accept the alternative hypothesis that sexual activity does has an effect on population mean longevity of male fruit flies. 5.3.5.1 Exercise: 1-way ANOVA Using ANOVA, test if fruit fly longevity is effected bt size (as measured by thorax length). What are your null and alternative hypothesis? What can you conclude from these results? 5.3.6 1-way ANOVA with &gt; 2 groups As mentioned at the start of this section, an ANOVA can be used when there are more than 2 levels in your categorical explanatory/independent variable (unlike a t-test). For example, we will consider the following case: We are still interested in whether sexual activity effects the longevity of male fruit flies but want to understand this at a finer level (e.g. does the amount of sexual activity matter?). Thus, in this experiment, there are 3 categories for sexual activity. Specifically, males were kept: none - alone low - with a new virgin fruit fly every day high - with a new set of eight virgin fruit flies every day So, for this case, our hypotheses are as follows: Null Hypothesis, \\(H_{0}\\): \\(\\mu_{isolated} = \\mu_{low} = \\mu_{high}\\) Alternative Hypothesis, \\(H_{A}\\): at least one group’s population mean differs from that of the other groups 5.3.7 Reload and explore the data # Create categorical groups and subset data fruitfly_3groups &lt;- fruitfly %&gt;% # Convert factors to character variables mutate_if(is.factor, as.character) %&gt;% # Create 3-level activity variable mutate(activity = ifelse(activity %in% c(&quot;isolated&quot;, &quot;one&quot;, &quot;many&quot;), &quot;none&quot;, activity)) %&gt;% # Subset to equal group sizes for activity group_by(activity) %&gt;% sample_n(25) Let’s explore these data (again, ignore the thorax variable). # get number of rows and columns dim(fruitfly_3groups) ## [1] 75 3 # view first 6 records of data head(fruitfly_3groups) ## # A tibble: 6 x 3 ## # Groups: activity [1] ## thorax longevity activity ## &lt;dbl&gt; &lt;int&gt; &lt;chr&gt; ## 1 0.64 19 high ## 2 0.78 33 high ## 3 0.88 54 high ## 4 0.8 30 high ## 5 0.88 47 high ## 6 0.82 54 high And, again as a good practice, let’s visualize the data before we perform our statistical analysis. # re-order factors to make them show up how we would like them on the plot # instead of alphabetically (default R behaviour) fruitfly_3groups$activity &lt;- factor(fruitfly_3groups$activity, levels = c(&quot;none&quot;,&quot;low&quot;,&quot;high&quot;)) # plot raw data points for each group as a transparent grey/black point # overlay mean as a red diamond ggplot(fruitfly_3groups, aes(x = activity, y = longevity)) + geom_jitter(position = position_jitter(0.15), alpha = 0.7) + stat_summary(fun = mean, geom = &quot;point&quot;, shape = 18, size = 4, color = &quot;red&quot;) + xlab(&quot;Sexual Activity&quot;) + ylab(&quot;Longevity (days)&quot;) So, it looks the sample means of longevity for both low and high activity are lower than the sample means of the isolated male fruit fly. Are these differences in the sample means indicating that there are differences in the true population means between any of these groups? Again we turn to ANOVA to answer this: # create an ANOVA &quot;model&quot; object fruitfly_3groups_model &lt;- aov(longevity ~ activity, data = fruitfly_3groups) # view output of aov() as a nice dataframe using tidy() from the broom package tidy(fruitfly_3groups_model) ## # A tibble: 2 x 6 ## term df sumsq meansq statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 activity 2 10123. 5061. 28.3 8.48e-10 ## 2 Residuals 72 12872. 179. NA NA So, what does this output mean? Similar to a 1-way, 2 group ANOVA, we look at the p-value to determine if we reject (or fail to reject) our null hypothesis. In this case, we have a single p-value which has a very small value of \\(8.4803266\\times 10^{-10}\\). Given that this is much much smaller than the commonly used threshold for rejecting the null hypothesis, p &lt; 0.05, we can reject our null hypothesis that all the population mean for longevity of male fruit flies is equal between all groups, and accept the alternative hypothesis that at least one group’s population mean differs from that of the other groups. But which one(s) differ? 5.3.8 Assess which groups differ This is something ANOVA alone cannot tell us. To answer this, we need to either perform pair-wise t-tests (followed by an adjustment or correction for multiple comparisons, such as a Bonferroni correction, or False Discovery Rate) OR follow the ANOVA with a contrast-test, such as Tukey’s honestly significant difference (HSD) test. We’ll do both here, and show that we get similar results: # pairwise t-tests to observe group differences tidy(pairwise.t.test(fruitfly_3groups$longevity, fruitfly_3groups$activity, p.adjust.method = &quot;bonferroni&quot;, pool.sd = TRUE, paired = FALSE)) ## # A tibble: 3 x 3 ## group1 group2 p.value ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 low none 2.93e- 2 ## 2 high none 5.42e-10 ## 3 high low 2.81e- 5 # Tukey&#39;s HSD test to observe group differences tidy(TukeyHSD(fruitfly_3groups_model, &quot;activity&quot;)) ## # A tibble: 3 x 7 ## term contrast null.value estimate conf.low conf.high adj.p.value ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 activity low-none 0 -10.0 -19.1 -0.990 2.61e- 2 ## 2 activity high-none 0 -28.1 -37.1 -19.0 4.56e-10 ## 3 activity high-low 0 -18.0 -27.1 -8.99 2.76e- 5 From both of these multiple comparison tests, we see that there is no significant difference between the population mean longevity of male fruit flies who had no or little sexual activity. However, high sexual activity does appear to matter, as the population mean longevity of male fruit flies who had high sexual activity is significantly different from that of male flies who had either no or low sexual activity. 5.3.9 2-way ANOVA with 2 groups Let’s continue to add complexity to our ANOVA model. In this experiment, we not only interested in how sexual activity might effect longevity; we are also interested in body size (assessed via thorax length). We do this because the literature indicates body size affects fruit fly longevity. Thus, now we have two categories/explanatory variables to look at sexual activity (back to our first version with levels no and yes) and thorax length (with levels short and long). For this experiment, we have two sets of null and alternative hypotheses: Hypotheses for sexual activity Null Hypothesis, \\(H_{0}\\): \\(\\mu_{No} = \\mu_{Yes}\\) Alternative Hypothesis, \\(H_{A}\\): \\(\\mu_{No} \\ne \\mu_{Yes}\\) Hypotheses for thorax length Null Hypothesis, \\(H_{0}\\): \\(\\mu_{short} = \\mu_{long}\\) Alternative Hypothesis, \\(H_{A}\\): \\(\\mu_{short} \\ne \\mu_{long}\\) Now that we have our case setup, let’s re-look at our 2 level data but now notice the thorax information. # get number of rows and columns dim(fruitfly_2groups) ## [1] 80 3 # view first 6 records of data head(fruitfly_2groups) ## # A tibble: 6 x 3 ## # Groups: activity, thorax [1] ## thorax longevity activity ## &lt;chr&gt; &lt;int&gt; &lt;chr&gt; ## 1 long 70 no ## 2 long 72 no ## 3 long 75 no ## 4 long 81 no ## 5 long 90 no ## 6 long 96 no Next, let’s plot these data. # re-order factors to make them show up how we would like them on the plot # instead of alphabetically (default R behaviour) fruitfly_2groups$thorax &lt;- factor(fruitfly_2groups$thorax, levels = c(&quot;short&quot;,&quot;long&quot;)) # plot strip charts of longevity, grouped by sexual activity # and colored by thorax length ggplot(fruitfly_2groups, aes(x = activity, y = longevity, color = thorax)) + stat_summary(fun = mean, geom = &quot;point&quot;, shape = 5, size = 4, position = position_dodge(0.5)) + geom_jitter(position = position_dodge(0.5), alpha = 0.3) + scale_color_manual(values=c(&quot;black&quot;, &quot;dodgerblue3&quot;)) + xlab(&quot;Sexual Activity&quot;) + ylab(&quot;Longevity (days)&quot;) This data visualization suggests that both sexual activity and body size/thorax length may effect longevity. Let’s confirm (or disprove) this intuition by performing a 2-way (or 2-factor) ANOVA. To perform a 2-way ANOVA, we modify the formula notation that we pass into to aov function by adding an additional factor/category/explanatory variable through the use of the + sign and the name of the new variable. Thus, our formula for this case is: longevity ~ activity + thorax Everything else remains the same: # create an ANOVA &quot;model&quot; object fruitfly_2var_model &lt;- aov(longevity ~ activity + thorax, data = fruitfly_2groups) # view output of aov() as a nice dataframe using tidy() from the broom package tidy(fruitfly_2var_model) ## # A tibble: 3 x 6 ## term df sumsq meansq statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 activity 1 5729. 5729. 38.8 2.31e- 8 ## 2 thorax 1 9658. 9658. 65.4 6.94e-12 ## 3 Residuals 77 11374. 148. NA NA Now, we see that we get back an additional line in our results summary that corresponds to the hypotheses regarding the effect of body size/thorax length on longevity. The p-values for both sexual activity (\\(2.3099823\\times 10^{-8}\\)) and size (\\(6.9440175\\times 10^{-12}\\)) are very, very small. Thus, we can reject both of our null hypotheses and infer that both sexual activity and size have statistically significant effect on longevity. 5.3.10 2-way ANOVA with 2 groups including an interaction term Oftentimes when we are dealing with experiments/cases where we have 2 or more factor/category/explanatory variables, we first want to ask if there is an interaction effect between them and their influences/effects on the response variable. What do we mean by interaction effect? Essentially, an interaction effect is observed when the effect of two explanatory variables on the response variable is not additive (for example, their effect could instead be synergistic). Our hypotheses for whether or not there is an interaction are: Null Hypothesis, \\(H_{0}\\): There is no interaction effect between sexual activity and thorax length on the mean longevity of the population. Alternative Hypothesis, \\(H_{A}\\): There is an interaction effect between sexual activity and thorax length on the mean longevity of the population. In a simple case, as presented in this experiment, we first assess the hypotheses in regards to the presence or absence of interaction. If we reject the interaction effect null hypothesis, then we interpret the data only in regards to this null hypothesis. If we fail to reject the interaction effect null hypothesis, then we can proceed and investigate/test the hypotheses for each individual factor/category/explanatory variable (often referred to as “main effects”). Can we get an intuitive sense for this via visualization? Yes we can by making an interaction plot (see example below). Here, we are looking at the slope of the lines that connects the means. If the slopes of the interaction lines are parallel, then the ANOVA results will very likely tell us that we will fail to reject the interaction effect null hypothesis. On the other hand, if they are not parallel, the ANOVA results will very likely tell us to reject the interaction effect null hypothesis, and we can infer that there is an interaction effect on the response variable between the two factor/category/explanatory variables. Let’s look at the interaction plot for our case: # plot to investigate possible interaction effect of sexual # activity and thorax length on longevity ggplot(fruitfly_2groups, aes(x = activity, y = longevity, color = thorax)) + stat_summary(fun = mean, geom = &quot;point&quot;, shape = 18, size = 3) + stat_summary(fun = mean, geom = &quot;line&quot;, aes(group = thorax)) + scale_color_manual(values=c(&quot;black&quot;, &quot;dodgerblue3&quot;)) + xlab(&quot;Sexual Activity&quot;) + ylab(&quot;Longevity (days)&quot;) Although not perfectly parallel, the lines on the interaction plot are pretty close to parallel. So, the ANOVA results will very likely tell us that we will fail to reject the interaction effect null hypothesis. Let’s proceed with the analysis to be sure. One way to include an interaction term in your ANOVA model is to use the * symbol between two factor/category/explanatory variables. This causes R to test the null hypotheses for the effect of each individual factor/category/explanatory variables as well as the combined effect of these two explanatory variables. Thus, for us, our formula notation is now: longevity ~ activity * thorax Importantly, using * causes R to test all possible interactions. So if we had a formula A * B * C, it would test all combinations of the 3 variables. If instead, you want to specify specific interaction term(s), you can use :. In the case of our formula, this is the same as * but serves as an example of the other notation type. longevity ~ activity + thorax + activity:thorax Everything else about our input aov remains the same as our previous model. # create an ANOVA &quot;model&quot; object fruitfly_2var_model2 &lt;- aov(longevity ~ activity * thorax, data = fruitfly_2groups) # view output of aov() as a nice dataframe using tidy() from the broom package tidy(fruitfly_2var_model2) ## # A tibble: 4 x 6 ## term df sumsq meansq statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 activity 1 5729. 5729. 38.3 2.83e- 8 ## 2 thorax 1 9658. 9658. 64.6 9.41e-12 ## 3 activity:thorax 1 6.61 6.61 0.0442 8.34e- 1 ## 4 Residuals 76 11367. 150. NA NA As stated above, as a rule of thumb for cases such as these, the first hypotheses we should attend to are those regarding the interaction effect (or lack thereof). We can see our output from ANOVA now has an additional line that refers to the testing of the interaction effect hypothesis. \\(activity:thorax, 1, 6.6125, 6.6125, 0.0442107, 0.8340247\\) We observe that the p-value from this line is not very small, \\(0.8340247\\), and not less than the standard p-value threshold for rejecting null hypotheses (0.05). Thus, as our interaction plot suggested, we fail to reject the null hypotheses and conclude that there is no interaction effect between sexual activity and thorax length on the mean longevity of the population). We would then proceed to investigate the hypotheses of each main effect independently. This could be done by either interpreting the relevant p-values from our current ANOVA results table, or re-running the analysis without the interaction term (as done in the previous case). 5.3.10.1 Exercise: ANOVA Determine whether the following statements are true or false? ANOVA tests the null hypothesis that the sample means are all equal? We use ANOVA to compare the variances of the population? A one-way ANOVA is equivalent to a t-test when there are 2 groups to be compared. In rejecting the null hypothesis, one can conclude that all the population means are different from one another? Questions courtesy of Dr. Gabriela Cohen Freue’s DSCI 562 course (UBC) If you are attending a 3 x 2-hour workshop, this is the end of day 1 5.4 Linear regression 5.4.1 Load and explore the data Now, we will work with a data frame that Jennifer Bryan (U. of British Columbia, RStudio) put together in the gapminder package. Unlike the fruit fly data, no pre-manipulation is needed so let’s view these data as is. gapminder ## # A tibble: 1,704 x 6 ## country continent year lifeExp pop gdpPercap ## &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Afghanistan Asia 1952 28.8 8425333 779. ## 2 Afghanistan Asia 1957 30.3 9240934 821. ## 3 Afghanistan Asia 1962 32.0 10267083 853. ## 4 Afghanistan Asia 1967 34.0 11537966 836. ## 5 Afghanistan Asia 1972 36.1 13079460 740. ## 6 Afghanistan Asia 1977 38.4 14880372 786. ## 7 Afghanistan Asia 1982 39.9 12881816 978. ## 8 Afghanistan Asia 1987 40.8 13867957 852. ## 9 Afghanistan Asia 1992 41.7 16317921 649. ## 10 Afghanistan Asia 1997 41.8 22227415 635. ## # … with 1,694 more rows We see that the data contain information on life expectancy (lifeExp), population (pop), and gross domestic product per capita (gdpPercap, a rough measure of economical richness) for many countries across many years. A very naive working hypothesis that you may come to is that our life expectancy grew with time. This would be represent in r with lifeExp ~ year. We can explore this hypothesis graphically. gapminder %&gt;% ggplot(aes(x = year, y = lifeExp)) + geom_point() + labs(y=&quot;Life expectancy (yrs)&quot;) Although there is very high variance, we do see a certain trend with mean life expectancy increasing over time. Similarly, we can naively hypothesize that life expectancy is higher where the per-capita gdp is higher. In R, this is lifeExp ~ gdpPercap. gapminder %&gt;% ggplot(aes(x = gdpPercap, y = lifeExp)) + geom_point() + labs(y=&quot;Life expectancy (yrs)&quot;, x=&quot;Per-capita GDP&quot;) 5.4.2 Linear models A linear regression model describes the change of a dependent variable, say lifeExp, as a linear function of one or more explanatory variables, say year. This means that increasing by \\(x\\) the variable year will have an effect \\(\\beta \\cdot x\\) on the dependent variable lifeExp, whatever the value \\(x\\) is. In mathematical terms: \\[ \\mbox{lifeExp} = \\alpha + \\beta \\cdot \\mbox{year} \\] We call \\(\\alpha\\) the intercept of the model, or the value of lifeExp when year is equal to zero. When we go forward in time, increasing year, lifeExp increases (if \\(\\beta\\) is positive, otherwise it decreases): \\[ \\alpha + \\beta \\cdot \\left(\\mbox{year} + x \\right) = \\alpha + \\beta \\cdot \\mbox{year} + \\beta \\cdot x = \\mbox{lifeExp} + \\beta \\cdot x \\] 5.4.3 Key assumptions number of assumptions must be satisfied for a linear model to be reliable. These are: the predictor variables should be measured with not too much error (weak exogeneity) the variance of the response variable should be roughly the same across the range of its predictors (homoscedasticity, a fancy pants word for “constant variance”) the discrepancies between observed and predicted values should be independent the predictors themselves should be non-colinear (a rather technical issues, given by the way we solve the model, that may happen when two predictors are perfectly correlated or we try to estimate the effect of too many predictors with too little data). Here, we only mention these assumptions, but for more details, take a look at wiki. When we have only one predictive variable (what is called a simple linear regression model), the formula we just introduced describes a straight line. The task of a linear regression method is identifying the best fitting slope and intercept of that straight line. But what does best fitting means in this context? We will first adopt a heuristic definition of it but will rigorously define it later on. Let’s consider a bunch of straight lines in our first plot: gapminder %&gt;% ggplot(aes(x = year, y = lifeExp)) + geom_point() + geom_abline(intercept = 0, slope = 0.033, colour = &quot;green&quot;) + geom_abline(intercept = -575, slope = 0.32, colour = &quot;purple&quot;) + geom_hline(aes(yintercept = mean(lifeExp)), colour = &quot;red&quot;) + geom_vline(aes(xintercept = mean(year)), colour = &quot;blue&quot;) + labs(y=&quot;Life expectancy (yrs)&quot;) So, which line best describes the data? To determine this, we must fit a linear model to the data. 5.4.4 Simple linear regression To obtain the slope and intercept of the green line, we can use the built-in R function lm(). This function works very similarly to the aov function we used earlier in that we must give it a model and data. The output of lm() is also messy but we want to use summary instead of tidy in order to see all the relevant results. # create a lm &quot;model&quot; object lifeExp_model1 &lt;- lm(lifeExp ~ year, data = gapminder) # view output of lm() as using summary() summary(lifeExp_model1) ## ## Call: ## lm(formula = lifeExp ~ year, data = gapminder) ## ## Residuals: ## Min 1Q Median 3Q Max ## -39.949 -9.651 1.697 10.335 22.158 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -585.65219 32.31396 -18.12 &lt;2e-16 *** ## year 0.32590 0.01632 19.96 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 11.63 on 1702 degrees of freedom ## Multiple R-squared: 0.1898, Adjusted R-squared: 0.1893 ## F-statistic: 398.6 on 1 and 1702 DF, p-value: &lt; 2.2e-16 Now, however, we are interested in more than just the p-value. The Estimate values are the best fit for the intercept, \\(\\alpha\\), and the slope, \\(\\beta\\). The slope, the parameter the links year to lifeExp, is a positive value: every 1 year, the life expectancy increases of \\(0.3259038\\) years. This is in line with our hypothesis. Moreover, its p-value, the probability of finding a correlation at least as strong between predictive and response variable, is rather low at \\(7.5467946\\times 10^{-80}\\) (but see this for a cautionary tale about p-values!). Using this slope and intercept, we can plot this best fit line on our data. gapminder %&gt;% ggplot(aes(x = year, y = lifeExp)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE, colour = &quot;green&quot;) + labs(y=&quot;Life expectancy (yrs)&quot;) ## `geom_smooth()` using formula &#39;y ~ x&#39; Another important bit of information in our results is the R-squared values, both are the Multiple and Adjusted R-squared. These tell us how much of variance in the life expectancy data is explained by the year. In this case, not much (0.1897571, 0.1892811, respectively) 5.4.5 Residuals We can further explore our linear model by plotting some diagnostic plots. Base R provides a quick and easy way to view all of these plots at once with plot. # Set plot frame to 2 by 2 par(mfrow=c(2,2)) # Create diagnostic plots plot(lifeExp_model1) Whoa right?! Let’s break it down. Overall, these diagnostic plots are useful for understanding the model residuals. The residuals are the discrepancies between the life expectancy we would have guessed by the model and the observed values in the available data. In other words, the distances between the straight line and actual data points. In a linear regression model, these residuals are the values we try to minimize when we fit the straight line. There is a lot of information contained in these 4 plots, and you can find in-depth explanations here. For our purposes today, let’s focus on just the Residuals vs Fitted and Normal Q-Q plots. The Residuals vs Fitted plot shows the differences between the best fit line and all the available data points. When the model is a good fit for the data, this plot should have no discernible pattern. That is, the red line should not form a shape like an ‘S’ or a parabola. Another way to look it is that the points should look like ‘stars in the sky,’ e.g. random. This second description is not great for these data since year is an integer (whole number) but we do see that the red line is relatively straight and without pattern. The Normal Q-Q plot directly compares the best fit and actual data values. A good model closely adheres to dotted line and points that fall off the line should not portray any pattern. In our case, this plot indicates that this simple linear model may not be the best fit for these data. Notice how either end deviates more and more from the line and the plot forms somewhat of an ‘S’ pattern. These ends are particularly important in a linear model. Because we’ve chosen to use a simple linear model, outliers, or observed values that are very far away from our best fit line, are very important (in jargon, they have a high leverage, see the fourth diagnostic plot). This is especially true if the outliers are at the edge of the predicting variable ranges such as we see in our Q-Q plot. 5.4.5.1 Exercise: Linear models Looking at the summary plots above, do you feel that our model can be extrapolated to a much wider year range? Why or why not? Fit a linear model of life expectancy as a function of per-capita gdp. Using the summary table and diagnostic plots, discuss whether or not you think this is a good fit for these data. If you are attending a 2 x 3-hour workshop, this is the end of day 1 5.5 Cautions when using linear models R (and most other statistical software) will fit, plot, summarize, etc. a linear model regardless of whether that model is a good fit for the data. For example, the Anscombe data sets are very different data that give the same slope, intercept, and mean in a linear model. We can access these data in R and format them with absc &lt;- with(datasets::anscombe, tibble(X=c(x1,x2,x3,x4), Y=c(y1,y2,y3,y4), anscombe_quartet=gl(4,nrow(datasets::anscombe)) ) ) Plotting these data reveals the problem. absc %&gt;% ggplot(aes(x=X,y=Y,group=anscombe_quartet)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE) + scale_x_continuous(limits = c(0,20)) + facet_wrap(~anscombe_quartet) ## `geom_smooth()` using formula &#39;y ~ x&#39; This is why you should always, always, always plot your data before attempting regression! 5.6 Multiple linear regression So far, we have dealt with simple regression models, where we had only one predictor variable. However, lm() handles much more complex models. Consider for example a model of life expectancy as a function of both the year and the per-capita gdp. lifeExp ~ year + gdpPercap ## lifeExp ~ year + gdpPercap This formula does not describe a straight line anymore, but a plane in a 3D space. Still a very flat thingy. Let’s fit this model. # create a lm &quot;model&quot; object lifeExp_model2 &lt;- lm(lifeExp ~ year + gdpPercap, data = gapminder) # view output of lm() as using summary() summary(lifeExp_model2) ## ## Call: ## lm(formula = lifeExp ~ year + gdpPercap, data = gapminder) ## ## Residuals: ## Min 1Q Median 3Q Max ## -67.262 -6.954 1.219 7.759 19.553 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -4.184e+02 2.762e+01 -15.15 &lt;2e-16 *** ## year 2.390e-01 1.397e-02 17.11 &lt;2e-16 *** ## gdpPercap 6.697e-04 2.447e-05 27.37 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 9.694 on 1701 degrees of freedom ## Multiple R-squared: 0.4375, Adjusted R-squared: 0.4368 ## F-statistic: 661.4 on 2 and 1701 DF, p-value: &lt; 2.2e-16 We can assess this model with plots similar to our simple linear regression. par(mfrow=c(2,2)) plot(lifeExp_model2) 5.6.1 Multiple is the formula, not the predictor The linearity of a model is in how the predictive variables are put together, not necessarily in the predictors themselves. In the last exercise, you saw that gdpPercap is not the best predictor of lifeExp because it does not seem to fit linearly. This carries forward into our multiple linear regression as is apparent in the last plot. One way to improve our model is to use a transformed gdpPercap predictor. But transformed how? Let’s pick a (silly) function to start. Here, we take the sine of the gdpPercap. gapminder %&gt;% ggplot(aes(x = sin(gdpPercap), y = lifeExp)) + geom_point() Doesn’t look much better. 5.6.1.1 Exercise: Transforming predictors Find a function that makes the plot more “linear” and fit a model of life expectancy as a function of the transformed per-capita gdp. Is it a better model? Go back to your original gdpPercap vs. lifeExp plot and think about what function creates a similar trend. 5.6.2 Transforming predictors: log lifeExp_model3a &lt;- lm(lifeExp ~ year + log(gdpPercap), data = gapminder) Or we can create a new variable in the data frame and use that variable in the model gapminder &lt;- gapminder %&gt;% mutate(log_gdp = log(gdpPercap)) lifeExp_model3b &lt;- lm(lifeExp ~ year + log_gdp, data = gapminder) Both yield the same result. summary(lifeExp_model3a) ## ## Call: ## lm(formula = lifeExp ~ year + log(gdpPercap), data = gapminder) ## ## Residuals: ## Min 1Q Median 3Q Max ## -27.2291 -3.8454 0.6065 4.7737 17.8644 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -3.911e+02 1.942e+01 -20.14 &lt;2e-16 *** ## year 1.956e-01 9.927e-03 19.70 &lt;2e-16 *** ## log(gdpPercap) 7.770e+00 1.381e-01 56.27 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 6.877 on 1701 degrees of freedom ## Multiple R-squared: 0.7169, Adjusted R-squared: 0.7165 ## F-statistic: 2153 on 2 and 1701 DF, p-value: &lt; 2.2e-16 summary(lifeExp_model3b) ## ## Call: ## lm(formula = lifeExp ~ year + log_gdp, data = gapminder) ## ## Residuals: ## Min 1Q Median 3Q Max ## -27.2291 -3.8454 0.6065 4.7737 17.8644 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -3.911e+02 1.942e+01 -20.14 &lt;2e-16 *** ## year 1.956e-01 9.927e-03 19.70 &lt;2e-16 *** ## log_gdp 7.770e+00 1.381e-01 56.27 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 6.877 on 1701 degrees of freedom ## Multiple R-squared: 0.7169, Adjusted R-squared: 0.7165 ## F-statistic: 2153 on 2 and 1701 DF, p-value: &lt; 2.2e-16 5.6.3 Transforming predictors: polynomial Another option is using a polynomial transformation of our model. lifeExp_model4 &lt;- lm(lifeExp ~ year + poly(gdpPercap), data = gapminder) summary(lifeExp_model4) ## ## Call: ## lm(formula = lifeExp ~ year + poly(gdpPercap), data = gapminder) ## ## Residuals: ## Min 1Q Median 3Q Max ## -67.262 -6.954 1.219 7.759 19.553 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -413.59192 27.65674 -14.95 &lt;2e-16 *** ## year 0.23898 0.01397 17.11 &lt;2e-16 *** ## poly(gdpPercap) 272.44154 9.95433 27.37 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 9.694 on 1701 degrees of freedom ## Multiple R-squared: 0.4375, Adjusted R-squared: 0.4368 ## F-statistic: 661.4 on 2 and 1701 DF, p-value: &lt; 2.2e-16 Or explicit write a polynomial equation for one of our predictor. lifeExp_model5 &lt;- lm(lifeExp ~ year + gdpPercap + I(gdpPercap^2), data = gapminder) summary(lifeExp_model5) ## ## Call: ## lm(formula = lifeExp ~ year + gdpPercap + I(gdpPercap^2), data = gapminder) ## ## Residuals: ## Min 1Q Median 3Q Max ## -31.1048 -6.3211 0.3511 6.8441 25.7228 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -3.088e+02 2.426e+01 -12.73 &lt;2e-16 *** ## year 1.819e-01 1.228e-02 14.81 &lt;2e-16 *** ## gdpPercap 1.396e-03 3.671e-05 38.02 &lt;2e-16 *** ## I(gdpPercap^2) -1.344e-08 5.558e-10 -24.18 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 8.364 on 1700 degrees of freedom ## Multiple R-squared: 0.5814, Adjusted R-squared: 0.5807 ## F-statistic: 787.1 on 3 and 1700 DF, p-value: &lt; 2.2e-16 Note that we must use I(gdpPercap^2) instead of gdpPercap^2 because the symbols ^, *, and : have a particular meaning in a linear model. As we saw in ANOVA, these symbols are used to specify interactions between predicting variables. 5.6.3.1 Exercise: Multiple linear regression So far, we have worked with lifeExp as our independent variable. Now, in small groups, try to produce a model of population (pop) using one or more of the variables available in gapminder. If you are attending a 3 x 2-hour workshop, this is the end of day 2 5.6.4 Interactions and ANalysis of COVArianvce (ANCOVA) So far, our models for life expectancy have built upon continuous (or discrete but incremental) variables. However, we may wonder if being in one continent rather than another has a differential effect on the correlation between year and lifeExp. Let’s take a look at our data set now with continent in mind. gapminder %&gt;% ggplot(aes(x = year, y = lifeExp, colour = continent)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se= FALSE) + labs(y=&quot;Life expectancy (yrs)&quot;) ## `geom_smooth()` using formula &#39;y ~ x&#39; The slopes for each continent seem different, but how can we tell if the difference is significant? Here’s where we can combine our linear model with the ANOVA function we learned earlier! First, using the special character *, we model the effects of year, continent, and their interaction on life expectancy. lifeExp_model6 &lt;- lm(lifeExp ~ year*continent, data = gapminder) Then, we call the ANOVA function aov() on the model: summary(aov(lifeExp_model6)) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## year 1 53919 53919 1046.0 &lt; 2e-16 *** ## continent 4 139343 34836 675.8 &lt; 2e-16 *** ## year:continent 4 3566 892 17.3 6.46e-14 *** ## Residuals 1694 87320 52 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Based on these results, it really seems that the continent should have a role in the model. However, it is not always like this. Let’s take a closer look at Europe and Oceania. gapminder %&gt;% filter(continent %in% c(&quot;Oceania&quot;,&quot;Europe&quot;)) %&gt;% lm(lifeExp ~ year*continent, data = .) %&gt;% aov() %&gt;% summary() ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## year 1 5598 5598 399.070 &lt; 2e-16 *** ## continent 1 132 132 9.414 0.00231 ** ## year:continent 1 1 1 0.065 0.79895 ## Residuals 380 5330 14 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Note that this is an example of how the tidyverse can be used to link together a bunch of functions, instead of creating many new R objects as we’ve been doing thus far. When just looking at Oceania and Europe, continent has a significant effect on the intercept of the model, but not on its slope. This makes sense since in our plot, these lines (blue and purple) appear parallel but with different y-intercepts. 5.7 Linear Mixed Effects models 5.7.1 Motivations for LME Let’s take a look at the esoph data set, which comes pre-downloaded in R. These data contain information on smoking, alcoholism, and (o)esophageal cancer. Specifically, we are interested in if the number of controls ncontrols affects the number of cases ncases of cancer for each age group agegp. Here’s what the data look like (with a tad bit of vertical jitter): p &lt;- ggplot(esoph, aes(ncontrols, ncases, group=agegp, colour=agegp)) + geom_jitter(height=0.25) + scale_colour_discrete(&quot;Age Group&quot;) + ylab(&quot;Number of Cases&quot;) + xlab(&quot;Number of Controls&quot;) p It seems each age group has a different relationship. Should we then fit regression lines for each group separately? Here’s what we get, if we do. p + geom_smooth(method=&quot;lm&quot;, se=FALSE, size=0.5) ## `geom_smooth()` using formula &#39;y ~ x&#39; But each group has so few observations which makes the regression less powerful. esoph %&gt;% group_by(agegp) %&gt;% dplyr::summarise(n=length(ncases)) %&gt;% as.data.frame ## agegp n ## 1 25-34 15 ## 2 35-44 15 ## 3 45-54 16 ## 4 55-64 16 ## 5 65-74 15 ## 6 75+ 11 Question: can we borrow information across groups to strengthen regression, while still allowing each group to have its own regression line? Yes – we can use Linear Mixed Effects (LME) models. An LME model is just a linear regression model for each group, with different slopes and intercepts, but the collection of slopes and intercepts is assumed to come from some normal distribution. 5.7.2 Definitions With one predictor (\\(X\\)), we can write an LME as follows: \\[ Y = \\left(\\beta_0 + b_0\\right) + \\left(\\beta_1 + b_1\\right) X + \\varepsilon, \\] where the error term \\(\\varepsilon\\) has mean zero, and the \\(b_0\\) and \\(b_1\\) terms are normally distributed having a mean of zero, and some unknown variances and correlation. The \\(b_0\\) and \\(b_1\\) terms indicate group-to-group differences from average. The \\(\\beta\\) terms are called the fixed effects, and the \\(b\\) terms are called the random effects. Since the model has both types of effects, it’s said to be a mixed model – hence the name of “LME.” Note that we don’t have to make both the slope and intercept random. For example, we can remove the \\(b_0\\) term, which would mean that each group is forced to have the same (fixed) intercept \\(\\beta_0\\). Also, we can add more predictors (\\(X\\) variables). 5.7.3 Fitting LME Two R packages exist for working with mixed effects models: lme4 and nlme. We’ll be using the lme4 package (check out this discussion on Cross Validated for a comparison of the two packages). Let’s fit the model. Just like our other models we need to give a formula and data. esoph_model &lt;- lmer(ncases ~ ncontrols + (ncontrols | agegp), data=esoph) Let’s take a closer look at the formula, which in this case is ncases ~ ncontrols + (ncontrols | agegp). On the left of the ~ is the response variable, as usual (just like for lm). On the right, we need to specify both the fixed and random effects. The fixed effects part is the same as usual: ncontrols indicates the explanatory variables that get a fixed effect. Then, we need to indicate which explanatory variables get a random effect. The random effects can be indicated in parentheses, separated by +, followed by a |, after which the variable(s) that you wish to group by are indicated. So | can be interpreted as “grouped by.” Now let’s look at the model output: summary(esoph_model) ## Linear mixed model fit by REML [&#39;lmerMod&#39;] ## Formula: ncases ~ ncontrols + (ncontrols | agegp) ## Data: esoph ## ## REML criterion at convergence: 388.6 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -2.6511 -0.3710 -0.1301 0.3683 4.8056 ## ## Random effects: ## Groups Name Variance Std.Dev. Corr ## agegp (Intercept) 1.69426 1.3016 ## ncontrols 0.00573 0.0757 0.26 ## Residual 3.73290 1.9321 ## Number of obs: 88, groups: agegp, 6 ## ## Fixed effects: ## Estimate Std. Error t value ## (Intercept) 1.63379 0.59991 2.723 ## ncontrols 0.04971 0.03677 1.352 ## ## Correlation of Fixed Effects: ## (Intr) ## ncontrols 0.038 The random and fixed effects are indicated here. Under the “Random effects:” section, we have the variance of each random effect, and the lower part of the correlation matrix of these random effects. Under the “Fixed effects:” section, we have the estimates of the fixed effects, as well as the uncertainty in the estimate (indicated by the Std. Error). We can extract the collection of slopes and intercepts for each group with our handy tidy function. tidy(esoph_model) ## # A tibble: 6 x 6 ## effect group term estimate std.error statistic ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 fixed &lt;NA&gt; (Intercept) 1.63 0.600 2.72 ## 2 fixed &lt;NA&gt; ncontrols 0.0497 0.0368 1.35 ## 3 ran_pars agegp sd__(Intercept) 1.30 NA NA ## 4 ran_pars agegp cor__(Intercept).ncontrols 0.259 NA NA ## 5 ran_pars agegp sd__ncontrols 0.0757 NA NA ## 6 ran_pars Residual sd__Observation 1.93 NA NA Alternatively, we can use the coef function: coef(esoph_model) ## $agegp ## (Intercept) ncontrols ## 25-34 0.2674414 -0.002916658 ## 35-44 0.7227576 -0.001128636 ## 45-54 2.2834044 0.036587951 ## 55-64 3.5107945 0.064244491 ## 65-74 1.8699071 0.171921200 ## 75+ 1.1484472 0.029580126 ## ## attr(,&quot;class&quot;) ## [1] &quot;coef.mer&quot; Let’s put these regression lines on the plot. First, we must extract the relevant slopes and intercepts. # Put the slopes and intercepts with orignal data ## Exact slopes and intercepts par_coll &lt;- coef(esoph_model)$agegp %&gt;% tibble::rownames_to_column() # Bind to orig data esoph &lt;- ddply(esoph, ~ agegp, function(df){ pars &lt;- subset(par_coll, rowname==unique(df$agegp)) int &lt;- pars$`(Intercept)` slp &lt;- pars$ncontrols cbind(df, intercept=int, slope=slp) }) Then we can add these lines to the plot. ## Plot ggplot(esoph, aes(ncontrols, ncases, group=agegp, colour=agegp)) + geom_jitter(height=0.25) + geom_abline(aes(intercept=intercept, slope=slope, colour=agegp)) + scale_colour_discrete(&quot;Age Group&quot;) + ylab(&quot;Number of Cases&quot;) + xlab(&quot;Number of Controls&quot;) So, each group still gets its own regression line, but tying the parameters together with a normal distribution gives us a more powerful regression. 5.7.3.1 Exercise: LME Using the sleepstudy dataset, fit an LME on Reaction against Days, grouped by Subject. What is the intercept and slope of subject #310 in the model from question 1? CHALLENGE. Using the Teams dataset from the Lahman package, fit a model on runs (R) from the variables ‘walks’ (BB) and ‘Hits’ (H), grouped by team (teamID). Hint: wrap the scale function around each predictor variable. 5.8 Generalized Linear models In the linear models discussed so far, we always assumed that the response can take any numeric value or at least any numeric value in a large range. However, we often have response data that does not quite fit this assumption. This includes, for instance, count data that can only take non-negative integer values (i.e. 0, 1, 2, 3, …). Another example is binary data, where the response can take only one of two values (e.g. yes/no, low/high, 0/1, etc.). 5.8.1 Definition Generalized Linear Models (GLMs) are exactly what their name suggests, a generalization of linear models introduced to different kinds of data. With GLMs, we want to model the mean \\(\\mu\\) (or rather a function called link) of the assumed distribution as a linear function of some covariates. The model can be written as \\[ g(\\mu) = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\cdots + \\beta_p x_p \\] and we want to estimate the parameters \\(\\beta_0\\) to \\(\\beta_p\\) for the \\(p\\) covariates from the available data. The choice of the distribution is guided by the type of the response data (and the limited set of distributions for GLMs). The link function \\(g()\\) is needed to convert the mean of the assumed distribution into a linear function of the model parameters, but it also makes it more difficult to interpret the parameters. The distributions usually have a “natural” link function, but other choices would be available too. Before we discuss this in more detail, let’s see how we can actually fit a GLM in R. 5.8.2 Fitting GLMs To fit a generalized linear model in R, we use the function glm which works very similarly to the already known lm function. However, it allows us to specify the distribution we want to use via the argument family. Each family comes with a default link function. The help page for ?family lists all supported types of GLMs. We explore some of them below. 5.8.3 Logistic regression (family = binomial) We will first discuss the case of binary response data (e.g. no/yes, 0/1, failure/success, …). Oftentimes, we are interested in the probability of a success under certain circumstances, i.e. we want to model the success probability given a set of covariates. The natural choice for this kind of data is to use the Binomial distribution. This distribution corresponds to the number of successes in \\(m\\) trials, if each trial is independent and has the same success probability. Binary data can be thought of as a single trial (i.e. \\(m = 1\\)). The mean of the Binomial distribution is the success probability \\(p\\) and the usual link function is the log of the odds. This gives us the logistic regression model: \\[ \\log \\left( \\frac{p}{1 - p} \\right) = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\cdots + \\beta_p x_p \\] Importantly, each trial needs to be independent and must have the same success probability! As a first example, we will try logistic regression on the UC Berkeley Admission data (UCBAdmissions, pre-installed in R). We want to model log odds of being admitted, using the biological sex (incorrectly attributed as gender in the data set) of the applicant and the department as covariates. Note: The data set does not contain one row per applicant, but rather has the number of applications that fall in each of the possible combinations. This can be easily used in R via the weights argument to glm. # Load and format data ucb &lt;- as.data.frame(UCBAdmissions) %&gt;% dplyr::rename(sex=Gender) # Fit GLM binomial ucb_model &lt;- glm(Admit ~ sex * Dept, data = ucb, family = binomial, weights = Freq) summary(ucb_model) ## ## Call: ## glm(formula = Admit ~ sex * Dept, family = binomial, data = ucb, ## weights = Freq) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -22.1022 -15.6975 0.3243 13.0256 24.6314 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -0.49212 0.07175 -6.859 6.94e-12 *** ## sexFemale -1.05208 0.26271 -4.005 6.21e-05 *** ## DeptB -0.04163 0.11319 -0.368 0.71304 ## DeptC 1.02764 0.13550 7.584 3.34e-14 *** ## DeptD 1.19608 0.12641 9.462 &lt; 2e-16 *** ## DeptE 1.44908 0.17681 8.196 2.49e-16 *** ## DeptF 3.26187 0.23120 14.109 &lt; 2e-16 *** ## sexFemale:DeptB 0.83205 0.51039 1.630 0.10306 ## sexFemale:DeptC 1.17700 0.29956 3.929 8.53e-05 *** ## sexFemale:DeptD 0.97009 0.30262 3.206 0.00135 ** ## sexFemale:DeptE 1.25226 0.33032 3.791 0.00015 *** ## sexFemale:DeptF 0.86318 0.40267 2.144 0.03206 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 6044.3 on 23 degrees of freedom ## Residual deviance: 5167.3 on 12 degrees of freedom ## AIC: 5191.3 ## ## Number of Fisher Scoring iterations: 6 The summary of the glm output shows us the value of each of the parameters as well as whether they are significantly different from 0. However, since our covariates are categorical, each of the two are reflected by multiple parameters (one for each “level” and combination of levels). With the Anova function from the package car, we can check the significance of each of the two covariates as a whole. Anova(ucb_model) ## Analysis of Deviance Table (Type II tests) ## ## Response: Admit ## LR Chisq Df Pr(&gt;Chisq) ## sex 1.53 1 0.215928 ## Dept 763.40 5 &lt; 2.2e-16 *** ## sex:Dept 20.20 5 0.001144 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 This tells us that sex by itself has no significant effect on the log-odds of being admitted, but the interaction effect with the department seems to be important. Hence, we can not remove any of the covariates without significantly degrading the fit. drop1(ucb_model, test = &quot;Chisq&quot;) ## Single term deletions ## ## Model: ## Admit ~ sex * Dept ## Df Deviance AIC LRT Pr(&gt;Chi) ## &lt;none&gt; 5167.3 5191.3 ## sex:Dept 5 5187.5 5201.5 20.204 0.001144 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 In GLMs with a link function (as in logistic regression), the interpretation of the parameters can be tricky. The sign of the parameter (i.e. is it positive or negative) can be interpreted as whether the covariate increases (“+”) or decreases (“-”) the odds, and hence, the probability of success. Also, the relative magnitudes tell you which covariate increases/decreases the probability more. However, the magnitude itself is not directly interpretable, i.e. you can not say “the probability of being admitted is 1.05 less for females than for males.” We can make statements about the probabilities themselves. The lsmeans function provides a nice overview of the fitted “success” (in our case “being admitted”) probabilities for the different combinations of the predictors, including a confidence interval. ucb_model_sum &lt;- lsmeans(ucb_model, ~ sex + Dept, type = &quot;response&quot;) ucb_model_sum ## sex Dept prob SE df asymp.LCL asymp.UCL ## Male A 0.379 0.0169 Inf 0.347 0.413 ## Female A 0.176 0.0366 Inf 0.115 0.259 ## Male B 0.370 0.0204 Inf 0.331 0.410 ## Female B 0.320 0.0933 Inf 0.169 0.522 ## Male C 0.631 0.0268 Inf 0.577 0.682 ## Female C 0.659 0.0195 Inf 0.620 0.696 ## Male D 0.669 0.0230 Inf 0.622 0.713 ## Female D 0.651 0.0246 Inf 0.601 0.697 ## Male E 0.723 0.0324 Inf 0.655 0.781 ## Female E 0.761 0.0215 Inf 0.716 0.800 ## Male F 0.941 0.0122 Inf 0.912 0.961 ## Female F 0.930 0.0139 Inf 0.897 0.952 ## ## Confidence level used: 0.95 ## Intervals are back-transformed from the logit scale This summary can also be grouped by one of the predictors, e.g. by the department. summary(ucb_model_sum, by = &quot;Dept&quot;) ## Dept = A: ## sex prob SE df asymp.LCL asymp.UCL ## Male 0.379 0.0169 Inf 0.347 0.413 ## Female 0.176 0.0366 Inf 0.115 0.259 ## ## Dept = B: ## sex prob SE df asymp.LCL asymp.UCL ## Male 0.370 0.0204 Inf 0.331 0.410 ## Female 0.320 0.0933 Inf 0.169 0.522 ## ## Dept = C: ## sex prob SE df asymp.LCL asymp.UCL ## Male 0.631 0.0268 Inf 0.577 0.682 ## Female 0.659 0.0195 Inf 0.620 0.696 ## ## Dept = D: ## sex prob SE df asymp.LCL asymp.UCL ## Male 0.669 0.0230 Inf 0.622 0.713 ## Female 0.651 0.0246 Inf 0.601 0.697 ## ## Dept = E: ## sex prob SE df asymp.LCL asymp.UCL ## Male 0.723 0.0324 Inf 0.655 0.781 ## Female 0.761 0.0215 Inf 0.716 0.800 ## ## Dept = F: ## sex prob SE df asymp.LCL asymp.UCL ## Male 0.941 0.0122 Inf 0.912 0.961 ## Female 0.930 0.0139 Inf 0.897 0.952 ## ## Confidence level used: 0.95 ## Intervals are back-transformed from the logit scale Similarly, we can get the odds ratio between males and females in each department. This basically tells us how different the odds are between male and female applicants in each department. contrast(ucb_model_sum, &quot;pairwise&quot;, by = &quot;Dept&quot;) ## Dept = A: ## contrast odds.ratio SE df z.ratio p.value ## Male / Female 2.864 0.752 Inf 4.005 0.0001 ## ## Dept = B: ## contrast odds.ratio SE df z.ratio p.value ## Male / Female 1.246 0.545 Inf 0.503 0.6151 ## ## Dept = C: ## contrast odds.ratio SE df z.ratio p.value ## Male / Female 0.883 0.127 Inf -0.868 0.3855 ## ## Dept = D: ## contrast odds.ratio SE df z.ratio p.value ## Male / Female 1.085 0.163 Inf 0.546 0.5852 ## ## Dept = E: ## contrast odds.ratio SE df z.ratio p.value ## Male / Female 0.819 0.164 Inf -1.000 0.3174 ## ## Dept = F: ## contrast odds.ratio SE df z.ratio p.value ## Male / Female 1.208 0.369 Inf 0.619 0.5359 ## ## Tests are performed on the log odds ratio scale This tells us that sex seems to make a significant difference in Department \\(A\\) but not so in the other departments. This, in turn, causes the significant interaction effect we saw before. 5.8.3.1 Exercise: Logistic GLM In the plasma data (from the HSAUR3 package), use logistic regression to estimate the probabilities of ESR &gt; 20, given the level of fibrinogen in the blood. Using the womensrole data set from the HSAUR3 package, try to fit a logistic regression to the agreement with the statement, given the years of education and the respondent’s sex (also attributed as gender in these data). 5.8.4 Count data (family = poisson) When we are dealing with count data, the Poisson distribution is often used as a model. The Poisson distribution models the number of events during a fixed period of time (or space, etc.). It is completely characterized by the rate parameter \\(\\mu\\). Both the mean and the variance of the Poisson distribution are equal to the rate parameter. In other words, a larger rate also implies a larger spread of the data. This is a rather strong assumption and we will learn how to check if this assumption is reasonable for a given data set. The usual link function for Poisson GLMs is the log, so our GLM for the rate \\(\\mu\\) is: \\[ \\log (\\mu) = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\cdots + \\beta_p x_p. \\] Let’s use this model on the polyps data set (pre-installed). We want to explain the number of colonic polyps at 12 months by means of the age of the patient and whether they are in the treatment group. polyps_model1 &lt;- glm(number ~ treat + age, data = polyps, family = poisson) summary(polyps_model1) ## ## Call: ## glm(formula = number ~ treat + age, family = poisson, data = polyps) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -4.2212 -3.0536 -0.1802 1.4459 5.8301 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 4.529024 0.146872 30.84 &lt; 2e-16 *** ## treatdrug -1.359083 0.117643 -11.55 &lt; 2e-16 *** ## age -0.038830 0.005955 -6.52 7.02e-11 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for poisson family taken to be 1) ## ## Null deviance: 378.66 on 19 degrees of freedom ## Residual deviance: 179.54 on 17 degrees of freedom ## AIC: 273.88 ## ## Number of Fisher Scoring iterations: 5 We assumed that the mean and the variance are equal. But how close is this assumption to the truth? R supports the “quasi-” family which allows for the variance to be different. polyps_model2 &lt;- glm(number ~ treat + age, data = polyps, family = quasipoisson) summary(polyps_model2) ## ## Call: ## glm(formula = number ~ treat + age, family = quasipoisson, data = polyps) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -4.2212 -3.0536 -0.1802 1.4459 5.8301 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 4.52902 0.48106 9.415 3.72e-08 *** ## treatdrug -1.35908 0.38533 -3.527 0.00259 ** ## age -0.03883 0.01951 -1.991 0.06284 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for quasipoisson family taken to be 10.72805) ## ## Null deviance: 378.66 on 19 degrees of freedom ## Residual deviance: 179.54 on 17 degrees of freedom ## AIC: NA ## ## Number of Fisher Scoring iterations: 5 The fitted quasi-Poisson model results in dispersion parameter different from 1, which makes the assumption of equal mean and variance highly questionable. If the assumption of equal mean and variance is wrong, the standard errors of the parameters are grossly underestimated. The uncoupling of the mean and the variance does not change the parameter estimates, but the significance of the parameters in the model will be different. Similarly to logistic regression, we can investigate the difference in the rate between two levels of a categorical covariate: polyps_model2_sum &lt;- lsmeans(polyps_model2, ~ treat, type = &quot;response&quot;) contrast(polyps_model2_sum, &quot;pairwise&quot;) ## contrast ratio SE df z.ratio p.value ## placebo / drug 3.89 1.5 Inf 3.527 0.0004 ## ## Tests are performed on the log scale The significant (p= \\(4.2012649\\times 10^{-4}\\)) rate.ratio of \\(3.8926236\\) tells us that the rate of the number of colonic polyps at 12 months for subjects in the placebo group is \\(3.8926236\\) times higher than for subjects in the treatment group. 5.8.4.1 Exercise: Poisson GLM Check which covariates have a significant effect on the response in the model fitted with the Poisson family and with the quasi-Poisson family and compare the results. What do you observe? 5.8.5 Negative binomial model for count data An over-dispersed Poisson regression model (i.e. fitted with quasi-Poisson) is similar to a Negative Binomial (NB) regression model. This can be fitted with the glm.nb function from the MASS package. For instance, we can model the days absent from school based on the sex of students, their age, ethnic background, and learner status. These data are within quine from this same package. quine_model &lt;- glm.nb(Days ~ Sex * (Age + Eth * Lrn), data = quine) ## equivalent to ## quine_model &lt;- glm.nb(Days ~ Sex * Age + Sex * Eth * Lrn, data = quine) summary(quine_model) ## ## Call: ## glm.nb(formula = Days ~ Sex * (Age + Eth * Lrn), data = quine, ## init.theta = 1.597990733, link = log) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.8950 -0.8827 -0.2299 0.5669 2.1071 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 3.01919 0.29706 10.163 &lt; 2e-16 *** ## SexM -0.47541 0.39550 -1.202 0.229355 ## AgeF1 -0.70887 0.32321 -2.193 0.028290 * ## AgeF2 -0.61486 0.37141 -1.655 0.097826 . ## AgeF3 -0.34235 0.32717 -1.046 0.295388 ## EthN -0.07312 0.26539 -0.276 0.782908 ## LrnSL 0.94358 0.32246 2.926 0.003432 ** ## EthN:LrnSL -1.35849 0.37719 -3.602 0.000316 *** ## SexM:AgeF1 -0.01486 0.46225 -0.032 0.974353 ## SexM:AgeF2 1.24328 0.46134 2.695 0.007040 ** ## SexM:AgeF3 1.49319 0.45337 3.294 0.000989 *** ## SexM:EthN -0.60586 0.36896 -1.642 0.100572 ## SexM:LrnSL -0.70467 0.46536 -1.514 0.129966 ## SexM:EthN:LrnSL 2.11991 0.58056 3.651 0.000261 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for Negative Binomial(1.598) family taken to be 1) ## ## Null deviance: 234.56 on 145 degrees of freedom ## Residual deviance: 167.56 on 132 degrees of freedom ## AIC: 1093 ## ## Number of Fisher Scoring iterations: 1 ## ## ## Theta: 1.598 ## Std. Err.: 0.213 ## ## 2 x log-likelihood: -1063.025 Anova(quine_model) ## Analysis of Deviance Table (Type II tests) ## ## Response: Days ## LR Chisq Df Pr(&gt;Chisq) ## Sex 0.9284 1 0.3352783 ## Age 14.9609 3 0.0018503 ** ## Eth 16.9573 1 3.823e-05 *** ## Lrn 5.6903 1 0.0170588 * ## Eth:Lrn 2.5726 1 0.1087268 ## Sex:Age 19.8297 3 0.0001841 *** ## Sex:Eth 0.6547 1 0.4184372 ## Sex:Lrn 1.4965 1 0.2212106 ## Sex:Eth:Lrn 12.9647 1 0.0003174 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 From the ANOVA table we see that several terms are not significant, but the highest order term involving any of the four factors is significant. Therefore we cannot remove any terms from the model. drop1(quine_model, test = &quot;Chisq&quot;) ## Single term deletions ## ## Model: ## Days ~ Sex * (Age + Eth * Lrn) ## Df Deviance AIC LRT Pr(&gt;Chi) ## &lt;none&gt; 167.56 1091.0 ## Sex:Age 3 187.39 1104.8 19.830 0.0001841 *** ## Sex:Eth:Lrn 1 180.52 1102.0 12.965 0.0003174 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 We can use lsmeans to compare groups but the relationships are hard to interpret because there are high order interaction terms in the model. quine_model_sum1 &lt;- lsmeans(quine_model, ~ Sex + Eth + Lrn, type = &quot;response&quot;) summary(quine_model_sum1, by = c(&quot;Sex&quot;, &quot;Eth&quot;)) ## Sex = F, Eth = A: ## Lrn response SE df asymp.LCL asymp.UCL ## AL 13.50 2.78 Inf 9.01 20.2 ## SL 34.68 7.63 Inf 22.53 53.4 ## ## Sex = M, Eth = A: ## Lrn response SE df asymp.LCL asymp.UCL ## AL 16.57 3.12 Inf 11.46 24.0 ## SL 21.04 5.73 Inf 12.35 35.9 ## ## Sex = F, Eth = N: ## Lrn response SE df asymp.LCL asymp.UCL ## AL 12.55 2.49 Inf 8.50 18.5 ## SL 8.29 1.87 Inf 5.33 12.9 ## ## Sex = M, Eth = N: ## Lrn response SE df asymp.LCL asymp.UCL ## AL 8.40 1.60 Inf 5.78 12.2 ## SL 22.85 5.76 Inf 13.94 37.5 ## ## Results are averaged over the levels of: Age ## Confidence level used: 0.95 ## Intervals are back-transformed from the log scale summary(quine_model_sum1, by = c(&quot;Eth&quot;, &quot;Lrn&quot;)) ## Eth = A, Lrn = AL: ## Sex response SE df asymp.LCL asymp.UCL ## F 13.50 2.78 Inf 9.01 20.2 ## M 16.57 3.12 Inf 11.46 24.0 ## ## Eth = N, Lrn = AL: ## Sex response SE df asymp.LCL asymp.UCL ## F 12.55 2.49 Inf 8.50 18.5 ## M 8.40 1.60 Inf 5.78 12.2 ## ## Eth = A, Lrn = SL: ## Sex response SE df asymp.LCL asymp.UCL ## F 34.68 7.63 Inf 22.53 53.4 ## M 21.04 5.73 Inf 12.35 35.9 ## ## Eth = N, Lrn = SL: ## Sex response SE df asymp.LCL asymp.UCL ## F 8.29 1.87 Inf 5.33 12.9 ## M 22.85 5.76 Inf 13.94 37.5 ## ## Results are averaged over the levels of: Age ## Confidence level used: 0.95 ## Intervals are back-transformed from the log scale quine_model_sum2 &lt;- lsmeans(quine_model, ~ Sex + Age, type = &quot;response&quot;) summary(quine_model_sum2, by = &quot;Sex&quot;) ## Sex = F: ## Age response SE df asymp.LCL asymp.UCL ## F0 22.53 6.11 Inf 13.23 38.35 ## F1 11.09 1.73 Inf 8.16 15.06 ## F2 12.18 2.68 Inf 7.92 18.74 ## F3 16.00 3.70 Inf 10.16 25.19 ## ## Sex = M: ## Age response SE df asymp.LCL asymp.UCL ## F0 12.36 2.58 Inf 8.21 18.60 ## F1 5.99 1.49 Inf 3.68 9.76 ## F2 23.16 4.26 Inf 16.15 33.22 ## F3 39.05 9.80 Inf 23.88 63.87 ## ## Results are averaged over the levels of: Eth, Lrn ## Confidence level used: 0.95 ## Intervals are back-transformed from the log scale summary(quine_model_sum2, by = &quot;Age&quot;) ## Age = F0: ## Sex response SE df asymp.LCL asymp.UCL ## F 22.53 6.11 Inf 13.23 38.35 ## M 12.36 2.58 Inf 8.21 18.60 ## ## Age = F1: ## Sex response SE df asymp.LCL asymp.UCL ## F 11.09 1.73 Inf 8.16 15.06 ## M 5.99 1.49 Inf 3.68 9.76 ## ## Age = F2: ## Sex response SE df asymp.LCL asymp.UCL ## F 12.18 2.68 Inf 7.92 18.74 ## M 23.16 4.26 Inf 16.15 33.22 ## ## Age = F3: ## Sex response SE df asymp.LCL asymp.UCL ## F 16.00 3.70 Inf 10.16 25.19 ## M 39.05 9.80 Inf 23.88 63.87 ## ## Results are averaged over the levels of: Eth, Lrn ## Confidence level used: 0.95 ## Intervals are back-transformed from the log scale 5.8.5.1 Exercise: Quasi-Poisson vs. negative binomial GLM Fit the above model with a quasi-Poisson family and check the over-dispersion in that fit. Is there a difference in the significance of any terms compared to the NB model? Would a Poisson model be appropriate as well? 5.9 Survey Please provide us with feedback through this short survey. "],["intermediate-r-programming-1.html", "Chapter 6 Intermediate R programming 6.1 Introduction 6.2 Objects, classes, and attributes 6.3 S3 objects 6.4 S4 objects 6.5 Functions 6.6 Building a function 6.7 Packages 6.8 Creating a package project 6.9 Further resources 6.10 Survey", " Chapter 6 Intermediate R programming 6.1 Introduction In this workshop, you will learn to use R as a programming environment, allowing you to write more complex, yet clearer data analysis code. We will teach you three fundamental concepts of R programming: classes, functions, and packages. You will learn how to: access specific parts of an object and its attributes, use different classes of objects in generic R functions, create your own generic functions, structure a package and package libraries, and create your own package and distribute it to other R users. This is an intermediate workshop series that assumes prior experience with R (such as that in our Introduction to R workshop). 6.1.1 Setup instructions Please come to the workshop with your laptop setup with the required software and data files as described in our setup instructions. 6.1.2 Background Please read Hallam SJ et al. 2017. Sci Data 4: 170158 “Monitoring microbial responses to ocean deoxygenation in a model oxygen minimum zone” to learn more about the data used in this workshop. You can also check out this short video showing how the sampling was done! 6.1.3 Data description The data in this workshop were collected as part of an on-going oceanographic time series program in Saanich Inlet, a seasonally anoxic fjord on the East coast of Vancouver Island, British Columbia (Figure 1). / Figure 1. Map of Saanich Inlet indicating conventional sample collection stations (S1-S9). Data used in this workshop is sourced from S3. Saanich Inlet is a steep sided fjord characterized by a shallow glacial sill located at the mouth of the inlet that restricts circulation in basin waters below 100 m (Figure 2). / Figure 2. Structure of Saanich Inlet. The glacial sill restricts water circulation into and out of the lower depth of the inlet basin. During spring and summer months, elevated primary production (like photosynthesis) in surface waters combined with restricted circulation results in progressive water column stratification and complete oxygen starvation (anoxia) in deep basin waters. In late summer, pulses of oxygenated nutrient-rich ocean waters upwelling from the Haro Straight cascade over the sill, displacing oxygen starved bottom waters upward. The intensity of these renewal events varies from year to year with implications for microbial ecology and biogeochemical cycles (Figure 3). / Figure 3. Contour plot of water column oxygen concentrations over multiple years in the time series. Warmer colors indicate high oxygen concentrations while cooler colors are low. Note the recurring pattern of oxygen decline below 100 m depth intervals followed by seasonal renewal events in late Summer into early Fall carrying more oxygenated waters into the Inlet. The seasonal cycle of stratification and deep water renewal enables spatial and temporal profiling across a wide range of water column energy states and nutrients, thus making Saanich Inlet a model ecosystem for studying microbial community responses to ocean deoxygenation. Ocean deoxygenation is a widespread phenomenon currently increasing due to climate change. The data we will use in this workshop include various geochemical measurements at many depths in Saanich Inlet. Samples were taken approximately monthly from 2006 to 2014, though there is much missing data to contend with. For a brief introduction to the data used in this workshop series, see Hallam SJ et al. 2017. Sci Data 4: 170158 “Monitoring microbial responses to ocean deoxygenation in a model oxygen minimum zone.” More detailed information on the environmental context and time series data can be found in Torres-Beltrán M et al. 2017. Sci Data 4: 170159. “A compendium of geochemical information from the Saanich Inlet water column.” 6.1.4 Making an RStudio project Projects allow you to divide your work into self-contained contexts. Let’s create a project to work in. In the top-right corner of your RStudio window, click the “Project: (None)” button to show the projects dropdown menu. Select “New Project…” &gt; “New Directory” &gt; “New Project.” Under directory name, input “intermediate_R” and choose a parent directory to contain this project on your computer. 6.1.5 Installing and loading packages At the beginning of every R script, you should have a dedicated space for loading R packages. R packages allow any R user to code reproducible functions and share them with the R community. Packages exist for anything ranging from microbial ecology to complex graphics to multivariate modeling and beyond. In this workshop, we will use several packages within the tidyverse as well as several development tools listed below. Here, we load the necessary packages which must already be installed (see setup instructions for details). library(tidyverse) # Easily Install and Load the &#39;Tidyverse&#39; library(lmerTest) # Tests in Linear Mixed Effects Models library(devtools) # Tools to Make Developing R Packages Easier library(roxygen2) # In-Line Documentation for R library(testthat) # Unit Testing for R library(usethis) # Automate Package and Project Setup 6.1.6 Downloading data The following command downloads the data from our GitHub and since you’re working in a Project, saves it in the Project directory under the data directory on your computer. write.csv( read.csv(&quot;https://raw.githubusercontent.com/EDUCE-UBC/educer/main/data-raw/data_intermediate_ws.csv&quot;), &quot;data/Saanich_Data_clean.csv&quot;, row.names=FALSE) 6.1.7 Load data Let’s start by reading in the data we will use for this workshop. Here, we specify that the data file is named “Saanich_Data_clean.csv.” Furthermore, we specify that the first row of the data contain variable names, and that empty, NA, NAN, or ND values should be treated by R as missing data (coded in R as NA). dat &lt;- read_csv(file=&quot;data/Saanich_Data_clean.csv&quot;, col_names=TRUE, na=c(&quot;&quot;, &quot;NA&quot;, &quot;NAN&quot;, &quot;ND&quot;)) ## ## ── Column specification ──────────────────────────────────────────────────────── ## cols( ## Cruise = col_double(), ## Date = col_date(format = &quot;&quot;), ## Depth = col_double(), ## O2_uM = col_double(), ## NO3_uM = col_double(), ## H2S_uM = col_double(), ## Depth_m = col_double() ## ) If you would like to learn more about how these data were cleaned and manipulated, please see our R tidyverse workshop. 6.2 Objects, classes, and attributes Practically speaking, everything you encounter in R is an object. Everything your store in your work-space is an object. Built-in functions are objects. Data sets are objects. ggplot2 graphics are objects (base R graphics are not). Every object has a class, and many objects have attributes. You can use the class function to determine an objects class and the attributes function to see its attributes. 6.3 S3 objects 6.3.1 Vectors Vectors are the most common type of object in R. In fact, a scalar is a vector of length 1, and matrix is a vector of vectors! Once you store a vector in your R environment, you have created an R object and… Every object has a mode and storage mode. Vectors have an inherent class based on their mode. There are 5 data types that are stored in basic (atomic) vectors class mode storage.mode numeric numeric double integer numeric integer character character character logical logical logical complex varies varies 6.3.1.1 Numeric x &lt;- 1 class(x) ## [1] &quot;numeric&quot; mode(x) ## [1] &quot;numeric&quot; 6.3.1.2 Integer x &lt;- 1L class(x) ## [1] &quot;integer&quot; mode(x) ## [1] &quot;numeric&quot; 6.3.1.3 Character x &lt;- &quot;1L&quot; class(x) ## [1] &quot;character&quot; mode(x) ## [1] &quot;character&quot; 6.3.1.4 Logical x &lt;- TRUE class(x) ## [1] &quot;logical&quot; mode(x) ## [1] &quot;logical&quot; 6.3.1.5 Matrix (complex) x &lt;- c(1,1) dim(x) &lt;- c(2,1) class(x) ## [1] &quot;matrix&quot; &quot;array&quot; mode(x) ## [1] &quot;numeric&quot; attributes(x) ## $dim ## [1] 2 1 6.3.1.6 Exercise: Working with vectors Assign x the value \"a\". What are its class and mode? Give it dimensions c(1,1). What are its class and mode? 6.3.2 Complex objects Complex objects can be vectors at heart (like the matrix above) but not all are. Since everything in R is an object, you can determine the class and attributes of pretty much anything in R. 6.3.2.1 Built in functions class ## function (x) .Primitive(&quot;class&quot;) class(class) ## [1] &quot;function&quot; attributes(class) ## NULL 6.3.2.2 Graphics p1 &lt;- dat %&gt;% ggplot(aes(x=O2_uM, y=Depth_m)) + geom_point() + geom_smooth(method=&quot;lm&quot;) + scale_y_reverse(limits=c(200, 0)) + labs(x=expression(O[2]*&quot; &quot;*(mu*g/mL)), y=&quot;Depth (m)&quot;, title=&quot;Oxygen decreases with depth and is less variable at lower depths&quot;) p1 ## `geom_smooth()` using formula &#39;y ~ x&#39; class(p1) ## [1] &quot;gg&quot; &quot;ggplot&quot; attributes(p1) ## $names ## [1] &quot;data&quot; &quot;layers&quot; &quot;scales&quot; &quot;mapping&quot; &quot;theme&quot; ## [6] &quot;coordinates&quot; &quot;facet&quot; &quot;plot_env&quot; &quot;labels&quot; ## ## $class ## [1] &quot;gg&quot; &quot;ggplot&quot; 6.3.2.3 Statistical models m1 &lt;- lm(O2_uM ~ Depth_m, data=dat) m1 ## ## Call: ## lm(formula = O2_uM ~ Depth_m, data = dat) ## ## Coefficients: ## (Intercept) Depth_m ## 190.527 -1.184 class(m1) ## [1] &quot;lm&quot; attributes(m1) ## $names ## [1] &quot;coefficients&quot; &quot;residuals&quot; &quot;effects&quot; &quot;rank&quot; ## [5] &quot;fitted.values&quot; &quot;assign&quot; &quot;qr&quot; &quot;df.residual&quot; ## [9] &quot;xlevels&quot; &quot;call&quot; &quot;terms&quot; &quot;model&quot; ## ## $class ## [1] &quot;lm&quot; 6.3.2.4 Summaries s1 &lt;- summary(m1) s1 ## ## Call: ## lm(formula = O2_uM ~ Depth_m, data = dat) ## ## Residuals: ## Min 1Q Median 3Q Max ## -173.900 -32.462 -2.995 31.631 164.641 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 190.52676 2.63921 72.19 &lt;2e-16 *** ## Depth_m -1.18384 0.02274 -52.06 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 43.25 on 1247 degrees of freedom ## Multiple R-squared: 0.6849, Adjusted R-squared: 0.6847 ## F-statistic: 2711 on 1 and 1247 DF, p-value: &lt; 2.2e-16 class(s1) ## [1] &quot;summary.lm&quot; attributes(s1) ## $names ## [1] &quot;call&quot; &quot;terms&quot; &quot;residuals&quot; &quot;coefficients&quot; ## [5] &quot;aliased&quot; &quot;sigma&quot; &quot;df&quot; &quot;r.squared&quot; ## [9] &quot;adj.r.squared&quot; &quot;fstatistic&quot; &quot;cov.unscaled&quot; ## ## $class ## [1] &quot;summary.lm&quot; 6.3.3 Data objects Data sets are also complex objects that can be stored as several different classes in R - some vector-based, some not. In general, data sets are organized into what we would call a “table” outside of R (something with rows and columns). However, in R, a “table” can be stored as several different classes: table, matrix, data frame, tibble, or array. Though they may contain the same information, each has specific characteristics and attributes that impact how functions interact with it. 6.3.3.1 Table, matrix, data frame, tibble, or array? So, which class should be used for a given data set? In general: Matrices, data frames, and tibbles are what we traditionally think of as a table with rows and columns. Tables are a specific type of data frame like summaries or contingency tables. Arrays are vectors with a dimensions attribute like a vector that contains 2 data frames. Practically speaking, the data sets you will work with will be matrices, data frames, or tibbles, and you will only encounter tables and arrays as outputs of analyses in R. Thus, you only need to decide between 3 classes for your data. 6.3.3.2 Matrix, data frame, or tibble? And in fact, a tibble is just how a data frame is stored in the tidyverse. For all intents and purposes, a tibble is a data frame. Functionally, they are the same and should be considered equivalent when used in R. Tibbles merely simplify data frames a little by: Printing only the first ten rows and all the columns that fit on one screen in the console (making large data frames easier to work with) Subsetting to another tibble no matter what (data frames sometimes subset to another data frame, sometimes to a vector) Thus, you only really need to decide between 2 classes for your data. 6.3.3.3 Data frame or matrix? To answer this, let’s take a look at our Saanich data. Currently these data are stored in our R environment as a tibble (i.e. data frame) because we read them in using the tidyverse function read_csv. We will subset to only the first 5 rows and columns so that we can see the attributes of these data more easily. dat.df &lt;- dat[1:5, 1:5] class(dat.df) ## [1] &quot;tbl_df&quot; &quot;tbl&quot; &quot;data.frame&quot; We can reformat these data to a matrix. dat.mat &lt;- as.matrix(dat.df) class(dat.mat) ## [1] &quot;matrix&quot; &quot;array&quot; When we look at the data in these two formats, they look the same. dat.df ## # A tibble: 5 x 5 ## Cruise Date Depth O2_uM NO3_uM ## &lt;dbl&gt; &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 18 2008-02-13 0.01 225. 18.9 ## 2 18 2008-02-13 0.02 221. 24.0 ## 3 18 2008-02-13 0.04 202. 29.2 ## 4 18 2008-02-13 0.06 198. 24.0 ## 5 18 2008-02-13 0.075 194. 23.8 dat.mat ## Cruise Date Depth O2_uM NO3_uM ## [1,] &quot;18&quot; &quot;2008-02-13&quot; &quot;0.010&quot; &quot;225.237&quot; &quot;18.871&quot; ## [2,] &quot;18&quot; &quot;2008-02-13&quot; &quot;0.020&quot; &quot;220.985&quot; &quot;23.974&quot; ## [3,] &quot;18&quot; &quot;2008-02-13&quot; &quot;0.040&quot; &quot;201.769&quot; &quot;29.204&quot; ## [4,] &quot;18&quot; &quot;2008-02-13&quot; &quot;0.060&quot; &quot;197.947&quot; &quot;23.981&quot; ## [5,] &quot;18&quot; &quot;2008-02-13&quot; &quot;0.075&quot; &quot;193.762&quot; &quot;23.831&quot; But they are fundamentally different objects. attributes(dat.df) ## $names ## [1] &quot;Cruise&quot; &quot;Date&quot; &quot;Depth&quot; &quot;O2_uM&quot; &quot;NO3_uM&quot; ## ## $row.names ## [1] 1 2 3 4 5 ## ## $class ## [1] &quot;tbl_df&quot; &quot;tbl&quot; &quot;data.frame&quot; attributes(dat.mat) ## $dim ## [1] 5 5 ## ## $dimnames ## $dimnames[[1]] ## NULL ## ## $dimnames[[2]] ## [1] &quot;Cruise&quot; &quot;Date&quot; &quot;Depth&quot; &quot;O2_uM&quot; &quot;NO3_uM&quot; A matrix is a long vector with an attribute (dim) that gives an easier way to access similar items. Each row (water sample) and column (same type of measurement) should have something in common. Internally, the data stacks the columns. A matrix is accessed like a vector. A data frame is a special type of list. Each element of the list is a vector and each vector must be the same length. The same element number of each vector should have something in common (the same water sample). A data frame is accessed like a list. So, if a function works with vectors, you should use a matrix. If a function works with lists, you should use a data frame. Practically speaking, you should always store your data in a data frame/tibble so that you can work with the tidyverse and simply convert it to a matrix with as.matrix within any function that requires that formatting. Also, R can tell you the functions that work on a given class with methods, which will show you functions that are loaded and can run on the given class type. A package has to be loaded into the current R session for methods to list functions from that package that would work. methods(class=&quot;data.frame&quot;) ## [1] [ [[ [[&lt;- [&lt;- ## [5] %within% $&lt;- aggregate anti_join ## [9] anyDuplicated anyNA arrange_ arrange ## [13] as_factor as_tibble as.col_spec as.data.frame ## [17] as.list as.matrix as.tbl augment ## [21] auto_copy Boxplot brief by ## [25] cbind coerce coerce&lt;- collapse ## [29] collect complete_ complete compute ## [33] corresp count dim dimnames ## [37] dimnames&lt;- distinct_ distinct do_ ## [41] do dplyr_col_modify dplyr_reconstruct dplyr_row_slice ## [45] drop_na_ drop_na droplevels duplicated ## [49] edit expand_ expand extract_ ## [53] extract factorize fill_ fill ## [57] filter_ filter format formula ## [61] fortify full_join gather_ gather ## [65] ggplot_add glance glimpse group_by_ ## [69] group_by group_data group_indices_ group_indices ## [73] group_keys group_map group_modify group_nest ## [77] group_size group_split group_trim group_vars ## [81] groups head HSAURtable initialize ## [85] inner_join intersect is.na lda ## [89] left_join loglm1 Math merge ## [93] mutate_ mutate n_groups na.contiguous ## [97] na.exclude na.omit nest_by nest_join ## [101] nest_legacy nest Ops pivot_longer ## [105] pivot_wider plot print prompt ## [109] pull qda rbind relocate ## [113] rename_ rename_with rename replace_na ## [117] right_join row.names row.names&lt;- rows_delete ## [121] rows_insert rows_patch rows_update rows_upsert ## [125] rowsum rowwise S same_src ## [129] sample_frac sample_n select_ select ## [133] semi_join separate_ separate_rows_ separate_rows ## [137] separate setdiff setequal show ## [141] slice_ slice_head slice_max slice_min ## [145] slice_sample slice_tail slice slotsFromS3 ## [149] some split split&lt;- spread_ ## [153] spread stack str strings2factors ## [157] subset summarise_ summarise summary ## [161] Summary t tail tally ## [165] tbl_vars tidy transform transmute_ ## [169] transmute type.convert ungroup union_all ## [173] union unique unite_ unite ## [177] unnest_legacy unnest unstack whichNames ## [181] within ## see &#39;?methods&#39; for accessing help and source code 6.3.3.4 Exercise: Class and attributes Obtain the summary table of dat. What are its class and attributes? Read in the data table Saanich_Data.csv using the base R function read.table. What are this object’s class and attributes? Are they any different from the object create when we used read_csv to read in the same data? 6.3.4 The R list object You have already worked with an R list object - the data frame. When you call a specific variable in a data frame with dataframe$variable, you are actually calling the vector of the variable contained within the list of vectors that make up the dataframe. This can apply more generally to any list object with listName$elementName. So, what is a list? Simply speaking, it is a more complicated vector. Like a basic vector, it can contain any number of elements including zero. Unlike a basic vector, where each element is a single piece of data of the same type, each element of a list can be any R object and each object can be a different type. A list can even contain an object that is itself a list! Being able to contain objects of different types also differentiates lists from arrays since arrays are vectors requiring all elements to be the same type. For example, we can make a list that contains a variety of objects like so. x &lt;- list(data = dat, Function = lm, model = m1, random = list(sample(LETTERS, 5), sample(1:1000, 5))) class(x) ## [1] &quot;list&quot; mode(x) ## [1] &quot;list&quot; length(x) ## [1] 4 attributes(x) ## $names ## [1] &quot;data&quot; &quot;Function&quot; &quot;model&quot; &quot;random&quot; x$random ## [[1]] ## [1] &quot;I&quot; &quot;G&quot; &quot;Y&quot; &quot;J&quot; &quot;V&quot; ## ## [[2]] ## [1] 919 538 235 289 185 Due to the random nature of sample, your letters and numbers in random may be different. Similarly, we can pull out pieces of statistical model fitted in R since the output is a list object. For example, we previously fit a linear model of oxygen by depth with m1 &lt;- lm(O2 ~ Depth, data=dat). If we wanted to see only the coefficients of this model, we would use m1$coefficients ## (Intercept) Depth_m ## 190.526759 -1.183835 6.3.4.1 Exercise: R lists Obtain the summary() of m1 and save it as m2. What is the class and mode of m2? Using a single line of code, pull out just the p-values from m2. Hint: You will need to use both $ and [ ]. 6.4 S4 objects So far, everything we’ve talked about has been S3 objects. However, there is another class of object in R - S4. It is up the the package developer to determine if a function outputs as an S3 or S4 object, so you may come across both classes during your data analysis. S4 objects operate differently than S3, so let’s see an example with lmer (linear mixed effects model) from the lmerTest package. Here, we will determine if oxygen concentration in Saanich Inlet differs by year (represented by Cruise #) knowing that we repeatedly measured the same depths over time. m3 &lt;- lmer(O2_uM ~ Cruise + (0 + Cruise | Depth_m), dat) class(m3) ## [1] &quot;lmerModLmerTest&quot; ## attr(,&quot;package&quot;) ## [1] &quot;lmerTest&quot; mode(m3) ## [1] &quot;S4&quot; # Many of the objects in m3 are very large so we will just look at the names here names(attributes(m3)) ## [1] &quot;vcov_varpar&quot; &quot;Jac_list&quot; &quot;vcov_beta&quot; &quot;sigma&quot; &quot;resp&quot; ## [6] &quot;Gp&quot; &quot;call&quot; &quot;frame&quot; &quot;flist&quot; &quot;cnms&quot; ## [11] &quot;lower&quot; &quot;theta&quot; &quot;beta&quot; &quot;u&quot; &quot;devcomp&quot; ## [16] &quot;pp&quot; &quot;optinfo&quot; &quot;class&quot; For your purposes, the major difference between S3 and S4 is that to access S4 data, you need to use @ instead of $, m3@beta ## [1] 85.2611236 -0.5859695 Also, elements of S4 class objects can be S3 class objects. VC &lt;- VarCorr(m3) class(VC) ## [1] &quot;VarCorr.merMod&quot; attributes(VC) ## $names ## [1] &quot;Depth_m&quot; ## ## $sc ## [1] 44.93441 ## ## $useSc ## [1] TRUE ## ## $class ## [1] &quot;VarCorr.merMod&quot; 6.4.0.1 Exercise: S4 objects Compute and store the variance-covariance matrix of m3 using vcov(). What class and mode is it? What elements does it contain? What are the dimensions of factors within this object? 6.5 Functions In math, a function is a rule for providing an output given some input. So, if you give a function an input, you know it will always provide the same output. So if \\(f(3) = 2\\), it will always be so, but it won’t tell us anything about what \\(f(4)\\) is. In software, this definition is often relaxed but not for R (most of the time). This means that you can trust a function to do the same thing every time you give it the same data. We call such a function a pure function. An example of a pure function in R is sqrt(x) \\(= \\sqrt{x}\\), because it will always have the same output per input. An example of a non-pure function in R is setwd(), because it changes the current working directory and thus, can have side-effects on other code. You may want to turn your code into a function for a number of reasons. Common reasons are: To make your code more user friendly To avoid duplicating code To prevent mistakes and errors To break a large problem into smaller chunks Here, we will build a function to test the relationship between depth and all the geochemical variables for a given cruise in the Saanich data set. This will show you several aspects of coding functions including: creating a generic function for loops accessing data from complex outputs printing to a table saving results to the hard-drive 6.5.1 Basics Here is the basic syntax of a function in R. function(argument1, argument2) # &#39;function&#39; is a function that makes functions { # Body # This code runs when you call the function a = argument1 + argument2 b = argument1 * argument2 return( a + b ) } Functions have class function and are R objects just like any other. class(function(){}) ## [1] &quot;function&quot; If we author a function, it is likely that we would like to use it later. We can do this by assigning it to a variable. Let’s create a function to calculate the square of a number. We save this function as f. f &lt;- function(x) { return(x*x) } Then, we can call this function, give it a value for x, and it will return x2. f(5) ## [1] 25 Remember that we are only calling a function that happens to be stored in an R function. We can still view the function stored by f. f ## function(x) ## { ## return(x*x) ## } 6.5.1.1 Exercise: Writing a function Put the following math into a function \\[ f(x) = 1 + 2x - 5x^2 + x^3 \\] Set x to 1:1000/1000*6-1 Plot the results with plot(x, f(x) , main=&quot;The answer looks like this&quot;) 6.5.2 Arguments A function’s arguments are the ‘inputs.’ You can input arguments by name or by order in your function just like you can with R’s built-in functions. For example, if we create a function to calculate the division of two numbers f &lt;- function(a,b){ out = a/b return(out) } We can either give the arguments by name, which allows us to input them in any order. f(a=1, b=2) ## [1] 0.5 f(b=2, a=1) ## [1] 0.5 Or we can input them without names but in the same order as in the function. f(1, 2) ## [1] 0.5 Because in this second case, a different order yields a different result. f(2, 1) ## [1] 2 6.5.3 Triple-dot argument The triple-dot argument ... allows you some flexibility in your custom function by allowing additional arguments not defined in the original function( ). These additional arguments are passed to sub-functions within the larger function( ). A common culprit from ... is the plot function. Here, we will plot the cosine of x and use ... so that we can add parameters to the plot after the fact. f &lt;- function(x, ...) { y = cos(x) plot(x, y, ...) } f(1:1000/50) f(1:1000/50, type=&#39;l&#39;) f(1:1000/50, type=&#39;l&#39;, col=&quot;purple&quot;) By explicitly specifying parameters for the plot function, we can lazily and creatively add style to the plot without including these parameters in our original function( ). 6.5.4 Scoping Scoping is a method used by R to decide what the value of a variable is. R first looks for the variable as defined in the function or its arguments and second looks for the variable in the global environment (all that stuff in the upper right panel in RStudio). There are some additional rules that exist for efficiency’s sake, but these are the ones that matter to you. Thus, it is possible to define a function that references a variable that is not defined within that function. For one, you can define a variable globally and then use that variable as-is within your function. z &lt;- 1 # Define z globally f &lt;- function(x) # no local definition of z is given { return(x + z) } f(x=2) # Failing to find z in the function or inputs, R looks for it globally and finds z = 1 to result in 2 + 1 = 3 ## [1] 3 Or you can override globally defined variables within your function. z &lt;- 1 # Define z globally f &lt;- function(x) { z = 2 # local definition of z overrides global definition return(x + z) } f(x=2) # R finds the function&#39;s definition of z=2 first and thus gives 2 + 2 = 4 ## [1] 4 However, if your function calls for an input that is defined globally, it will fail. z &lt;- 1 f &lt;- function(x, z) # function calls for inputs for both x and z { return(x + z) } f(x=2) # Function expects inputs for x and z but not finding z, fails ## Error in f(x = 2): argument &quot;z&quot; is missing, with no default Or if a variable is defined neither in the function nor in the global environment, your function will not run. y ## [1] 4 5 6 7 f = function(x) { y = 2*y # y does not exist anywhere return(x + y) } f(3) ## [1] 11 13 15 17 6.5.4.1 Exercise: Scoping For the following, try to determine what the function will return without running the code then check yourself by running it. Remove all instances of x, z, and f( ) from your environment so that you are starting fresh for this exercise. rm(x) rm(z) rm(f) What happens when we run f()? Why? f &lt;- function() { return(2*x) } f() What will f() return? Why? x &lt;- 1 f &lt;- function() { x = 2 return(2*x) } f() What does the final y call return? y &lt;- 1 f &lt;- function(x) { y = x+2 return(x*y) } f(1) y 6.6 Building a function Now that we understand the basics of functions, let’s get to work building our function to test the relationship between depth and geochemical variables in Saanich Inlet. Each step will add to our growing function. Look for ### on lines that are changed in each step. 6.6.1 Step 1. Define and test your task The first thing we need to do is determine what code we need to perform our given task on a single set of inputs. In this case, we need to Subset the data dat to a single cruise (Let’s use Cruise 72) Fit a linear model for 1 variable by depth (Let’s do oxygen) Summarize the linear model Extract p-values from the summary Remember from earlier in this workshop, a “summary.lm” object is an R list object so we need to use $ to obtain the p-values dat.subset &lt;- dat %&gt;% filter(Cruise == 72) model &lt;- lm(dat.subset$O2_uM ~ dat.subset$Depth_m) sum &lt;- summary(model) sum ## ## Call: ## lm(formula = dat.subset$O2_uM ~ dat.subset$Depth_m) ## ## Residuals: ## Min 1Q Median 3Q Max ## -46.360 -20.673 -2.353 21.741 67.887 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 158.9952 17.7699 8.947 3.64e-07 *** ## dat.subset$Depth_m -1.0215 0.1534 -6.660 1.08e-05 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 32.98 on 14 degrees of freedom ## Multiple R-squared: 0.7601, Adjusted R-squared: 0.7429 ## F-statistic: 44.35 on 1 and 14 DF, p-value: 1.079e-05 pval &lt;- sum$coefficients[,&quot;Pr(&gt;|t|)&quot;] print(pval) ## (Intercept) dat.subset$Depth_m ## 3.636103e-07 1.079146e-05 Step 2: Turn the task into a function Now, the simplest function to complete this task only needs y (the geochemical variable) and data as inputs. Notice that we need to call y within the function as dat.subset[[y]] in order to retrieve the data in dat.subset$O2 instead of just the character value “O2.” lm.function &lt;- function(data, y){ ### dat.subset &lt;- data %&gt;% filter(Cruise == 72) model &lt;- lm(dat.subset[[y]] ~ dat.subset$Depth_m) ### sum &lt;- summary(model) pval &lt;- sum$coefficients[,&quot;Pr(&gt;|t|)&quot;] print(pval) } ### lm.function(data=dat, y=&quot;O2_uM&quot;) ## (Intercept) dat.subset$Depth_m ## 3.636103e-07 1.079146e-05 However, to make this function more robust (and thus, useful in other situations), let’s allow the user to change multiple inputs including the Cruise as well as x and y in the linear model. lm.function &lt;- function(data, cruise, x, y){ ### dat.subset &lt;- data %&gt;% filter(Cruise == cruise) ### model &lt;- lm(dat.subset[[y]] ~ dat.subset[[x]]) ### sum &lt;- summary(model) pval &lt;- sum$coefficients[,&quot;Pr(&gt;|t|)&quot;] print(pval) } lm.function(data=dat, cruise=72, x=&quot;Depth_m&quot;, y=&quot;O2_uM&quot;) ## (Intercept) dat.subset[[x]] ## 3.636103e-07 1.079146e-05 Importantly, we can compare the outputs of Steps 1 and 2 to verify that our function is completing the correct task. 6.6.2 Step 3: Add packages For reproducibility, you should include all necessary packages within a function. Since we are using filter from the tidyverse, we will start our function by requiring this package. Best practices dictate that we use require( ) within a function instead of library( ), which we used to load packages into our project earlier in this workshop. Both of these functions load packages into the environment. However, require( ) outputs a warning and continues if the package is not found, whereas library( ) issues an error that would halt the function. lm.function &lt;- function(data, cruise, x, y){ require(tidyverse) ### dat.subset &lt;- data %&gt;% filter(Cruise == cruise) model &lt;- lm(dat.subset[[y]] ~ dat.subset[[x]]) sum &lt;- summary(model) pval &lt;- sum$coefficients[,&quot;Pr(&gt;|t|)&quot;] print(pval) } lm.function(data=dat, cruise=72, x=&quot;Depth_m&quot;, y=&quot;O2_uM&quot;) ## (Intercept) dat.subset[[x]] ## 3.636103e-07 1.079146e-05 6.6.3 Step 4: Start you r documentation It’s early yet but it’s never too early to start commenting your code! Functions can get very complex and even simple functions are not particularly readable if you weren’t the one to write them (or even if you were for that matter). So, let’s let the user know what everything does! lm.function &lt;- function(data, cruise, x, y){ # Load necessary packages require(tidyverse) # Subset the data to the cruise of interest dat.subset &lt;- data %&gt;% filter(Cruise == cruise) # Fit a linear model model &lt;- lm(dat.subset[[y]] ~ dat.subset[[x]]) # Summarize the model sum &lt;- summary(model) # Extract p-values from the summary pval &lt;- sum$coefficients[,&quot;Pr(&gt;|t|)&quot;] # Print p-values to the console print(pval) } 6.6.4 Step 5: Loop through multiple y-variables A word of preface: For loops are not the most efficient or fastest method out there. Whenever possible, you should use apply functions instead of loops. However, for loops get the job done and are extremely useful if you want to run a simple function on a list of inputs. A basic for loop is like so. You determine what you want to name a and b. for(a in b){ # Perform some task(s) on each element in b, one after the other # A single element in b is represented by a } For example, we can loop through several years and print them in a phrase. for(year in 2015:2020){ phrase = paste(&quot;The year is&quot;, year) print(phrase) } ## [1] &quot;The year is 2015&quot; ## [1] &quot;The year is 2016&quot; ## [1] &quot;The year is 2017&quot; ## [1] &quot;The year is 2018&quot; ## [1] &quot;The year is 2019&quot; ## [1] &quot;The year is 2020&quot; It doesn’t matter what we call the elements (year in example above), just as long as you use the same name within the loop (date in example below). for (date in 2015:2020){ phrase = paste(&quot;The year is&quot;, date) print(phrase) } ## [1] &quot;The year is 2015&quot; ## [1] &quot;The year is 2016&quot; ## [1] &quot;The year is 2017&quot; ## [1] &quot;The year is 2018&quot; ## [1] &quot;The year is 2019&quot; ## [1] &quot;The year is 2020&quot; So, let’s apply a loop across 2 geochemical variables in our function. Notice that we don’t start the loop until the model because package loading and data sub-setting are the same for every geochemical variable we may input. There is no need to do these steps multiple times with a loop. Notice that in the loop, we replace what was y with the name that we’ve given any single element in y, y.variable. We also must now provide multiple y variables as input and these are provided as a list with c() lm.function &lt;- function(data, cruise, x, y){ # Load necessary packages require(tidyverse) # Subset the data to the cruise of interest dat.subset &lt;- data %&gt;% filter(Cruise == cruise) for(y.variable in y){ # Loop through all variables provided in y ### # Fit a linear model model &lt;- lm(dat.subset[[y.variable]] ~ dat.subset[[x]]) ### # Summarize the model sum &lt;- summary(model) # Extract p-values from the summary pval &lt;- sum$coefficients[,&quot;Pr(&gt;|t|)&quot;] # Print p-values to the console print(pval) } } lm.function(data=dat, cruise=72, x=&quot;Depth_m&quot;, y=c(&quot;O2_uM&quot;,&quot;NO3_uM&quot;)) ## (Intercept) dat.subset[[x]] ## 3.636103e-07 1.079146e-05 ## (Intercept) dat.subset[[x]] ## 0.0003919827 0.0740374332 And with this added complexity, we want to check that our function is still completing its tasks correctly. So, we should compare the function’s output to the same tasks by-hand. dat.subset &lt;- dat %&gt;% filter(Cruise == 72) model &lt;- lm(dat.subset$O2_uM ~ dat.subset$Depth_m) sum &lt;- summary(model) pval &lt;- sum$coefficients[,&quot;Pr(&gt;|t|)&quot;] print(pval) ## (Intercept) dat.subset$Depth_m ## 3.636103e-07 1.079146e-05 model &lt;- lm(dat.subset$NO3_uM ~ dat.subset$Depth_m) sum &lt;- summary(model) pval &lt;- sum$coefficients[,&quot;Pr(&gt;|t|)&quot;] print(pval) ## (Intercept) dat.subset$Depth_m ## 0.0003919827 0.0740374332 6.6.4.1 Exercise: Using a custom function Apply the current lm.function to all the available geochemical variables in the Saanich data set. Which ones appear to be significantly correlated with depth? Copy the lm.function and alter it to print out the models’ adjusted R-squared values instead of p-values. Be sure to run the function with inputs to make sure it works! 6.6.5 Step 6: Save results to a table Now that we can obtain linear model p-values across all our variables of interest, let’s save them to a cleanly formatted table. 6.6.6 Step 6.1: Save all iterations of the loop The first thing to consider is what class of data the p-values are currently being output as. We can check this from our pval object created above, since this object is saved in the global environment. class(pval) ## [1] &quot;numeric&quot; attributes(pval) ## $names ## [1] &quot;(Intercept)&quot; &quot;dat.subset$Depth_m&quot; is.vector(pval) ## [1] TRUE length(pval) ## [1] 2 It looks like our output is a vector of length 2 with names. If we simply try to save the output of our function to the global environment with &lt;&lt;-, we will only see the last iteration of the loop. lm.function &lt;- function(data, cruise, x, y){ # Load necessary packages require(tidyverse) # Subset the data to the cruise of interest dat.subset &lt;- data %&gt;% filter(Cruise == cruise) for(y.variable in y){ # Loop through all variables provided in y # Fit a linear model model &lt;- lm(dat.subset[[y.variable]] ~ dat.subset[[x]]) # Summarize the model sum &lt;- summary(model) # Extract p-values from the summary pval &lt;- sum$coefficients[,&quot;Pr(&gt;|t|)&quot;] ### } # Save results to the global environment pval &lt;&lt;- pval ### } lm.function(data=dat, cruise=72, x=&quot;Depth_m&quot;, y=c(&quot;O2_uM&quot;, &quot;NO3_uM&quot;)) pval ## (Intercept) dat.subset[[x]] ## 0.0003919827 0.0740374332 So, we need to create a list object to hold each iteration’s result and then rbind (row bind) them into a single results object lm.function &lt;- function(data, cruise, x, y){ # Load necessary packages require(tidyverse) # Create an empty list to hold results pval = list() ### # Subset the data to the cruise of interest dat.subset &lt;- data %&gt;% filter(Cruise == cruise) for(y.variable in y){ # Loop through all variables provided in y # Fit a linear model model &lt;- lm(dat.subset[[y.variable]] ~ dat.subset[[x]]) # Summarize the model sum &lt;- summary(model) # Extract p-values from the summary. Save into the pval list based on the y.variable name pval[[y.variable]] &lt;- sum$coefficients[,&quot;Pr(&gt;|t|)&quot;] ### } # Bind all results into 1 single object pval &lt;&lt;- do.call(rbind,pval) ### } lm.function(data=dat, cruise=72, x=&quot;Depth_m&quot;, y=c(&quot;O2_uM&quot;, &quot;NO3_uM&quot;)) pval ## (Intercept) dat.subset[[x]] ## O2_uM 3.636103e-07 1.079146e-05 ## NO3_uM 3.919827e-04 7.403743e-02 6.6.7 Step 6.2 Beautify the output Column names Now we have all the results in table, but the column names aren’t very informative. So, we can rename the columns with rename_at. lm.function &lt;- function(data, cruise, x, y){ # Load necessary packages require(tidyverse) # Create an empty list to hold results pval = list() # Subset the data to the cruise of interest dat.subset &lt;- data %&gt;% filter(Cruise == cruise) for(y.variable in y){ # Loop through all variables provided in y # Fit a linear model model &lt;- lm(dat.subset[[y.variable]] ~ dat.subset[[x]]) # Summarize the model sum &lt;- summary(model) # Extract p-values from the summary. Save into the pval list based on the y.variable name pval[[y.variable]] &lt;- sum$coefficients[,&quot;Pr(&gt;|t|)&quot;] } # Bind all results into 1 single object pval &lt;- do.call(rbind,pval) ### No longer saved to global environment # Rename columns pval &lt;&lt;- pval %&gt;% ### rename_at(vars(colnames(pval)), ~c(&quot;Intercept.p&quot;, &quot;Depth.p&quot;)) ### } lm.function(data=dat, cruise=72, x=&quot;Depth_m&quot;, y=c(&quot;O2_uM&quot;, &quot;NO3_uM&quot;)) ## Error in UseMethod(&quot;tbl_vars&quot;): no applicable method for &#39;tbl_vars&#39; applied to an object of class &quot;c(&#39;matrix&#39;, &#39;array&#39;, &#39;double&#39;, &#39;numeric&#39;)&quot; However, our data output is current a matrix and the tidyverse only works on data frames. Thus, we need to reformat pval. lm.function &lt;- function(data, cruise, x, y){ # Load necessary packages require(tidyverse) # Create an empty list to hold results pval = list() # Subset the data to the cruise of interest dat.subset &lt;- data %&gt;% filter(Cruise == cruise) for(y.variable in y){ # Loop through all variables provided in y # Fit a linear model model &lt;- lm(dat.subset[[y.variable]] ~ dat.subset[[x]]) # Summarize the model sum &lt;- summary(model) # Extract p-values from the summary. Save into the pval list based on the y.variable name pval[[y.variable]] &lt;- sum$coefficients[,&quot;Pr(&gt;|t|)&quot;] } # Bind all results into 1 single object pval &lt;- as.data.frame(do.call(rbind,pval)) ### # Rename columns pval &lt;&lt;- pval %&gt;% rename_at(vars(colnames(pval)), ~c(&quot;Intercept.p&quot;, &quot;Depth.p&quot;)) } lm.function(data=dat, cruise=72, x=&quot;Depth_m&quot;, y=c(&quot;O2_uM&quot;, &quot;NO3_uM&quot;)) pval ## Intercept.p Depth.p ## O2_uM 3.636103e-07 1.079146e-05 ## NO3_uM 3.919827e-04 7.403743e-02 Dynamic naming Let’s go one step further and make all of the names dynamic, i.e. based on the function’s inputs, so that we don’t need to change the function if we input a different x-variable. Note how we can create a dynamic name using data from the pval output (example, col1) or from the function inputs (examples, col2 and table.name). lm.function &lt;- function(data, cruise, x, y){ # Load necessary packages require(tidyverse) # Create an empty list to hold results pval = list() # Subset the data to the cruise of interest dat.subset &lt;- data %&gt;% filter(Cruise == cruise) for(y.variable in y){ # Loop through all variables provided in y # Fit a linear model model &lt;- lm(dat.subset[[y.variable]] ~ dat.subset[[x]]) # Summarize the model sum &lt;- summary(model) # Extract p-values from the summary. Save into the pval list based on the y.variable name pval[[y.variable]] &lt;- sum$coefficients[,&quot;Pr(&gt;|t|)&quot;] } # Bind all results into 1 single object pval &lt;- as.data.frame(do.call(rbind,pval)) # Create dynamic column names col1 &lt;- paste(colnames(pval)[1], &quot;p&quot;, sep=&quot;.&quot;) ### col2 &lt;- paste(x, &quot;p&quot;, sep=&quot;.&quot;) ### table.name &lt;- paste(x, &quot;lm_pvals&quot;, sep=&quot;_&quot;) ### # Rename columns pval &lt;- pval %&gt;% ### rename_at(vars(colnames(pval)), ~c(col1, col2)) ### # Rename output table and save to environment assign(table.name, pval, envir = .GlobalEnv) ### } lm.function(data=dat, cruise=72, x=&quot;Depth_m&quot;, y=c(&quot;O2_uM&quot;, &quot;NO3_uM&quot;)) Depth_m_lm_pvals ## (Intercept).p Depth_m.p ## O2_uM 3.636103e-07 1.079146e-05 ## NO3_uM 3.919827e-04 7.403743e-02 Now we have a very robust function that we can use to fit linear models across any variables in our data set! 6.6.8 Step 7: Complete documentation The final step in any analysis, but especially in functions, is to go over your documentation and commenting to make sure everything is clear. Best practices are to include an introduction to the function that includes the purpose, license, inputs, and outputs. Since functions can get quite long, it is also best practices to save them in separate R scripts. So our lm.function would be in a file lm.function.R that contains &quot; lm.function: Estimates p-values from linear models of geochemical data from a single Saanich Inlet cruise Kim Dill-Mcfarland kadm@mail.ubc.ca University of British Columbia LICENSE Copyright (C) 2018 Kim Dill-McFarland This program is free software: you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU General Public License for more details. You should have received a copy of the GNU General Public License along with this program. If not, see &lt;http://www.gnu.org/licenses/&gt;. INPUTS data: a data.frame or tibble containing variables of interest cruise: Cruise # for subsetting data, 18-100 available in current data x: x-variable in lm, for example &#39;Depth_m&#39; y: y-variables in lm, for example c(&#39;O2_uM&#39;, &#39;NO3_uM&#39;) OUTPUTS data frame of p-values named [x]_lm_pvals &quot; lm.function &lt;- function(data, cruise, x, y){ # Load necessary packages require(tidyverse) # Create an empty list to hold results pval = list() # Subset the data to the cruise of interest dat.subset &lt;- data %&gt;% filter(Cruise == cruise) for(y.variable in y){ # Loop through all variables provided in y # Fit a linear model model &lt;- lm(dat.subset[[y.variable]] ~ dat.subset[[x]]) # Summarize the model sum &lt;- summary(model) # Extract p-values from the summary. Save into the pval list based on the y.variable name pval[[y.variable]] &lt;- sum$coefficients[,&quot;Pr(&gt;|t|)&quot;] } # Bind all results into 1 single object pval &lt;- as.data.frame(do.call(rbind,pval)) # Create dynamic column names col1 &lt;- paste(colnames(pval)[1], &quot;p&quot;, sep=&quot;.&quot;) col2 &lt;- paste(x, &quot;p&quot;, sep=&quot;.&quot;) table.name &lt;- paste(x, &quot;lm_pvals&quot;, sep=&quot;_&quot;) # Rename columns pval &lt;- pval %&gt;% ### rename_at(vars(colnames(pval)), ~c(col1, col2)) # Rename output table and save to environment assign(table.name, pval, envir = .GlobalEnv) } And you could load this function into any project with #source(&quot;lm.function.R&quot;) Also, you should share your function with the world! Consider posting on GitHub or another open source site. You’d be surprised how many people might find your function useful as-is or as a starting off point to customize for their own needs. 6.6.8.1 Exercise: Using a dynamic custom function Using our final lm.function, determine the linear fits for all geochemical variables for Cruise 12. Choose a different x variable and determine if any of the Saanich geochemical variables correlate with it. 6.6.8.2 Challenge exercise: You may not have time in this workshop to complete these challenging exercises. However, we encourage you to complete them on your own to test your knowledge of functions and improve your coding skills! How would you alter lm.function to accept sub-setting to multiple cruise #s? Hint: Think about using %in% when filtering the data. How would you alter lm.function to output FDR corrected p-values? 6.7 Packages Base R, in and of itself, is a pretty basic statistical program. What makes R so powerful (and popular) is the ability to load custom packages relevant to a wide range of data types and analyses. You have already seen the utility of packages such as the tidyverse in this and other workshops. But why code your own package? Practically speaking, creating reproducible functions in self-contained R scripts is sufficient to meet most (if not all) of your analysis needs. We are certainly not suggesting that you wrap all of your R code into packages! However, there are a number of advantages to turning your code into an R package. Creating a package helps you improve your code and documentation Packages facilitate disseminate of your code to your collaborators, colleagues, and others in your field Sharing packages contributes to the continuously evolving open source community Publicly sharing packages (like CRAN or GitHub) builds your recognition in the community and adds citeable value to your CV But there are also certain challenges in creating a package such as: rigorous quality checks to submit a package on central repositories like CRAN the need to continuously resolve bugs and other maintenance work the requirement to continuously roll out new updates for continued hosting on CRAN (or else the package is orphaned) So, the utility of packages is best served by functions that you use multiple times in multiple places and/or that you would like to share with others. Thus, functions in packages should be as general as possible so that they are applicable to a wide range of problems. Our lm.function does not necessarily meet these requirements as the data sub-setting set is specific to to our Saanich data set cruises. However, as an example, let’s turn this function into a package! 6.8 Creating a package project Fortunately, R has a package for creating packages! The devtools package provides all that you need to turn your code into a package. To begin, create a new project in a new directory in your workshop directory. “File” &gt; “New Project” &gt; “New Directory” &gt; “R package” and name it “testPackage.” You will be automatically provided with some files and directories to show you how a package must be organized including: DESCRIPTION: text file containing the basic description of your package R/: directory for R scripts man/: directory for help files NAMESPACE: context for looking up the value of an object associated with a name (more on this later) To start, your package is populated with a toy ‘Hello, world!’ example. 6.8.1 Description Open the DESCRIPTION file in a text editor or RStudio and update it with information relevant to this package. For example, mine would be: Package: testPackage Type: Package Title: Saanich Inlet Analyses Version: 0.1.0 Author: Kim Dill-McFarland Maintainer: Kim Dill-McFarland &lt;kadm@mail.ubc.ca&gt; Description: Functions to analyze longitudinal geochemical data from Saanich Inlet. In particular, applying analyses across multiple geochemical variables and printing relevant outputs to tables. License: GPL-2 Encoding: UTF-8 LazyData: true These are the required fields but you could include a number of additional metadata terms as needed. To see examples, go to your Packages tab (lower right quadrant of RStudio) and click on any package. You will see the first thing listed is the ‘DESCRIPTION file’ like the one you’ve made here. 6.8.2 R/ This is where your actual R code goes. It is generally a bad idea to put all functions into the same R file. However, it is also not necessary to use a separate R file for each individual function. Oftentimes, several functions are naturally connected and/or related and can, therefore, live in the same file. We only have 1 function for our test example so let’s copy our lm.function.R file into R/. We can also delete all the introductory text now that we’ll be incorporating this function into a package. 6.8.3 man/ This is where all your help files go, including the help page, manual, and vignette files. These files will be automatically created as be continue to develop our package. For example, the help page file .Rd will be created once we update our imports and exports below. 6.8.4 Namespace The namespace of your package is a pretty advanced topic that is pretty useless if you’re creating a package just for yourself. However, it is required for publicly hosted packages. Basically, the namespace is what it is: a space for names. It determines where R looks for objects (like functions) so that your package does not interfere with other packages and vice versa. You’ve actually already encountered a namespace issue when you loaded the tidyverse, because dplyr contains functions with the same names as those in another package, stats. ── Conflicts ──────────────────────────────────────────────────────── tidyverse_conflicts() ── ✖ dplyr::filter() masks stats::filter() ✖ dplyr::lag() masks stats::lag() So, it is best to avoid overlap in function names. However, with so many packages out there, it is difficult to be 100% sure that you’ve achieved 0% overlap. Thus, the NAMESPACE file. There are two parts of the namespace: imports and exports. Imports: how a function in your package finds a function in another package Exports: packages available outside of your package Fewer exports is preferred as it minimizes the potential for conflicts with other packages You start with the following and should not try to fill out the NAMESPACE file by-hand. exportPattern(&quot;^[[:alpha:]]+&quot;) 6.8.5 Adding imports and exports Instead, we can use devtools to automatically populate NAMESPACE and help page files. We know that our lm.function uses the filter function from the tidyverse (from the dplyr package specifically). Thus, we need to add the import of this package to our DESCRIPTION file. You simply add the following to the end of the file and save it. Then, we also delete the require() line from our function. Imports: dplyr This, however, will only tell R that our package depends on dplyr. We still need to state which functions we need from this package. This is where the NAMESPACE comes into play. But again, we do not want to try to fill out this file by-hand. Instead, we can use document (aka roxygen2) from devtools to fill out the file for us. To do this, we add special comments to lm.function.R. These special comments have the syntax #' and would be the following for lm.function. #&#39; Linear models across multiple variables #&#39; #&#39; Estimates p-values from linear models of geochemical data from a single Saanich Inlet cruise #&#39; #&#39; @param data data frame #&#39; @param cruise subsetting variable (numeric) #&#39; @param x independent variable in linear model (only singular accepted) #&#39; @param y dependent variable in linear model (list accepted) #&#39; @return a p-value table #&#39; #&#39; @importFrom dplyr filter #&#39; @export lm.function &lt;- function(data, cruise, x, y){ ... } The first line of the comments specifies the title of the help page and should, thus, be brief. The second paragraph is a short description of what the function does. All other paragraphs will be shown in the Details section of the help page by default. We then document the arguments (@param) and what the function returns (@return). All of these inputs contribute to the automatically generated help page in man/. The last part is finally stating which functions we need from what package (@importFrom) and what to export (e.g. our function). These inputs impact the NAMESPACE. Once we have properly documented the function, we can create the help pages and update the NAMESPACE file by running the following. BEFORE doing so, also delete the example ‘Hello, world!’ files. devtools::document() Updating testPackage documentation Loading testPackage Updating roxygen version in /Users/kim/GitHub/workshops_data_science/intermediate_R/testPackage/DESCRIPTION Writing NAMESPACE Writing NAMESPACE The same result can also be achieved directly from the menu bar with Build &gt; Document. If we open NAMESPACE, we will see that it has been updated. # Generated by roxygen2: do not edit by hand export(lm.function) importFrom(dplyr,filter) As has the help page % Generated by roxygen2: do not edit by hand % Please edit documentation in R/lm.function.R \\name{lm.function} \\alias{lm.function} \\title{Linear models across multiple variables} \\usage{ lm.function(data, cruise, x, y) } \\arguments{ \\item{data}{data frame} \\item{cruise}{subsetting variable (numeric)} \\item{x}{independent variable in linear model (only singular accepted)} \\item{y}{dependent variable in linear model (list accepted)} } \\value{ a p-value table } \\description{ Estimates p-values from linear models of geochemical data from a single Saanich Inlet cruise } 6.8.6 Test your package Now we can finally load the package. devtools::load_all() The same result can be achieved directly from the menu bar with Build &gt; Load All And then test out new function! Since we are now working in the package project, we should reload our data. library(tidyverse) dat &lt;- read_csv(file=&quot;../data/Saanich_Data_clean.csv&quot;, col_names=TRUE, na=c(&quot;&quot;, &quot;NA&quot;, &quot;NAN&quot;, &quot;ND&quot;)) lm.function(data=dat, cruise=72, x=&quot;Depth_m&quot;, y=c(&quot;O2_uM&quot;, &quot;NO3_uM&quot;)) Depth_lm_pvals 6.8.7 Test your package again Now that you know your package works locally on your computer, you can check if it will perform well globally. devtools::check() R CMD check results 0 errors | 0 warnings | 1 note checking R code for possible problems ... NOTE lm.function: no visible global function definition for ‘%&gt;%’ lm.function: no visible binding for global variable ‘Cruise’ lm.function: no visible global function definition for ‘lm’ lm.function: no visible global function definition for ‘rename_at’ lm.function: no visible global function definition for ‘vars’ Undefined global functions or variables: %&gt;% Cruise lm rename_at vars Consider adding importFrom(&quot;stats&quot;, &quot;lm&quot;) to your NAMESPACE file. Found the following assignments to the global environment: File ‘testPackage/R/lm.function.R’: assign(table.name, pval, envir = .GlobalEnv) If you then wanted to upload to CRAN or to Bioconductor, you would need to resolve the issues listed in the check. 6.9 Further resources Hadley Wickham’s R Packages is an excellent, FREE resource on package development. Writing R Extensions is the canonical, usually most up-to-date, reference for creating R packages. The goodpractice package provides a great benchmark to compare your package to. A great introduction to roxygen2 is available online. rOpenSci provides thorough instructions for how to create and validate your package through them. 6.10 Survey Please provide us with feedback through this short survey. "]]
